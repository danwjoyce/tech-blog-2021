<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Dan W Joyce</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 18 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hub05f02e644906e3d80c1c494250cac2e_12120_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Big Data, AI and Psychiatry</title>
      <link>/post/big-data-ai/</link>
      <pubDate>Sat, 18 Jun 2022 00:00:00 +0000</pubDate>
      <guid>/post/big-data-ai/</guid>
      <description>



&lt;p&gt;My colleague &lt;a href=&#34;https://twitter.com/lj_ali1&#34;&gt;Lia Ali&lt;/a&gt; pointed me to this – at this year’s Royal College of Psychiatrist’s International Congress, there’s a debate titled &lt;a href=&#34;https://www.rcpsych.ac.uk/events/congress/programme&#34;&gt;&lt;em&gt;Clinic appointment with Skynet? This house believes that the RCPsych should embrace Artificial Intelligence and Big Data in guiding clinical decision making and service development&lt;/em&gt;&lt;/a&gt;. Regrettably, I’m going to miss the Congress so won’t be able to attend this exciting panel discussion, but here’s some thoughts.&lt;/p&gt;
&lt;p&gt;Fair advance warning, this is a highly opinionated and somewhat personal/anecdotal piece so you know the risks …&lt;/p&gt;
&lt;p&gt;First of all – in my view – the two topics (Big Data and AI) would benefit from being debated separately, especially as two application domains are mentioned (clinical decision making and service development). And I’ll explain why as we go. I wonder if the two topics (AI and Big Data) are often headlined together because there’s an assumption that AI techniques – almost always, inductive learning algorithms – often require vast datasets. But not &lt;em&gt;all&lt;/em&gt; inductive methods required huge amounts of data (more on this later). Further, as I’ll elaborate, I think there is a tendency to see AI as some computational alchemy which turns variable quality (but voluminous) data into robust inferences and insights you couldn’t possibly have obtained using mature, established disciplines like statistics and epidemiology. My view is that intellectually, AI (however you conceive it or divide up the many sub-fields) inherits from the traditions of formal (rather than empirical) sciences and engineering. In this regard, given evidence-based medicine (EBM) inherits more from empirical science my view is that statistics, biostatistics and epidemiology as sanity-checks for the direction of travel and outputs of AI, machine learning and engineering-based treatments of clinical data.&lt;/p&gt;
&lt;div id=&#34;reputation-risk&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Reputation Risk&lt;/h1&gt;
&lt;p&gt;AI is a broad concept and is used today (in my view) vaguely and without qualification – almost as if everyone implicitly knows what the term means. But let’s be clear – the term carries &lt;em&gt;substantial&lt;/em&gt; historical baggage and investing blindly in “AI will solve this problem” is risky because if we fail, the cost in terms of resource, enthusiasm (often, to be read as “hype”), energy and reputation will be substantial. For this reason, I was left wanting by the &lt;a href=&#34;https://topol.hee.nhs.uk/&#34;&gt;Topol review&lt;/a&gt; – look at pp. 5 of the &lt;a href=&#34;https://topol.hee.nhs.uk/wp-content/uploads/HEE-Topol-Review-Mental-health-paper.pdf&#34;&gt;Topol review for mental health&lt;/a&gt; to see why.&lt;/p&gt;
&lt;p&gt;An anecdote: when I was writing up my Ph.D. in early 2000, my supervisor (one of the most helpful, tolerant and supportive academics I’ve ever met) told me bluntly “Do not juxtapose the words &lt;em&gt;artificial&lt;/em&gt; and &lt;em&gt;intelligence&lt;/em&gt; in your thesis title … No one will examine it”.&lt;/p&gt;
&lt;p&gt;And this was precisely because at that time, there was a history of at least two “waves” of AI that had failed; so-called &lt;a href=&#34;https://en.wikipedia.org/wiki/AI_winter&#34;&gt;AI winters&lt;/a&gt; – and as I discuss shortly, skepticism about the status of current machine learning methods.&lt;/p&gt;
&lt;p&gt;Debatably, the first AI winter resulted from the failures of what became know in the 1980s-1990s as connectionism, or, the period of neural networks research dating back to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neuron&#34;&gt;McCulloch-Pitts&lt;/a&gt; model neuron, early learning algorithms such as the &lt;a href=&#34;https://en.wikipedia.org/wiki/ADALINE&#34;&gt;Widrow-Hoff&lt;/a&gt; rule and Frank Rosenblatt’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Perceptron&#34;&gt;perceptron&lt;/a&gt; that culimated in 1969, when Minsky and Papert’s book “Perceptrons” showed that certain kinds of neural networks can’t learn a fundemantal logical operator, the exclusive OR (XOR) function. Like most stories, there’s subtlety that get’s ignored but ultimately, in the UK at least, the 1973 &lt;a href=&#34;https://en.wikipedia.org/wiki/Lighthill_report&#34;&gt;Lighthill report&lt;/a&gt; certainly kaiboshed the field. Neural networks (at least, the feed-forward, function approximating kind most like modern deep networks) came alive again in the 1980s, primarily because it was discovered that if you add more layers of model McCulloch-Pitts style neurons between the input and output and combine this architecture with an algorithm to adjust the parameters between model neurons (the &lt;a href=&#34;https://en.wikipedia.org/wiki/Backpropagation&#34;&gt;back-propogation&lt;/a&gt; rule) suddenly, you could do much more. Notice something here: a way out of a difficult slump is to a) build a bigger network and b) innovate on learning algorithms that can optimise and estimate the model’s parameters to make it perform. Sound familiar ?&lt;/p&gt;
&lt;p&gt;More anecdotal data: toward the end of the 1990s, where I was working, the enthusiasm for neural networks – invigorated again in 1987 by the publication of the two-volume book &lt;a href=&#34;https://en.wikipedia.org/wiki/Connectionism&#34;&gt;“Parallel Distributed Processing: Explorations in the Microstructure of Cognition”&lt;/a&gt; by Rumelhart, McClelland and the PDP Research Group – was waning because&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it could be shown that multilayer perceptrons (the pre-cursor to today’s contemporary deep learning networks) essentially implemented multivariable (and in some cases, multivariate) regression&lt;/li&gt;
&lt;li&gt;there was a Bayesian formulation of multilayer perceptrons and their training by the back-propogation method (nowadays, &lt;a href=&#34;https://en.wikipedia.org/wiki/Automatic_differentiation&#34;&gt;autodiff&lt;/a&gt;) which offered a solid mathematical framework and dispelled some of the mythology around these techniques (that is to say, the story was “they don’t do anything special but are another way of implementing regression”)&lt;/li&gt;
&lt;li&gt;additionally, where I was working, techniques like &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;support vector machines&lt;/a&gt; (SVMs) and decision trees (such as Breiman’s &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34;&gt;CART&lt;/a&gt; method) could perform as well (if not better) on classification tasks. And many people where excited about &lt;a href=&#34;https://en.wikipedia.org/wiki/Fuzzy_logic&#34;&gt;fuzzy logic&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alongside this, “classical” AI – based on symbolic reasoning, rather than the numerical, probabilistic and statistical methods of neural networks – had been shown to be brittle and in some cases, computationally intractable; examples include the &lt;a href=&#34;https://plato.stanford.edu/entries/frame-problem/&#34;&gt;frame problem&lt;/a&gt; and symbolic &lt;a href=&#34;https://core.ac.uk/download/pdf/82093176.pdf&#34;&gt;search methods&lt;/a&gt; for planning actions. Expert systems – the 1980s version of engineering applications of symbolic AI to problems such as decision support in medicine – didn’t get very far either.&lt;/p&gt;
&lt;p&gt;Anyway – in the 1990s and 2000s researchers attempted to position themseleves outside the umbrella of AI because in essence, it was a toxic brand and this is because there had been too many promises that failed to deliver in the previous hype-cycles of the 1960s, 70s, 80s and 1990s.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ai-and-the-nhs&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; AI and the NHS&lt;/h1&gt;
&lt;p&gt;Anecdotally, when speaking with colleagues across industry, academia and the NHS, I’ve heard the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Real AI” is contemporary neural networks (by implication, anything else is not &lt;em&gt;real&lt;/em&gt; AI) – this is just machismo and patently non-sense. I’ve heard it said that the “NHS doesn’t do real AI” because it doesn’t deploy large, sophisticated neural networks. Let’s assume that “real AI” is equated specifically with the branch of machine learning that studies large neural networks and then let’s cherry pick one impressive achievement – beating a human at Go. By some estimates, DeepMind spent around &lt;a href=&#34;https://www.wired.com/story/deepminds-losses-future-artificial-intelligence/&#34;&gt;$35 million&lt;/a&gt; training a system to play Go. Take a look at the &lt;a href=&#34;https://www.nature.com/articles/nature16961.pdf&#34;&gt;paper&lt;/a&gt;, and you’ll see that they used either 48 CPUs and 8 GPUs, or for the distributed version, 1,202 CPUs and 176 GPUs. And it tooks 3 weeks of compute time to train.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You might reasonably ask “So what”. But if I want to buy cloud-based resource to implement and run my neural network-based solution to some problem, I might go to AWS and cost an on-demand &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/p4/&#34;&gt;p4d.24xlarge&lt;/a&gt; instance with 8 GPUs and I’m going to drive this thing hard for training and then use in production for inference – that’s over 23,000 USD (or £18,800) per month if I want it switched on 24/7 or if I buy a reserved instance, I can get this down to just over 8,000 USD (£6500) per month if I purchase for three years. To buy my own &lt;em&gt;single&lt;/em&gt; GPU (of a similar specification to those in the AWS instance described) will set me back about £9,000 ignoring the costs of a host server, storage and maintenance costs for running the rig.&lt;/p&gt;
&lt;p&gt;So, if we’re going to equate “real AI” with neural networks, the NHS better be sure it’s going to be worth it.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Contemporary large, deep neural networks can locate non-linear and long-range interactions (say, between variables) in a way that standard statistical techniques cannot – I’ve heard this said often and the architectures of modern neural networks are certainly configurable for this to be the case … but I’ve not seen this advantage demonstrated in applications (alluded to, certainly, but not demonstrated).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is one of those alchemy-like principles which I wonder may be based on an analogy with some actual theory. Back in the day (1989) Cybenko published a &lt;a href=&#34;https://en.wikipedia.org/wiki/Universal_approximation_theorem&#34;&gt;universal approximation theorem&lt;/a&gt; which showed multilayer perceptrons could approximate any real-valued mathematical function to an arbitrary degree of accuracy. Similar theorems have been developed for modern deep networks. These essentially tell us that neural networks &lt;em&gt;can&lt;/em&gt; capture very sophisticated and complex mappings from inputs to outputs &lt;em&gt;but&lt;/em&gt; they these theorems don’t deliver a specification for the actual, realisable network (that is, the inputs, hidden layers, parameterisation and so on). I don’t know for sure, but perhaps using these ideas (universal approximation) analogically – combined with the representational sophistication afforded by very large networks – provide us with hope that these long-range interactions, dependencies and correlations (say, in very long time series) might be captured or exploited.&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There seems to be an implicit belief that if one throws a big enough neural network at a problem, it can magically divine signal from noise and “add value” in ways that traditional inductive inference (e.g. statistical methods) cannot. It’s seems (to me at least) that there’s a tacit belief that low-fidelity data can be transformed by contemporary AI without critically looking at the source data and the desired application.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I think this last point is particularly pernicious. Some neural network architectures can do amazing things, but they tend to be in domains where – from the design of these neural networks – you’d &lt;em&gt;predict&lt;/em&gt; that they &lt;em&gt;should&lt;/em&gt; work. For example, &lt;a href=&#34;https://en.wikipedia.org/wiki/Convolutional_neural_network&#34;&gt;convolutional neural networks&lt;/a&gt; in radiology and imaging tasks. But still, they’ve been &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2021.02.28.21252634v1.full&#34;&gt;shown&lt;/a&gt; to learn aberrant mappings which are kinds of “short-cuts” from inputs to desired outputs that bear no resemblance to clinical reasoning; roughly, correlations between image features and classification task that turn out to be unlike any information a skilled radiologist would use.&lt;/p&gt;
&lt;p&gt;I’ll finish this section with this thought: it’s easy to be a nay-sayer and dump on other’s work. But my point is simply that we &lt;em&gt;do&lt;/em&gt; require more critical thinking that focuses on how data that is available mesh with robust and reliable methods that might be reasonably combined to produce useful results. And not just blindly expecting AI (however conceived) will “just solve the problem” – which as described above, didn’t get us very far.&lt;/p&gt;
&lt;p&gt;Also, I’d add that we should be more prosaic with our language – in some applications of AI or machine learning (ML) we are really just saying “We used methods derived from computational sciences and engineering to solve a problem”. A great example is when we use regularised regression to combine an element of feature selection with learning the probability of some outcome alongside internal validation techniques like cross-validation, resampling etc. If you’re going to call (essentially) regression “AI” then where do you stop ? You might as well decide that linear algebra is basically AI.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-support-and-service-development&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; 2. Decision Support and Service Development&lt;/h1&gt;
&lt;p&gt;Helping clinicians and patients make a decision is very different from service development (again, in my opinion). The latter is (I think) about &lt;a href=&#34;https://bmchealthservres.biomedcentral.com/articles/10.1186/1472-6963-14-20&#34;&gt;changing how we deliver care&lt;/a&gt; in response to demand, capacity, shifts in medical technology and the demographics of the people that a healthcare system serves. The former is about sitting alongside a patient, their data and helping decide on management, establishing a diagnosis and so on.&lt;/p&gt;
&lt;p&gt;Somewhat over-simplistically, one could say that service development requires understanding e.g. data about the locality/population characteristics, audits of the operational aspects of services and their delivery to targets or clinical guidelines and methods for marrying the two with a view to changing how care is delivered. Certainly, this requires lots of data and perhaps, opportunistic re-use of existing routinely-collected data. And statistical process control (SPC) is described as being &lt;a href=&#34;https://www.england.nhs.uk/statistical-process-control-tool/&#34;&gt;widely used&lt;/a&gt; in the NHS for understanding how changes in services deliver differences in outcomes.&lt;/p&gt;
&lt;p&gt;But I doubt this is the same data required when sitting alongside a patient, trying to arrive at a shared decision using decision support tools – the QRISK cardiovascular risk score being an example. It &lt;em&gt;could&lt;/em&gt; be the case that if a service collects high-fidelity, granular data about patients that this same data could be reused at the population/service level, but I think the two things remain very different.&lt;/p&gt;
&lt;p&gt;And this leads us to the next point: what data and methods you need, or use, depends on detailed understanding of the task at hand.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;big-is-not-necessarily-better&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Big Is Not Necessarily Better&lt;/h1&gt;
&lt;p&gt;When I trained in medicine, we got well-intentioned, but rudimentary, training on statistical methods under the guise of the evidence-based medicine (EBM) curriculum. As part of this, we learn many examples of where the precision of some estimate increases as the sample size, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; increases. For example, the standard error of the mean is &lt;span class=&#34;math inline&#34;&gt;\(\sigma/\sqrt{n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is the sample standard deviation. Almost everywhere you look, you find &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in the denominator and this leads us to internalise the idea that “bigger &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; means more precision” and we should trust large studies over small studies.&lt;/p&gt;
&lt;p&gt;Anecdotally, I’ve been involved in a few studies where – in spectacular examples of terrible research practices – the team has decided to collect “just a few more samples”, to increase &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and shrink the standard error of some inferential estimates and even worse, to shrink uncertainty estimates to the point where it “reaches statistical significance”. That’s a whole other debate.&lt;/p&gt;
&lt;p&gt;But of course, it’s never that easy. Achieving more precision requires better measurement not just larger &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and psychiatry (alongside biological sciences and medicine more generally) has noisy and imprecise measurements.&lt;/p&gt;
&lt;div id=&#34;noise-and-measurement-error&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Noise and Measurement Error&lt;/h2&gt;
&lt;p&gt;An example, lifted from Loken and Gelman’s 2017 paper &lt;a href=&#34;https://www.science.org/doi/full/10.1126/science.aal3618&#34;&gt;“Measurement error and the replication crisis”&lt;/a&gt;: If I tell you I can run a mile in 5 minutes, that’s impressive. If I inform you that I did this with a rucksack loaded with bricks, then you’d likely conclude I would have been even faster &lt;em&gt;without&lt;/em&gt; the weight slowing me down. By analogy: I perform a study involving &lt;span class=&#34;math inline&#34;&gt;\(n=100\)&lt;/span&gt; people, collecting the kind of data that allows me to estimate the odds ratio of having a disorder given some factors I want to control for and including a categorical variable of interest, representing membership of some tentative risk group (e.g. natal sex, membership of a certain ethnic group and so on). I estimate the odds ratio for the categorical variable of interest at 1.2, but my uncertainty is wide and the odds ratio might be between 0.7 and 5.7. I write this up, stating we can’t be conclusive about the contribution of the categorical risk variable’s effect because the uncertainty interval [0.7, 5.7] includes 1.0. In my “future work, discussion and limitations” section, I write that “future work should include larger sample sizes” because (as above) I expect that my effect would be larger or more visible if it weren’t for the noise, and further, I can overcome the noise – increasing the precision of my estimate – by increasing &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Loken and Gelman show that: if you take two variables, &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; with an actual “ground truth” small effect of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and then consider two scenarios where the &lt;em&gt;observed&lt;/em&gt; measurements have either low or high measurement error (i.e. we add either a small or a large amount of noise) then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;with &lt;strong&gt;large&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and a truly modest effect size (i.e. the correlation between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is in reality small) – adding measurement error almost certainly reduces the observed correlation, making the effect less visible. Analogously, running a mile in 5 minutes but with a heavy rucksack.&lt;/li&gt;
&lt;li&gt;with &lt;strong&gt;small&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; and a truly modest effect size – adding measurement error can result in the observed correlation being &lt;em&gt;larger&lt;/em&gt;, making the effect artificially more “visible” to us. This represents the analogy and assumption we would have run faster (effect size/correlation would be larger) if we weren’t burdened by the rucksack (measurement error)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tells us that when we draw a conclusion from small samples, we should not rely on the idea that the impact of measurement error is obscuring what would have been a more impressive effect size.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;big-data-by-analogy-with-big-surveys&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Big Data by Analogy with Big Surveys&lt;/h2&gt;
&lt;p&gt;Now, I hear you: isn’t this why we need Big Data? But let’s consider the Big Data Paradox – introduced in Xiao-Li Meng’s 2018 paper &lt;a href=&#34;https://www.jstor.org/stable/pdf/26542550.pdf?casa_token=Qa7rVYZX-xsAAAAA:1-6jUYRwHrRW7ilgkUJE1SPJrTrL3zon8bsV_VCxB9vooWYTXz6ejmVTr7xeiNHIkWKvorcfefdaSTXoUMm9wtUcgXmQVLi77FwY8SMhvdGgEwaayA&#34;&gt;“Statistical Paradises and Paradoxes in Big Data”&lt;/a&gt; in the context of population surveys. Meng’s work shows that while increasing the sample size may reduce estimates of uncertainty, e.g. shrink confidence intervals for some statistic, it can also magnify bias in the statistic; consequently, we end up being very confident in summary statistics which are completely off-the-mark.&lt;/p&gt;
&lt;p&gt;Meng considers a finite sample &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of some population &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and focuses on the difference between some sample statistic (say, the average) &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n\)&lt;/span&gt; and the “true” population value of the same, &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_N\)&lt;/span&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for some sample &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, the difference (error) between the sample and population average is &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n - \bar{G}_N\)&lt;/span&gt; – how much we can trust the sample average &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n\)&lt;/span&gt; depends on this difference/error&lt;/li&gt;
&lt;li&gt;The error &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n - \bar{G}_N\)&lt;/span&gt; can be “decomposed” into a formula consisting of three terms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The three terms are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a &lt;em&gt;data quality&lt;/em&gt; measure – or the &lt;strong&gt;data defect correlation&lt;/strong&gt; measure – which captures total bias as the correlation between the event that an individual’s data &lt;em&gt;was&lt;/em&gt; recorded in the sample (a function of the sampling method) and the actual value recorded; in a high-quality sample, the values recorded should bear no relationship to whether or not the individual was included in the sample. Hence, the value of the data defect correlation will be low and ideally, close to zero.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A concrete example: In one electronic health record (EHR) study I was involved in, patient’s with long histories (and therefore, having larger volumes of clinical data in the EHR) were absent from the sample for purely technical reasons that we hadn’t spotted at the time (FYI, there was a bug in the software that just “dumped” patients from the sample if they had lots of historical notes). Assume we’re studying recurrent depression and using EHR samples to estimate the mean number of episodes as a function of age. In this case, the data defect correlation will be high because the event that the individual data was ‘captured’ in any sample would strongly correlate with both age and how many episodes they’d experienced (older people are more likely to have experienced more episodes and therefore have longer records).&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a &lt;em&gt;data quantity&lt;/em&gt; measure – expressed as a function of &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{(N-n)/n}\)&lt;/span&gt; so that if the sample is the entire population (complete data) &lt;span class=&#34;math inline&#34;&gt;\(n = N\)&lt;/span&gt; and this expression becomes zero (i.e. contributes nothing to the error).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note that the data quantity is defined such that the size of the sample &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; alone is not so important, rather, it’s the sample size as a proportion of the population size &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; that contributes to the overall error &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n - \bar{G}_N\)&lt;/span&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a &lt;em&gt;problem difficulty&lt;/em&gt; measure – expressed as a function of the standard deviation of the values measured in the population, which is to say, the more heterogenous the population values are, the harder it is to estimate a robust average &lt;span class=&#34;math inline&#34;&gt;\(\bar{G}_n\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Narratively, Meng is asking (see pp. 687) “Which one should we trust more, a 5% survey sample or an 80% administrative dataset?” or alternatively, “Should we invest in a small but well-designed, robust sample or large, routinely collected data from electronic health records, population surveillance system or a big online social-media opportunistic sample?”. The headline lesson of Meng’s paper is the &lt;strong&gt;Big Data Paradox&lt;/strong&gt;: “The more the data, the surer we fool ourselves”.&lt;/p&gt;
&lt;p&gt;A great example of the Big Data Paradox using Meng’s decomposition is Bradley &lt;em&gt;et al&lt;/em&gt;’s (2021) paper &lt;a href=&#34;https://www.nature.com/articles/s41586-021-04198-4?msclkid=ede012a7cfb711ec8629d39d2093074d&#34;&gt;“Unrepresentative big surveys significantly overestimated US vaccine uptake”&lt;/a&gt;. In this paper, they estimated the components above for three different survey designs of Covid vaccine uptake in the US. They had access to the actual number of vaccines delivered (i.e. the “true” population values) and could look at how biased the estimates were for the three surveys (samples). One of the surveys was a robustly designed, probabilistic sample of the population, but was relatively small at 995 people. Another was an opportunistically sampled survey from active social media users (with 181,949 people) and another was based on census addresses for which there was a mobile phone or email contact available consisting of 76,068 people.&lt;/p&gt;
&lt;p&gt;The results are interesting. Bradley &lt;em&gt;et al&lt;/em&gt; showed how the two very large surveys (relying more on opportunistic sampling) had high data defect correlations and the estimated average vaccine uptake differed systematically from the actual vaccinations administered. The small &lt;span class=&#34;math inline&#34;&gt;\(n=995\)&lt;/span&gt;, higher quality-sampled survey was, however, much closer to the population average.&lt;/p&gt;
&lt;p&gt;And the headline in Bradley &lt;em&gt;et al&lt;/em&gt;’s paper is that a survey of 250,000 people (with properties similar to the two large surveys) produces estimates of the population’s mean that are no more accurate than a truly random sample of just 10 (yes, ten) people.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Conclusion&lt;/h1&gt;
&lt;p&gt;The points I’m trying to articulate here are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;We need more precision in how we describe our methods – “AI” obscures what we mean, especially in the age of hype and over-promise. Sometimes, a more prosaic description of our methods might be less attention-grabbing but perhaps, more honest – so let’s see more papers saying “We used resampling methods for internal validation, on a modest data set, estimating a linear model’s parameters using maximum-likelihood” rather than assuming we need to call this AI to persuade readers of the value of what was undertaken. And let’s not assume contemporary AI has to be equated with the current trend for deep learning / massive neural networks&lt;/li&gt;
&lt;li&gt;Large neural network-models with millions/billions of parameters and sophisticated architectures are expensive to train and deploy – so we better be sure that they’re worth the investment&lt;/li&gt;
&lt;li&gt;Knowing the history of previous cycles of AI (and the subsequent “AI winters”) and understanding where and why they failed (which domains, applications and so on) is important; spectacular results enabled by increases in “raw” computational power and resources shouldn’t persuade us that everything is a breakthrough or a game-changer.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Big Data is not always better data – sure, size matters, but we should understand and attend to the intended use (or re-use) of any large data set; some opportunistically acquired data sets might be useful for some tasks, but give unreliable results for other tasks. For example, how we use routinely collected data for service development might radically differ from using the same data for individual-level clinical decision support&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’m aware that this blog covered none of the profound ethical or sociotechnical aspects, but that’s for another time.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stuff I Always Look Up ...</title>
      <link>/post/my-list-of-important-things/</link>
      <pubDate>Fri, 05 Nov 2021 00:00:00 +0000</pubDate>
      <guid>/post/my-list-of-important-things/</guid>
      <description>


&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#purpose&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;1&lt;/span&gt; Purpose&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#posterior-predictive-distributions&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2&lt;/span&gt; Posterior Predictive Distributions&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-probability&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.1&lt;/span&gt; Conditional Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#law-of-total-probability&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.2&lt;/span&gt; Law of Total Probability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conditional-independence&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.3&lt;/span&gt; Conditional Independence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-theorem&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.4&lt;/span&gt; Bayes Theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bayes-with-extra-conditioning&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.5&lt;/span&gt; Bayes with Extra Conditioning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deriving-the-ppd&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;2.6&lt;/span&gt; Deriving the PPD&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sampling-and-integration&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;3&lt;/span&gt; Sampling and Integration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logits-probabilities-and-odds&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4&lt;/span&gt; Logits, Probabilities and Odds&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#odds&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.1&lt;/span&gt; Odds&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#odds-ratios&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.2&lt;/span&gt; Odds Ratios&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistic-regression&#34;&gt;&lt;span class=&#34;toc-section-number&#34;&gt;4.3&lt;/span&gt; Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;purpose&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Purpose&lt;/h1&gt;
&lt;p&gt;This is a living, annotated bibliography of stuff I need to use on a semi-regular basis, always have to look up but in my chaotic file system, can never find. It also documents some embarrassing truths – stuff like &lt;em&gt;“Is variance the square root of standard deviation, or the other way round?”&lt;/em&gt; … Of course, it’s the other way round.&lt;/p&gt;
&lt;p&gt;So, point number one: If &lt;span class=&#34;math inline&#34;&gt;\(\sigma_X\)&lt;/span&gt; is the standard deviation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; then:
&lt;span class=&#34;math display&#34;&gt;\[
\mathrm{Var}(X) = \sigma^2_X
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-distributions&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Posterior Predictive Distributions&lt;/h1&gt;
&lt;p&gt;I spent a week trying to find a derivation of equation 1.4 on pp.7 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelman2014bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Gelman et al. 2014&lt;/a&gt;)&lt;/span&gt;, the posterior predictive distribution &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-rubin1984bayesianly&#34; role=&#34;doc-biblioref&#34;&gt;Rubin 1984&lt;/a&gt;)&lt;/span&gt; of new data &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt; given previously observed data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and a model with parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:PPD&#34;&gt;\[\begin{equation}
p( \tilde{y} | y ) = \int p\left( \tilde{y} | \theta \right) p\left( \theta | y \right)  d\theta \tag{2.1}
\label{eqn:finalPPD}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are explanations floating around on the internet, but none I could follow because they skipped steps and left me confused.&lt;/p&gt;
&lt;p&gt;We need a few basic laws and definitions from probability theory as follows &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2014introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt;:&lt;/p&gt;
&lt;div id=&#34;conditional-probability&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Conditional Probability&lt;/h2&gt;
&lt;p&gt;For two variables &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:condprob&#34;&gt;\[\begin{equation}
p(a | b) = \frac{p(a,b)}{p(b)} \tag{2.2}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Or re-arranged:
&lt;span class=&#34;math display&#34; id=&#34;eq:condprob2&#34;&gt;\[\begin{equation}
p(a,b) = p(a | b) p(b) \tag{2.3}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And for two variables &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, conditioned on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:condprob3&#34;&gt;\[\begin{equation}
p(a,b | c) = \frac{p(a,b,c)}{p(c)} \tag{2.4}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the re-arrangement:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(a,b,c) =  p(a,b | c) p(c)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(p(a,b,c)\)&lt;/span&gt; can also be factorised as &lt;em&gt;any&lt;/em&gt; of the following (depending on what we want to achieve):
&lt;span class=&#34;math display&#34; id=&#34;eq:factors3&#34; id=&#34;eq:factors2&#34; id=&#34;eq:factors1&#34;&gt;\[\begin{align}
  p(a,b,c) &amp;amp;= p(b,c|a) p(a) \tag{2.5} \\
          &amp;amp;= p(a,c|b) p(b) \tag{2.6} \\
          &amp;amp;= p(a,b|c) p(c) \tag{2.7}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;law-of-total-probability&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Law of Total Probability&lt;/h2&gt;
&lt;p&gt;From the joint probability &lt;span class=&#34;math inline&#34;&gt;\(p(a,b)\)&lt;/span&gt;, the marginal &lt;span class=&#34;math inline&#34;&gt;\(p(a)\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(a) = \int p \left( a, b \right) db
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And the continuous &lt;em&gt;law of total probability&lt;/em&gt; is &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2014introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; pp. 289:
&lt;span class=&#34;math display&#34; id=&#34;eq:lotp2&#34;&gt;\[\begin{align}
p(a) &amp;amp;= \int p \left( a,b \right) db \\
     &amp;amp;= \int p \left( a|b \right) p( b )db \tag{2.8}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we’ve used equation &lt;a href=&#34;#eq:condprob2&#34;&gt;(2.3)&lt;/a&gt; to re-write &lt;span class=&#34;math inline&#34;&gt;\(p \left( a,b \right)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(p \left( a|b \right) p( b )\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Adding conditioning on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; we obtain &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2014introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; pp. 54:
&lt;span class=&#34;math display&#34; id=&#34;eq:lotp-cond&#34;&gt;\[\begin{align}
\label{eqn:lotp_cont}
p \left( a|c \right) &amp;amp;= \int p \left( a,b | c \right) db \\
                     &amp;amp;= \int p \left( a|b,c \right) p( b | c ) db \tag{2.9}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-independence&#34; class=&#34;section level2&#34; number=&#34;2.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.3&lt;/span&gt; Conditional Independence&lt;/h2&gt;
&lt;p&gt;Two variables &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; are &lt;em&gt;conditionally independent&lt;/em&gt; given a third variable &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2014introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; pp. 58:
&lt;span class=&#34;math display&#34; id=&#34;eq:condind1&#34;&gt;\[\begin{equation}
( a \perp\!\!\!\perp c ) | b \iff p(a,c | b) = p( a | b ) p( c | b) \tag{2.10}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-theorem&#34; class=&#34;section level2&#34; number=&#34;2.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.4&lt;/span&gt; Bayes Theorem&lt;/h2&gt;
&lt;p&gt;For two variables &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:bayes&#34;&gt;\[\begin{equation}
p(a|b) = \frac{p(b|a)p(a)}{p(b)} \tag{2.11}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayes-with-extra-conditioning&#34; class=&#34;section level2&#34; number=&#34;2.5&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.5&lt;/span&gt; Bayes with Extra Conditioning&lt;/h2&gt;
&lt;p&gt;I frequently have to remind myself how to rewrite this form: &lt;span class=&#34;math inline&#34;&gt;\(p(a | b, c)\)&lt;/span&gt; – this is covered in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2014introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; Theorem 2.4.2 on pp. 54-56.&lt;/p&gt;
&lt;p&gt;There are a few useful ways to re-write.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using Conditional Probability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Assume it makes sense (in the problem we’re trying to solve) to view &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; as “one thing together” then using the formula for conditional probability we get:
&lt;span class=&#34;math display&#34; id=&#34;eq:bayesrewritecond&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{p(a,b,c)}{p(b,c)}  \tag{2.12}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Applying conditional probability again – equation &lt;a href=&#34;#eq:condprob3&#34;&gt;(2.4)&lt;/a&gt; and the different factorisations in &lt;a href=&#34;#eq:factors1&#34;&gt;(2.5)&lt;/a&gt; through &lt;a href=&#34;#eq:factors3&#34;&gt;(2.7)&lt;/a&gt; – to rewrite &lt;span class=&#34;math inline&#34;&gt;\(p(a,b,c)\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p(a,b,c) &amp;amp;= p(b,c|a) p(a)  \\
          &amp;amp;= p(a,c|b) p(b)  \\
          &amp;amp;= p(a,b|c) p(c)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Choosing the RHS as suited to the problem - here, we take &lt;span class=&#34;math inline&#34;&gt;\(p(b,c|a) p(a)\)&lt;/span&gt; as we are treating &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; as “one event” (we want to keep them together) and substitute in &lt;a href=&#34;#eq:bayesrewritecond&#34;&gt;(2.12)&lt;/a&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:bayesrewritecond2&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{ p(b,c|a) p(a) }{ p( b,c ) } \tag{2.13}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, we have to find an expression for the denominator &lt;span class=&#34;math inline&#34;&gt;\(p( b,c )\)&lt;/span&gt; and we have options including another application of conditional probability so &lt;span class=&#34;math inline&#34;&gt;\(p( b, c ) = p(b|c) p(c)\)&lt;/span&gt; resulting in:
&lt;span class=&#34;math display&#34; id=&#34;eq:bayesrewritecond2&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{ p(b,c|a) p(a) }{ p(b|c) p(c) } \tag{2.13}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Using the Chain Rule of Probability&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Start, as before, with:
&lt;span class=&#34;math display&#34; id=&#34;eq:bayesrewritecond3&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{p(a,b,c)}{p(b,c)} \tag{2.14}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This time, decompose &lt;span class=&#34;math inline&#34;&gt;\(p(a,b,c)\)&lt;/span&gt; differently, using the chain rule:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p(a,b,c) &amp;amp;= p(a|b,c)p(b,c)  \\
           &amp;amp;= p(b|a,c)p(a,c)  \\
           &amp;amp;= p(c|a,b)p(a,b)
\end{align}\]&lt;/span&gt;
Obviously, the first re-write gets us nowhere – it merely restates &lt;span class=&#34;math inline&#34;&gt;\(p(a|b,c)\)&lt;/span&gt;. Lets say the second factorisation is helpful for our problem &lt;span class=&#34;math inline&#34;&gt;\(p(b|a,c)p(a,c)\)&lt;/span&gt;, then we substitute in &lt;a href=&#34;#eq:bayesrewritecond3&#34;&gt;(2.14)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:bayesrewritecond4&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{p(b|a,c)p(a,c)}{p(b,c)} \tag{2.15}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now have to deal with &lt;span class=&#34;math inline&#34;&gt;\(p(a,c)\)&lt;/span&gt; in the numerator and &lt;span class=&#34;math inline&#34;&gt;\(p(b,c)\)&lt;/span&gt; in the denominator. Starting with the numerator, apply conditional probability again:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  p( a, c ) = p(a|c)p(c)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the denominator:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  p( b, c ) = p(b|c)p(c)
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What we end up with is Bayes theorem with extra conditioning on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; …&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes Theorem with Extra Conditioning on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Substitute both into &lt;a href=&#34;#eq:bayesrewritecond4&#34;&gt;(2.15)&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p( a | b, c ) &amp;amp;= \frac{p(b|a,c) p(a|c)p(c)} {p(b|c)p(c)} \\
                &amp;amp;= \frac{p(b|a,c) p(a|c)} {p(b|c)}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bayes Theorem with Extra Conditioning on &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Had we chosen, instead, to use &lt;span class=&#34;math inline&#34;&gt;\(p(c|a,b)p(a,b)\)&lt;/span&gt;, we end up with:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p( a | b, c ) &amp;amp;= \frac{p(a,b,c)}{p(b,c)} \\
                &amp;amp;= \frac{p(c|a,b)p(a,b)}{p(b,c)}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(p(a,b) = p(a|b)p(b)\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
  p( a | b, c ) = \frac{p(c|a,b)p(a|b)p(b)}{p(b,c)}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Of course, &lt;span class=&#34;math inline&#34;&gt;\(p(b,c) = p(c,b) = p(c|b)p(b)\)&lt;/span&gt; in the denominator:
&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  p( a | b, c ) &amp;amp;= \frac{p(c|a,b)p(a|b)p(b)}{p(c|b)p(b)} \\
                &amp;amp;= \frac{p(c|a,b)p(a|b)}{p(c|b)}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We chose between conditioning on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; depending on the problem we are trying to solve (i.e. does it make sense to consider everything being conditioned on &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; ?)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;deriving-the-ppd&#34; class=&#34;section level2&#34; number=&#34;2.6&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.6&lt;/span&gt; Deriving the PPD&lt;/h2&gt;
&lt;p&gt;Starting with the fundamental Bayesian modelling framework:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;before observing the data, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, the &lt;em&gt;prior distribution&lt;/em&gt; of the parameters is &lt;span class=&#34;math inline&#34;&gt;\(p(\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we have a &lt;em&gt;sampling distribution&lt;/em&gt; of the data &lt;em&gt;given&lt;/em&gt; parameters &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(p(\theta, y) = p(\theta)p(y|\theta)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;we then obtain the &lt;em&gt;posterior distribution&lt;/em&gt; of the parameters of the model given the observed data:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
    p(\theta|y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)p(y|\theta)}{p(y)}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So, we can think of the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; as the `output’ of Bayesian model estimation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We want to obtain a distribution for future values &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt; given the observed (and modelled) data &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; which is &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y}|y)\)&lt;/span&gt; using what we know about the posterior distribution arising from parameter estimation &lt;span class=&#34;math inline&#34;&gt;\(p(\theta | y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The first step is to write &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y}|y)\)&lt;/span&gt; using the law of total probability (with conditioning): equation &lt;a href=&#34;#eq:lotp-cond&#34;&gt;(2.9)&lt;/a&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:step1&#34;&gt;\[\begin{equation}
    p( \tilde{y} | y ) = \int p\left(\tilde{y},\theta | y \right) d\theta \tag{2.16}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Re-write the integrand &lt;span class=&#34;math inline&#34;&gt;\(p\left(\tilde{y},\theta | y \right)\)&lt;/span&gt; using equation &lt;a href=&#34;#eq:condprob3&#34;&gt;(2.4)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:step2&#34;&gt;\[\begin{equation}
    p( \tilde{y} | y ) = \int \frac{p(\tilde{y},\theta,y)}{p(y)} d\theta \tag{2.17}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We assert that &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt; is conditionally independent of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; &lt;em&gt;given&lt;/em&gt; the model parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:condind-ppd&#34;&gt;\[\begin{equation}
  ( \tilde{y} \perp\!\!\!\perp y ) | \theta \iff p( \tilde{y}, y | \theta) = p( \tilde{y} | \theta ) p( y | \theta) \tag{2.18}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To make use of the conditional independence &lt;span class=&#34;math inline&#34;&gt;\(p( \tilde{y}, y | \theta)\)&lt;/span&gt;, we have to factorise &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y},\theta,y)\)&lt;/span&gt; in equation &lt;a href=&#34;#eq:step2&#34;&gt;(2.17)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(a = \tilde{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(b = \theta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c = y\)&lt;/span&gt;; we are seeking a factorisation of &lt;span class=&#34;math inline&#34;&gt;\(p(a,b,c)\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(p(a,c|b)\)&lt;/span&gt; and inspecting the factorisations &lt;a href=&#34;#eq:factors1&#34;&gt;(2.5)&lt;/a&gt; through &lt;a href=&#34;#eq:factors3&#34;&gt;(2.7)&lt;/a&gt; we find that &lt;a href=&#34;#eq:factors2&#34;&gt;(2.6)&lt;/a&gt; matches:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:factor-cond-ind&#34;&gt;\[\begin{align}
  p(a,b,c) &amp;amp;= p(a,c|b) p(b) \\
  p( \tilde{y}, \theta, y) &amp;amp;= p( \tilde{y}, y | \theta) p(\theta) \tag{2.19}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substitution &lt;a href=&#34;#eq:factor-cond-ind&#34;&gt;(2.19)&lt;/a&gt; into &lt;a href=&#34;#eq:step2&#34;&gt;(2.17)&lt;/a&gt; we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:step3a&#34;&gt;\[\begin{equation}
p(\tilde{y} | y ) = \int \frac{p(\tilde{y},y | \theta) p(\theta)}{p(y)} d \theta \tag{2.20}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We make use of the equality from equation &lt;a href=&#34;#eq:condind-ppd&#34;&gt;(2.18)&lt;/a&gt; i.e. that &lt;span class=&#34;math inline&#34;&gt;\(p( \tilde{y}, y | \theta) = p( \tilde{y} | \theta ) p( y | \theta)\)&lt;/span&gt; and substitute into &lt;a href=&#34;#eq:step3a&#34;&gt;(2.20)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:step3b&#34;&gt;\[\begin{equation}
p(\tilde{y} | y ) = \int \frac{ p( \tilde{y} | \theta ) p( y | \theta)  p(\theta)}{p(y)} d \theta \tag{2.21}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recall that we want to make use of the ‘output’ of parameter estimation, the posterior distribution of the model parameters given the observed data &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;, and in equation &lt;a href=&#34;#eq:step3b&#34;&gt;(2.21)&lt;/a&gt; we see the term &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt;. All we need to do is re-write &lt;span class=&#34;math inline&#34;&gt;\(p(y|\theta)\)&lt;/span&gt; using Bayes rule, equation &lt;a href=&#34;#eq:bayes&#34;&gt;(2.11)&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(y|\theta) = \frac{p(\theta|y) p(y)}{p(\theta)}
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;And substitute into equation &lt;a href=&#34;#eq:step3b&#34;&gt;(2.21)&lt;/a&gt;:
&lt;span class=&#34;math display&#34; id=&#34;eq:step4&#34;&gt;\[\begin{align}
p(\tilde{y} | y ) &amp;amp;= \int \frac{ p( \tilde{y} | \theta ) p(\theta|y) p(y)   p(\theta)}{p(y)p(\theta)} d \theta  \\
                  &amp;amp;= \int p( \tilde{y} | \theta ) p(\theta|y) d \theta \tag{2.22} \\

\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;… and there we have it, the expression for the PPD, equation &lt;a href=&#34;#eq:PPD&#34;&gt;(2.1)&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;sampling-and-integration&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Sampling and Integration&lt;/h1&gt;
&lt;p&gt;This one came from &lt;a href=&#34;https://twitter.com/ChadScherrer&#34;&gt;Chad Scherrer&lt;/a&gt; who posted a &lt;a href=&#34;https://twitter.com/ChadScherrer/status/1292528021568552962?s=20&#34;&gt;tweet&lt;/a&gt; about the relationship between integration and sampling and I thought this was a really helpful heuristic.&lt;/p&gt;
&lt;p&gt;When I work through examples of Bayesian problems, my first thought is “&lt;strong&gt;how will I code this?&lt;/strong&gt;” and Chad’s tweet comes to mind, and helpfully, it follows nicely from the posterior predictive distribution example.&lt;/p&gt;
&lt;p&gt;To paraphrase Chad’s tweet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To sample from &lt;span class=&#34;math inline&#34;&gt;\(p(y|x)p(x)\)&lt;/span&gt; …
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Use the sample of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to sample &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Return &lt;span class=&#34;math inline&#34;&gt;\((x,y)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;To sample from &lt;span class=&#34;math inline&#34;&gt;\(\int p(y|x)p(x) dx\)&lt;/span&gt; …
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Sample &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Use the sample of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to sample &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Discard &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Return &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice then: we want to obtain samples from the &lt;em&gt;posterior predictive distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y}|y)\)&lt;/span&gt; i.e. for some new or unseen &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
p(\tilde{y} | y ) = \int p( \tilde{y} | \theta ) p(\theta|y) d \theta
\end{equation}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s how to proceed. We’ve used some Bayesian parameter estimation method (e.g. MCMC or similar) to obtain samples from &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt; and stored them as &lt;span class=&#34;math inline&#34;&gt;\(\Theta_S\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We have a function that returns a value &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}\)&lt;/span&gt; given an input &lt;span class=&#34;math inline&#34;&gt;\(\tilde{x}\)&lt;/span&gt; and parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y} = f(\tilde{x}; \theta)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For each sample &lt;span class=&#34;math inline&#34;&gt;\(\theta^{s} \in \Theta_S\)&lt;/span&gt; – one sample from &lt;span class=&#34;math inline&#34;&gt;\(p(\theta|y)\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;&lt;p&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}^s = f(\tilde{x}; \theta^{s})\)&lt;/span&gt; – a sample from &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde{y}|\theta)\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Store &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}^s\)&lt;/span&gt; and throw away &lt;span class=&#34;math inline&#34;&gt;\(\theta^{s}\)&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Our resulting collection of &lt;span class=&#34;math inline&#34;&gt;\(\tilde{y}^s\)&lt;/span&gt; are samples from the PPD from which we can then take expected values, quantiles etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;logits-probabilities-and-odds&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Logits, Probabilities and Odds&lt;/h1&gt;
&lt;p&gt;Can never remember these relations, so wrote them down explicitly for future reference.&lt;/p&gt;
&lt;div id=&#34;odds&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Odds&lt;/h2&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(p_A\)&lt;/span&gt; is the probability of event &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; then:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textrm{odds}_A = \frac{1}{(1-p_A)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;given the odds, we recover the probability as &lt;span class=&#34;math inline&#34;&gt;\(p_A = \frac{\textrm{odds}_A}{1+\textrm{odds}_A}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;log odds&lt;/strong&gt; are given by &lt;span class=&#34;math inline&#34;&gt;\(\ln(\textrm{odds}_A) = \ln \left( \frac{1}{1-p_A} \right) = \textrm{logit}(p_A) = \ln(p_A) - \ln(1-p_A)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, odds of 1.0 equals a probability &lt;span class=&#34;math inline&#34;&gt;\(p_A = 0.5\)&lt;/span&gt; – the probability of the event occurring is at chance level.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;odds-ratios&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Odds Ratios&lt;/h2&gt;
&lt;p&gt;For two events, &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; with probabilities &lt;span class=&#34;math inline&#34;&gt;\(p_A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_B\)&lt;/span&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\textrm{odds}_A = \frac{1}{(1-p_A)}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\textrm{odds}_B = \frac{1}{(1-p_B)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;odds ratio&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\textrm{OR}_{AB} = \frac{\textrm{odds}_A}{\textrm{odds}_B} = \frac{1-p_B}{1-p_A}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;logistic-regression&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Logistic Regression&lt;/h2&gt;
&lt;p&gt;In logistic regression applied to clinical situations, we are usually interested in a single “thing” associated with two discrete and mutually-exclusive events (e.g. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; = “dying” or &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; = “not dying”) – the thing either occurs or it does &lt;em&gt;not&lt;/em&gt; occur. In these circumstances &lt;span class=&#34;math inline&#34;&gt;\(p_B = 1 - p_A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To convert to the language of regression, denote an outcome &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; (for example, death, experiencing a side effect, obtaining a positive response to a treatment) then:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;the probability that &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; occurred is &lt;span class=&#34;math inline&#34;&gt;\(p_y\)&lt;/span&gt; (equivalent to &lt;span class=&#34;math inline&#34;&gt;\(p_A\)&lt;/span&gt; in the example above)&lt;/li&gt;
&lt;li&gt;the corresponding probability &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; &lt;strong&gt;does not&lt;/strong&gt; occur (&lt;span class=&#34;math inline&#34;&gt;\(¬y\)&lt;/span&gt;) is &lt;span class=&#34;math inline&#34;&gt;\(1-p_y\)&lt;/span&gt; (equivalent to &lt;span class=&#34;math inline&#34;&gt;\(p_B\)&lt;/span&gt; in the example above)&lt;/li&gt;
&lt;li&gt;the odds of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; occurring are &lt;span class=&#34;math inline&#34;&gt;\(\textrm{odds}_{y} = \frac{1}{(1-p_y)}\)&lt;/span&gt; and the odds of &lt;span class=&#34;math inline&#34;&gt;\(¬y\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(\textrm{odds}_{¬y} = \frac{1}{1-(1-p_y)} = \frac{1}{p_y}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;the odds ratio of &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(¬y\)&lt;/span&gt; is then &lt;span class=&#34;math inline&#34;&gt;\(\textrm{OR}_{(y,¬y)} = \frac{p_y}{1-p_y}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Further reading: Chapter 13 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gelman2020regression&#34; role=&#34;doc-biblioref&#34;&gt;Gelman, Hill, and Vehtari 2020&lt;/a&gt;)&lt;/span&gt; walks through all this with examples and code in R. On the web, Jay Rotella has a &lt;a href=&#34;https://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf&#34;&gt;nice PDF walk through&lt;/a&gt; of the same material with elaboration and graphical examples in R.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-blitzstein2014introduction&#34; class=&#34;csl-entry&#34;&gt;
Blitzstein, Joseph K, and Jessica Hwang. 2019. &lt;em&gt;Introduction to Probability&lt;/em&gt;. 2nd ed. Chapman; Hall/CRC.
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2020regression&#34; class=&#34;csl-entry&#34;&gt;
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. &lt;em&gt;Regression and Other Stories&lt;/em&gt;. Cambridge University Press.
&lt;/div&gt;
&lt;div id=&#34;ref-gelman2014bayesian&#34; class=&#34;csl-entry&#34;&gt;
Gelman, Andrew, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. &lt;em&gt;Bayesian Data Analysis&lt;/em&gt;. 3rd ed. Chapman; Hall/CRC.
&lt;/div&gt;
&lt;div id=&#34;ref-rubin1984bayesianly&#34; class=&#34;csl-entry&#34;&gt;
Rubin, Donald B. 1984. &lt;span&gt;“Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician.”&lt;/span&gt; &lt;em&gt;The Annals of Statistics&lt;/em&gt; 12 (4): 1151–72.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tensor Decompositions and Noisy Graphs</title>
      <link>/post/tensor-connectivity-matrices/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/tensor-connectivity-matrices/</guid>
      <description>
&lt;script src=&#34;/post/tensor-connectivity-matrices/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Here’s the problem set-up: we have a group of diseases/disorders/conditions (A,B,C and D) and associated known treatments (RxA, RxB, RxC and RxD). We want to build &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Definitions&#34;&gt;graphs&lt;/a&gt; that represent associations between the conditions and treatments for a group of patients. We are given time-ordered data (from their medical records) that records a sequence of recorded episodes (but not the precise dates of each) of both conditions and treatments. Ultimately, we want to discover patterns of common treatments and conditions to derive some understanding of multimorbidity.&lt;/p&gt;
&lt;p&gt;An example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tod-1.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The circles represent mentions of disorders and the squares treatment episodes. Here, we can see someone with a life-long condition (A) for which they had continuous treatment (or at least, at the resolution of recording, whenever A was mentioned, RxA was also). At some point, they develop an episode of condition B and on the second occurrence, they’re prescribed treatment RxB. And so on. We can see that we could represent each time-ordered series as a binary indicator vector, for instance condition B = (0,0,1,1,0,0,1,1,1) and treatment RxD = (0,0,0,0,0,1,1,0,0).&lt;/p&gt;
&lt;p&gt;We can construct a graph where nodes/vertices are the conditions and treatments, and the edges between represent relationships. In this example, A and RxA &lt;em&gt;always&lt;/em&gt; co-occur together so if an edge represents co-occurence, this would be a very strong association. Another method of constructing the graph is to systematically time-lag each condition/treatment vector and see if there’s a consistent pattern of e.g. condition B always preceding RxB. To do this, we use pairwise &lt;a href=&#34;https://en.wikipedia.org/wiki/Transfer_entropy&#34;&gt;transfer entropy&lt;/a&gt; (which generalises the notion of Granger temporality) for each pair of conditions and treatments.&lt;/p&gt;
&lt;p&gt;If we do this for the above time-ordered data, we get the following directed graph (note, squares are treatments and circles conditions):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-tod-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph can be parsimoniously represented as an adjacency matrix: rows and columns represent conditions/treatments and the &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt;th element is the strength of association (i.e. that condition/treatment &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; precedes condition/treatment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;). For the graph above, the adjacency matrix looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;conn-matrix-1.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Importantly, it’s an asymmetric directed graph, so the direction (as well as the strength) of associations matter. Ultimately, for this blogpost, it doesn’t really matter how the graph is constructed, just that we have a bunch of graphs represented as adjacency matrices.&lt;/p&gt;
&lt;p&gt;For this simulation, we built 4 example time-ordered sequences of conditions/treatments and then derived the adjacency matrices as before, resulting in the following 4 exemplar graphs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;four-exemplar-graphs.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then, we derive 25 noisy variations of each examplar, giving us 100 samples as follows:
&lt;img src=&#34;all-connect-matrix.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now assume that we are given the 100 noisy samples and we want to discover underlying graphs (which of course, in this simulated example, we know is 4 and we know what they look like). We can see glimpses of the original 4 exemplars in the table of samples, but if we take the “average” graph (adjacency matrix) we’d get this:
&lt;img src=&#34;avrg-connect-matrix.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_partition&#34;&gt;Graph partitioning&lt;/a&gt; methods try and tease out collections (or communities) of nodes/edges that form discrete graphs by selecting and cutting edges. One approach to this is spectral partitioning where first, we derive the Laplacian matrix (from the adjacency matrix) and perform eigen- or singular value decomposition (SVD) on it – to me, this seems like magic: you can carve up a graph (represented as an adjacency matrix) using linear algebra. I recently learned about non-negative matrix factorisation from a paper by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;Hassaine et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; on multimorbidity in clinical data which prompted me to read the original paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;Lee and Seung 1999&lt;/a&gt;)&lt;/span&gt; and a bit like SVD, seemed like more magic to me. Further, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;Hassaine et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; described using tensors – collections of matrices – for a similar task.&lt;/p&gt;
&lt;p&gt;So here we have our problem: given a bunch of 100 noisy adjacency matrices above, extract the underlying ‘exemplar’ or prototype graphs. The case presented here is not dissimilar to ideas in image processing – each matrix is a noisy image and we want to find some underlying latent structure. If each of the 100 noisy adjacency matrices is a sample, it seems logical to ‘stack’ them up and see what can be learned from this ‘stack.’ A series of matrices can be stacked up into a multidimensional array structure (a tensor) and there’s a literature on how to perform decomposition (akin to SVD) on tensors. So you can see how the logic proceeds: if matrix decompositions (like SVD) can can be used to partition a single graph (connectivity matrix), then maybe tensors can help tease out common structure in a stack of adjacency matrices.&lt;/p&gt;
&lt;div id=&#34;tensors&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Tensors&lt;/h1&gt;
&lt;p&gt;There are plenty of great online resources on &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor#Definition&#34;&gt;tensors&lt;/a&gt; and multilinear algerbra. One source that I found spectacularly helpful was &lt;a href=&#34;https://www.alexejgossmann.com/&#34;&gt;Alexej Gossman’s&lt;/a&gt; tutorial on Tucker and Rank-1 decompositions, not least because it serves as an introduction to programming with tensors in R. A frequently-cited and comprehensive tutorial paper is &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt; which contains the formal details alluded to here and from which we borrow notation. Much of what follows are notes to help me remember the core concepts.&lt;/p&gt;
&lt;p&gt;A tensor is essentially a multi-dimensional array – much the same as the notion of a multi-dimensional array of numbers in programmning languages – and generalises familiar objects like vectors and matrices.&lt;/p&gt;
&lt;p&gt;A tensor has an &lt;strong&gt;order&lt;/strong&gt; which equates the number of dimensions of the array.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;order one&lt;/strong&gt; tensor is a vector &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v} = \left[ v_1, v_2, \ldots, v_{I_1} \right]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt; elements (i.e. is of length &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt;). A vector of real numbers is denoted &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v} \in \mathbb{R}^{I_1}\)&lt;/span&gt; (and in what follows, all our tensors will contain real elements).&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;order two&lt;/strong&gt; tensor is a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{I_1 \times I_2}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(I_2\)&lt;/span&gt; columns.&lt;/p&gt;
&lt;p&gt;Before getting carried away, a simple motivating example. Take the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{2 \times 3}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE )
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want the first &lt;em&gt;row&lt;/em&gt; of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt; we’ll write this as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{1:}\)&lt;/span&gt; which we read as “set the row index &lt;span class=&#34;math inline&#34;&gt;\(i_1 = 1\)&lt;/span&gt; and retrieve &lt;em&gt;all&lt;/em&gt; columns” yielding (1, 2, 3).
Similarly, if we want the third &lt;em&gt;column&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt; we’ll write this as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{:3}\)&lt;/span&gt;, to be read as “set the column index &lt;span class=&#34;math inline&#34;&gt;\(i_2 = 3\)&lt;/span&gt; and retrieve all rows” yielding (3, 6).&lt;/p&gt;
&lt;div id=&#34;modes-and-fibres&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Modes and Fibres&lt;/h2&gt;
&lt;p&gt;Translating to the language of tensors; instead of discussing rows or columns, we generalise to &lt;strong&gt;fibres&lt;/strong&gt; and &lt;strong&gt;modes&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a matrix is a 2nd order tensor with indices &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;index &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; refers to the &lt;strong&gt;first mode&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;index &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt; refers to the &lt;strong&gt;second mode&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;mode-1 fibres&lt;/strong&gt; of the matrix above are the columns denoted by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{:j}\)&lt;/span&gt; which are (1, 4), (2, 5) and (3, 6),&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;mode-2 fibres&lt;/strong&gt; are the rows of the matrix above, denoted by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{i:}\)&lt;/span&gt; which are (1, 2, 3) and (4, 5, 6)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fibres of a tensor are obtained by ‘fixing’ all but one of the mode indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;third-order-tensors-and-slices&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Third Order Tensors and Slices&lt;/h2&gt;
&lt;p&gt;Now we come to &lt;strong&gt;third order&lt;/strong&gt; tensors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}} \in \mathbb{R}^{I_1 \times I_2 \times I_3}\)&lt;/span&gt; representing a “cuboid” of elements with three modes and indices ranging from &lt;span class=&#34;math inline&#34;&gt;\(i_1 = 1 \ldots I_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i_2 = 1 \ldots I_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_3 = 1 \ldots I_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s illustrate this by first of all defining two matrices of size &lt;span class=&#34;math inline&#34;&gt;\(2 \times 3\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X1 &amp;lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE )
X2 &amp;lt;- matrix( c(7,8,9,10,11,12), nrow = 2, ncol = 3, byrow = TRUE )
print(X1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(X2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, glue them together in the third mode with &lt;code&gt;X1&lt;/code&gt; at the front and &lt;code&gt;X2&lt;/code&gt; behind:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- as.tensor( abind( X1, X2, along = 3 ) )
print(X@data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By “stacking” &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; we’ve built the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}} \in \mathbb{R}^{2 \times 3 \times 2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This illustrates &lt;strong&gt;slices&lt;/strong&gt; – by fixing all the indices except two, we obtain a matrix that represents a ‘cut’ through the tensor in two of the modes. The &lt;strong&gt;frontal slices&lt;/strong&gt; are &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::k}\)&lt;/span&gt; and are simply the two matrices &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; in the code above. Note that we use the R package &lt;a href=&#34;https://cran.r-project.org/package=rTensor&#34;&gt;&lt;code&gt;rTensor&lt;/code&gt;&lt;/a&gt; which provides a class for tensor representations.&lt;/p&gt;
&lt;p&gt;We can slice the tensor on different modes, obtaining &lt;strong&gt;horizontal slices&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{i::}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[1,,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    7
## [2,]    2    8
## [3,]    3    9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[2,,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    4   10
## [2,]    5   11
## [3,]    6   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And &lt;strong&gt;lateral slices&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{:j:}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,1,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    7
## [2,]    4   10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,2,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    2    8
## [2,]    5   11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,3,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    3    9
## [2,]    6   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re not going to need more than 3rd order tensors for what follows, so we won’t generalise further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matricization-and-vectorization-of-tensors&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Matricization and Vectorization of Tensors&lt;/h2&gt;
&lt;p&gt;Order 3 tensors are cuboids and this is intutive. But if the order is higher, it becomes very hard to visualise, manipulate and define operations on tensors.&lt;/p&gt;
&lt;p&gt;One simple transformation is to &lt;strong&gt;vectorize&lt;/strong&gt; the tensor, which simply means ‘flattening’ the tensor into a vector following some convention for how elements are “read out” by systematically varying the indices with respect to each other (e.g. &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; varies slower than &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt;). We denote this operation &lt;span class=&#34;math inline&#34;&gt;\(\text{vec}(\mathcal{X})\)&lt;/span&gt; which gives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rTensor::vec(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  4  2  5  3  6  7 10  8 11  9 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this look like a vector of the column fibres of each frontal slice – i.e. (1, 4)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt;, (2, 5)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt;, (3, 6)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt; … concatenated together.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matricization&lt;/strong&gt; is the transforming of a tensor into a matrix represention by &lt;strong&gt;unfolding&lt;/strong&gt; the tensor along a mode. This helps us visualise as well as understand operations on tensors. The sources to understand this in detail are &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader&lt;/a&gt; (&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bader2006algorithm&#34; role=&#34;doc-biblioref&#34;&gt;Bader and Kolda&lt;/a&gt; (&lt;a href=&#34;#ref-bader2006algorithm&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;n-mode matricization&lt;/strong&gt; of a tensor yields a matrix with columns being the mode-n fibres of the tensor.&lt;/p&gt;
&lt;p&gt;Take our &lt;span class=&#34;math inline&#34;&gt;\(2 \times 3 \times 2\)&lt;/span&gt; tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we unfold the tensor along the &lt;em&gt;first&lt;/em&gt; mode we get the two frontal slices (matrices) &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::2}\)&lt;/span&gt; concatenated side-by-side as a matrix &lt;span class=&#34;math inline&#34;&gt;\(\left[ \mathbf{X}_{::1} \mid \mathbf{X}_{::2} \right]\)&lt;/span&gt; and we denote this with a bracketed subscript indicating the mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  2 6 
## Data: 
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    7    8    9
## [2,]    4    5    6   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if we unfold on the second mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(2)}\)&lt;/span&gt; we get the transposed frontal slices arranged side-by-side &lt;span class=&#34;math inline&#34;&gt;\(\left[ \mathbf{X}^{\intercal}_{::1} \mid \mathbf{X}^{\intercal}_{::2} \right]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  3 4 
## Data: 
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, unfolding in the third mode &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_{(3)}\)&lt;/span&gt; we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  2 6 
## Data: 
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    4    2    5    3    6
## [2,]    7   10    8   11    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is the matrix :
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{bmatrix}
    \text{vec}(\mathcal{X}_{::1})^T \\
    \text{vec}(\mathcal{X}_{::2})^T
  \end{bmatrix}
\]&lt;/span&gt;
That is, the first and second rows are the vectorization of the first and second frontal slices respectively:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind( vec(X[,,1]), vec(X[,,2] ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    4    2    5    3    6
## [2,]    7   10    8   11    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tensor-matrix-products&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Tensor-Matrix Products&lt;/h2&gt;
&lt;p&gt;An important operation is the product of a tensor and a matrix along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;th mode, or the &lt;strong&gt;n-mode product&lt;/strong&gt;. Restricting out attention to order 3 tensors (“cubes”) – we multiply the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}^{I_1 \times I_2 \times I_3}\)&lt;/span&gt; with a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{J \times I_n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is mode 1, 2 or 3.&lt;/p&gt;
&lt;p&gt;To my mind, the definition of the n-mode product is most easily understood in terms of matricization (unfolding), matrix products and the folding the result back into a tensor. The expression for the element-wise calculation is given in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt; pp. 460.&lt;/p&gt;
&lt;p&gt;For example, take the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q} \in \mathbb{R}^{3 \times 2}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- matrix( c(1,1,2,2,3,3), nrow = 3, ncol = 2, byrow = TRUE )
print(Q)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    1
## [2,]    2    2
## [3,]    3    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To multiply &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; by mode &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; we perform this sequence of operations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(n)}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; mode unfolding of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Compute the matrix product &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(n)} = \mathbf{Q} \mathbf{X}_{(n)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Fold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(n)}\)&lt;/span&gt; along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note, this only makes sense if the dimensions of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; are conformable with the unfolding of the tensor i.e. that the number of columns in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; equals &lt;span class=&#34;math inline&#34;&gt;\(I_n\)&lt;/span&gt;, the size of mode &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in the tensor&lt;/p&gt;
&lt;p&gt;Let’s do this for the first mode of our example. Here’s our tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(X@data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mode 1 of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; is of size &lt;span class=&#34;math inline&#34;&gt;\(I_1 = 2\)&lt;/span&gt; and our matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; has two columns, so we’re good.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Unfold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; along the first mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mode to take product with
n &amp;lt;- 1
X_1 &amp;lt;- k_unfold( X, n )@data
print(X_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    7    8    9
## [2,]    4    5    6   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the matrix product &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(1)} = \mathbf{Q} \mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_1_prime &amp;lt;- Q %*% X_1
print( X_1_prime )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    5    7    9   17   19   21
## [2,]   10   14   18   34   38   42
## [3,]   15   21   27   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(1)}\)&lt;/span&gt; along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th mode&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute the required modes for the resulting folded tensor
Q.dims &amp;lt;- dim(Q)
modes.X_prime &amp;lt;- X@modes
modes.X_prime[n] &amp;lt;- Q.dims[1]
# fold
Q_by_X.1 &amp;lt;- k_fold( X_1_prime, n, modes.X_prime )
print( Q_by_X.1@data )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    5    7    9
## [2,]   10   14   18
## [3,]   15   21   27
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   17   19   21
## [2,]   34   38   42
## [3,]   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Naturally, there’s a function to do this for us in one simple step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rTensor::ttm( X, Q, m = 1)@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    5    7    9
## [2,]   10   14   18
## [3,]   15   21   27
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   17   19   21
## [2,]   34   38   42
## [3,]   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-decompositions&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Matrix Decompositions&lt;/h1&gt;
&lt;p&gt;We can now venture into the decompositions, or factorizations, of matrices and tensors.&lt;/p&gt;
&lt;div id=&#34;singular-value-decomposition&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Singular Value Decomposition&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(314159)
A &amp;lt;- matrix(  c(1,0,0,1,
                1,0,0,1,
                1,1,1,1,
                1,0,0,1,
                1,0,0,1), ncol = 4, nrow = 5, byrow = TRUE)


A &amp;lt;- A + runif( 4*5, -0.1, 0.3)

UDV &amp;lt;- svd(A)
U &amp;lt;- UDV$u
D &amp;lt;- diag(UDV$d)
V &amp;lt;- UDV$v

recon.4 &amp;lt;- U[,1:4] %*% D[1:4,1:4] %*% t(V[,1:4])
recon.2 &amp;lt;- U[,1:2] %*% D[1:2,1:2] %*% t(V[,1:2])

par(mfrow=c(1,3),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(A), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Original&amp;quot; )
    image( t(recon.4), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;k=4&amp;quot; )
    image( t(recon.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;k=2&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;
This diagram shows on the left, an original image (a sort of noisy “H” shape) represented by a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{5 \times 4}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We subject this to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular valued decomposition&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbf{M} = \mathbf{UDV}^{\intercal}
\]&lt;/span&gt;
where the factor matrices are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the diagonal matrix of singular values, with largest value in the top left, descending to the smallest along the diagonal:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(D)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]      [,3]      [,4]
## [1,] 3.504158 0.000000 0.0000000 0.0000000
## [2,] 0.000000 1.323071 0.0000000 0.0000000
## [3,] 0.000000 0.000000 0.2577888 0.0000000
## [4,] 0.000000 0.000000 0.0000000 0.1509413&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is a matrix of &lt;span class=&#34;math inline&#34;&gt;\(5 \times 4\)&lt;/span&gt; left singular vectors of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(U)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]         [,3]        [,4]
## [1,] -0.4365701 -0.2339371  0.813531383 -0.01200304
## [2,] -0.4120556 -0.2359644 -0.512436719  0.37115595
## [3,] -0.5058560  0.8584828 -0.003890147  0.06573130
## [4,] -0.3961206 -0.1703899 -0.263178631 -0.86250248
## [5,] -0.4762509 -0.3515234 -0.079354732  0.33744341&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is a matrix of &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; right singular vectors of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]        [,3]        [,4]
## [1,] -0.6951074 -0.2107143 -0.68594850 -0.04358657
## [2,] -0.1943603  0.5802217 -0.03149829  0.79030037
## [3,] -0.2231139  0.7589987  0.03175303 -0.61084605
## [4,] -0.6551869 -0.2070343  0.72627423  0.01981508&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now ‘reconstruct’ an approximation to the original image &lt;span class=&#34;math inline&#34;&gt;\(\widetilde{\mathbf{M}} = \mathbf{UDV^{\intercal}}\)&lt;/span&gt; and this is shown in the middle picture with &lt;span class=&#34;math inline&#34;&gt;\(k = 4\)&lt;/span&gt; denoting that we use all 4 of the singular vectors.&lt;/p&gt;
&lt;p&gt;The “magic” alluded to earlier is when we use a &lt;strong&gt;truncated&lt;/strong&gt; version of the SVD – take the first &lt;span class=&#34;math inline&#34;&gt;\(1 \ldots k\)&lt;/span&gt; singular vectors and use these to reconstruct the image:
&lt;span class=&#34;math display&#34;&gt;\[
  \widetilde{\mathbf{M}} = \mathbf{U}_{:k} \mathbf{D}_{kk} \mathbf{V}^{\intercal}_{:k}
\]&lt;/span&gt;
With &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;, we get the third image on the right; a reconstructed image using half the vectors (i.e. compressed) which additionally helps de-noise the original image.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-negative-matrix-factorisation&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Non-Negative Matrix Factorisation&lt;/h2&gt;
&lt;p&gt;Another decomposition (factorisation) is the &lt;strong&gt;non-negative&lt;/strong&gt; matrix factorisation or NMF (see &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;Lee and Seung&lt;/a&gt; (&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt; for details). Here, we factor:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbf{M} = \mathbf{WH}^{\intercal}
\]&lt;/span&gt;
subject to the constraint that the factor matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt; are non-negative (cf. the matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{U}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}\)&lt;/span&gt; in SVD as shown above). This results in an algorithm for estimating the factor matrices which is iterative for each element of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt;. The remarkable feature of NMF is that the resulting factor matrices have a component-parts interpretation as follows; the columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt; are &lt;strong&gt;encodings&lt;/strong&gt; and columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; are &lt;strong&gt;weights&lt;/strong&gt; that map encodings to weighted-sums in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This means, we can use a truncated NMF decomposition to partition “component parts” of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As an example, with the same noisy “H” image, we apply NMF truncating to &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt; and reconstruct:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nmf.1 &amp;lt;- NMF(A, J=2)
W &amp;lt;- nmf.1$U
H &amp;lt;- nmf.1$V

recon.nmf &amp;lt;- W %*% t(H)


par(mfrow=c(1,2),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(A), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Original&amp;quot; )
    image( t(recon.nmf), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;NMF (k = 2)&amp;quot; )    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result is very similar to SVD. The “magic” here is in the following trick – take each column of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}_{:i}\)&lt;/span&gt; and multiply it by the corresponding column of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}^{\intercal}_{:i}\)&lt;/span&gt; and we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recon.nmf.1 &amp;lt;- W[,1] %*% t(H[,1])
recon.nmf.2 &amp;lt;- W[,2] %*% t(H[,2])
recon.nmf.sum &amp;lt;- recon.nmf.1 + recon.nmf.2

par(mfrow=c(1,3),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(recon.nmf.1), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (1st column)&amp;quot; )    
    image( t(recon.nmf.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (2nd column)&amp;quot; )
    image( t(recon.nmf.sum), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Sum&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;
So we see that the columns of the factor matrices capture component parts of the source matrix and reconstruction is the sum of those component parts. This is a direct consequence of imposing the constraint that the elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} = \mathbf{WH}^{\intercal}\)&lt;/span&gt; must be non-negative.&lt;/p&gt;
&lt;p&gt;We can repeat the experiment with &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(314150)
nmf.2 &amp;lt;- NMF(A, J=3,  algorithm = &amp;quot;Frobenius&amp;quot;)
W &amp;lt;- nmf.2$U
H &amp;lt;- nmf.2$V

recon.nmf &amp;lt;- W %*% t(H)

recon.nmf.1 &amp;lt;- W[,1] %*% t(H[,1])
recon.nmf.2 &amp;lt;- W[,2] %*% t(H[,2])
recon.nmf.3 &amp;lt;- W[,3] %*% t(H[,3])
recon.nmf.sum &amp;lt;- recon.nmf.1 + recon.nmf.2 + recon.nmf.3

par(mfrow=c(1,4),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)

    image( t(recon.nmf.1), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (1st column)&amp;quot; )    
    image( t(recon.nmf.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (2nd column)&amp;quot; )
    image( t(recon.nmf.3), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (3rd column)&amp;quot; )
    image( t(recon.nmf.sum), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Sum&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;
Notice now that the three components are the right and left vertical ‘bars’ and the one horizontal bar. The problem is, with &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt; the solution found by NMF is stable (i.e. it always extracts a component with the two vertical bars, and one hortizontal) but with &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt; the solution varied significantly between runs of the iterative NMF algorithm; sometimes local optima are found which make sense, other times we get less meaningful decompositions. These variable results are not shown here because we fix the pseudo-random number generator seed for reproducibility.&lt;/p&gt;
&lt;p&gt;One of the problems with applying NMF is finding the right &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that reflects the desired component-parts decomposition in the problem domain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tensor-decomposition&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Tensor Decomposition&lt;/h1&gt;
&lt;p&gt;For a survey of tensor decomposition techniques, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader&lt;/a&gt; (&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; is the go-to source. Here, we’ll focus on one decomposition that mirrors NMF for tensors, the &lt;strong&gt;non-negative Tucker decomposition&lt;/strong&gt; (NTD) for mode-3 tensors. Instead of having a target matrix, decomposed by multiplying factor matrices together, we have a more complex problem of factorising a cuboid. Recall our example: 100 noisy samples derived from 4 ‘prototypical’ graph adjacency matrices.&lt;/p&gt;
&lt;p&gt;With tensors, the intuitions from matrix decomposition apply, but instead of dealing with products of matrices, we now have a more complex situation shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tucker-schematic.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Tucker decomposition is then:
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{X} = \mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; is the tensor of dimensions &lt;span class=&#34;math inline&#34;&gt;\(I_1 \times I_2 \times I_3\)&lt;/span&gt; which for our application represents the &lt;span class=&#34;math inline&#34;&gt;\(8 \times 8\)&lt;/span&gt; adjacency matrices stacked in the third mode to build a tensor of size &lt;span class=&#34;math inline&#34;&gt;\(8 \times 8 \times 100\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; is the &lt;strong&gt;core tensor&lt;/strong&gt; which has dimensions &lt;span class=&#34;math inline&#34;&gt;\(R_1 \times R_2 \times R_3\)&lt;/span&gt; and is smaller than the dimensions of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; where each &lt;span class=&#34;math inline&#34;&gt;\(R_j \leq I_j\)&lt;/span&gt; – therefore, the core tensor “compresses” the information in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are the &lt;strong&gt;factor matrices&lt;/strong&gt; which have dimensions &lt;span class=&#34;math inline&#34;&gt;\(I_1 \times R_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(I_2 \times R_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_3 \times R_3\)&lt;/span&gt; respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By analogy with principal components analysis, the factor matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are the principal components in each of the modes and the core tensor represents the interactions between the components &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The iterative algorithm for computing the factor matrices and core tensor subject to non-negativity constraints can be found in in Chapter 7 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cichocki2009nonnegative&#34; role=&#34;doc-biblioref&#34;&gt;Cichocki et al. 2009&lt;/a&gt;)&lt;/span&gt; and connections to latent structure and statistics can be found in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-shashua2005non&#34; role=&#34;doc-biblioref&#34;&gt;Shashua and Hazan 2005&lt;/a&gt;)&lt;/span&gt;. Here, we use the &lt;a href=&#34;https://cran.r-project.org/package=nnTensor&#34;&gt;&lt;code&gt;nnTensor&lt;/code&gt;&lt;/a&gt; package implementation for NTD.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Application&lt;/h1&gt;
&lt;p&gt;Let’s return to the example we started with, recovering adjacency matrices from 100 noisy samples.&lt;/p&gt;
&lt;p&gt;The code for what follows is computationally expensive (so I’ve not shown the code here)&lt;/p&gt;
&lt;div id=&#34;finding-an-appropriate-rank-for-the-core-tensor&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Finding an Appropriate Rank for the Core Tensor&lt;/h2&gt;
&lt;p&gt;The example of NMF on the “H” image above demonstrates that knowing &lt;em&gt;in advance&lt;/em&gt; what the expected components are helps decide on the truncation (or reduced rank) to use. In the tensor example of 100 adjacency matrices, assume we don’t know that there are four prototypical adjacency matrices in advance. We need to decide on the size of the core tensor (and the corresponding factor matrices) without this information.&lt;/p&gt;
&lt;p&gt;Recall also that the core tensor is a compressed representation of the data in each of the three modes.&lt;/p&gt;
&lt;p&gt;Here’s the approach; we use NTD to obtain a compressed representation and treat the factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; (corresponding to the third mode of the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;) as a projection (i.e. dimensionality reduction) to locate a candidate number of clusters for all 100 samples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute a low-rank NTD with &lt;span class=&#34;math inline&#34;&gt;\(R_1 = R_2 = 8\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_3 = 3\)&lt;/span&gt; – so, compressing over the third mode (the ‘stacking’ of the adjacency matrices) but leaving the modes corresponding to the adjacency matrix rows/columns uncompressed. Reducing on &lt;span class=&#34;math inline&#34;&gt;\(R_3 = 3\)&lt;/span&gt; is somewhat arbitrary, but allows us to visualise the resulting factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; in Step 2&lt;/li&gt;
&lt;li&gt;Take the factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C} \in \mathbb{R}^{100 \times 3}\)&lt;/span&gt; which represents each of the 100 adjacency matrices projected in a space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^3\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Perform &lt;em&gt;k&lt;/em&gt;-means clustering in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^3\)&lt;/span&gt; with multiple re-starts over a range of candidate cluster numbers (e.g. 2 through to 10 clusters); for each number of clusters, use the Gap statistic to ascertain the optimal number of clusters &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-tibshirani2001estimating&#34; role=&#34;doc-biblioref&#34;&gt;Tibshirani, Walther, and Hastie 2001&lt;/a&gt;)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Repeat steps 1–3 a number of times (say, 20) recording the optimum number of clusters located in Step 3&lt;/li&gt;
&lt;li&gt;Take the mode number of clusters &lt;span class=&#34;math inline&#34;&gt;\(\#c\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We repeat steps 1–3 because (as for NMF) NTD uses an iterative algorithm not guaranteed to find a global optimum and often, NTD locates quite different factorisations for different initialisations.&lt;/p&gt;
&lt;p&gt;On our example data, here’s what we obtain:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;r3-clusters.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each plot is one of three projections of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; (it looks a lot cooler as a 3D plot but it’s harder to see the clusters clearly). So we can locate 4 distinct clusters, each corresponding to one of the ‘prototype’ network graphs from earlier:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;four-exemplar-graphs.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bader2006algorithm&#34; class=&#34;csl-entry&#34;&gt;
Bader, Brett W, and Tamara G Kolda. 2006. &lt;span&gt;“Algorithm 862: MATLAB Tensor Classes for Fast Algorithm Prototyping.”&lt;/span&gt; &lt;em&gt;ACM Transactions on Mathematical Software (TOMS)&lt;/em&gt; 32 (4): 635–53.
&lt;/div&gt;
&lt;div id=&#34;ref-cichocki2009nonnegative&#34; class=&#34;csl-entry&#34;&gt;
Cichocki, Andrzej, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. 2009. &lt;em&gt;Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-hassaine2020untangling&#34; class=&#34;csl-entry&#34;&gt;
Hassaine, Abdelaali, Gholamreza Salimi-Khorshidi, Dexter Canoy, and Kazem Rahimi. 2020. &lt;span&gt;“Untangling the Complexity of Multimorbidity with Machine Learning.”&lt;/span&gt; &lt;em&gt;Mechanisms of Ageing and Development&lt;/em&gt; 190: 111325. &lt;a href=&#34;https://doi.org/10.1016/j.mad.2020.111325&#34;&gt;https://doi.org/10.1016/j.mad.2020.111325&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kolda2009tensor&#34; class=&#34;csl-entry&#34;&gt;
Kolda, Tamara G, and Brett W Bader. 2009. &lt;span&gt;“Tensor Decompositions and Applications.”&lt;/span&gt; &lt;em&gt;SIAM Review&lt;/em&gt; 51 (3): 455–500.
&lt;/div&gt;
&lt;div id=&#34;ref-lee1999learning&#34; class=&#34;csl-entry&#34;&gt;
Lee, Daniel D, and H Sebastian Seung. 1999. &lt;span&gt;“Learning the Parts of Objects by Non-Negative Matrix Factorization.”&lt;/span&gt; &lt;em&gt;Nature&lt;/em&gt; 401 (6755): 788–91.
&lt;/div&gt;
&lt;div id=&#34;ref-shashua2005non&#34; class=&#34;csl-entry&#34;&gt;
Shashua, Amnon, and Tamir Hazan. 2005. &lt;span&gt;“Non-Negative Tensor Factorization with Applications to Statistics and Computer Vision.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 22nd International Conference on Machine Learning&lt;/em&gt;, 792–99.
&lt;/div&gt;
&lt;div id=&#34;ref-tibshirani2001estimating&#34; class=&#34;csl-entry&#34;&gt;
Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. &lt;span&gt;“Estimating the Number of Clusters in a Data Set via the Gap Statistic.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series B (Statistical Methodology)&lt;/em&gt; 63 (2): 411–23.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sequential Clinical Decision Making</title>
      <link>/post/2021-05-23-sequential-triage/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/2021-05-23-sequential-triage/</guid>
      <description>
&lt;script src=&#34;/post/2021-05-23-sequential-triage/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/2021-05-23-sequential-triage/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/2021-05-23-sequential-triage/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;In clinical decision making for serious but rare events, there has been discussion about how to use predictive models as tools in decision making.&lt;/p&gt;
&lt;p&gt;One example is in decision making for assessment and treatment in people at risk of suicide. A systematic review &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kessler2019suicide&#34; role=&#34;doc-biblioref&#34;&gt;Kessler et al.&lt;/a&gt; (&lt;a href=&#34;#ref-kessler2019suicide&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; of “suicide prediction models” cites the very real concern that the low positive predictive value (PPV) of current state-of-the-art models renders them at least clinically useless and at worst obviously dangerous. Most published models, however, attempt to predict the absolute risk of suicide based on some feature data (i.e. covariates, independent variables or predictors) for individuals – that is, these models attempt to identify people at risk of suicide. A central tennet of Kessler &lt;em&gt;et al&lt;/em&gt;’s argument is that &lt;strong&gt;net benefit&lt;/strong&gt; – rather than positive predictive value – is the appropriate decision-theoretic framework and in effect, predictive models might be better used as tools for screening out cases (of course, their argument and analysis is far more detailed but this is what I’m focusing on here). Kessler &lt;em&gt;et al&lt;/em&gt; describe how to improve the clinical utility of suicide prediction models by embedding them in a clinical triaging system and using thresholds for intervening (or not) derived from &lt;a href=&#34;https://www.mskcc.org/departments/epidemiology-biostatistics/biostatistics/decision-curve-analysis&#34;&gt;decision curve analysis&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;Vickers, Van Calster, and Steyerberg&lt;/a&gt; (&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Kessler &lt;em&gt;et al&lt;/em&gt;’s proposal is that if there is a high prevalence of &lt;em&gt;negative&lt;/em&gt; cases in routine clinical practice, then such a staged triaging system would enable scarce (and often, intrusive) clinical resources to be directed towards cases which are uncertain. In this post, we consider positive and negative predictive value, net benefit as well as examining a sequential triage model of clinical decision support.&lt;/p&gt;
&lt;div id=&#34;predictive-values&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Predictive Values&lt;/h1&gt;
&lt;p&gt;With an assumed representative sample of a population, let &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be the output of the decision rule/system, and &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; be whether or not the event occurred:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = 1\)&lt;/span&gt; represents the decision that a case is &lt;strong&gt;positive&lt;/strong&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Y = 0\)&lt;/span&gt; represents a &lt;strong&gt;negative&lt;/strong&gt; decision&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E = 1\)&lt;/span&gt; represents the serious event &lt;strong&gt;occuring&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E = 0\)&lt;/span&gt; that it did not&lt;/li&gt;
&lt;/ul&gt;
Consider the following hypothetical confusion matrix for a decision system on a representative validation sample of 1000 people:
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are 20 serious events in 1000 cases. The model gives a correct decision for 900/980 negative events (true negatives, TN) and decides that 80 negative cases are in fact positve (false positives; FP). The model performs poorly on decisions with positive cases; it decides 10/20 positive events are positive (true positives, TP) and makes the potentially catastrophic decision that 10/20 positives are in fact negatives (false negatives, FN).&lt;/p&gt;
&lt;p&gt;So, we find that :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity = &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 1 \mid E = 1)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt; = 0.5; the probability that the decision was positive, given the event was positive&lt;/li&gt;
&lt;li&gt;Specificity = &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 0 \mid E = 0)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt; = 0.918; the probability that the decision was negative, given the event was negative&lt;/li&gt;
&lt;li&gt;The prevalence of the serious event is 0.02&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As noted in the Altman and Bland classic &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-altman-pred-values1994&#34; role=&#34;doc-biblioref&#34;&gt;Altman and Bland 1994&lt;/a&gt;)&lt;/span&gt;, “the whole point of a diagnostic test is to use it to make a diagnosis, so we need to know the probability that the test will give the correct diagnosis” and sensitivity and specificity of the test (here, the decision rule) aren’t sufficient.&lt;/p&gt;
&lt;p&gt;The important point is, in a clinical situation, we are interested in the conditional probabilities &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E \mid Y)\)&lt;/span&gt; (the probability of the event given the decision rule output).&lt;/p&gt;
&lt;p&gt;However, we only have the conditionals &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y \mid E )\)&lt;/span&gt; and conditional probabilities do not commute so &lt;span class=&#34;math inline&#34;&gt;\(\Pr( Y \mid E) \neq \Pr( E \mid Y)\)&lt;/span&gt;. Failure to recognise this difference is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy&#34;&gt;prosecutor’s fallacy&lt;/a&gt; or the fallacy of the &lt;a href=&#34;https://rationalwiki.org/wiki/Confusion_of_the_inverse&#34;&gt;transposed conditional&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2019introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; Chapter 2.8.&lt;/p&gt;
&lt;p&gt;We will need to enumerate the probabilities of other conditions (i.e. states of &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 1 \mid E = 0)\)&lt;/span&gt; is the false positive rate, or 1-specificity = &lt;span class=&#34;math inline&#34;&gt;\(1-\Pr(Y=0 \mid E=0)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(FP/(FP+TN)\)&lt;/span&gt; = 0.082&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 0 \mid E = 1)\)&lt;/span&gt; is the false negative rate, or 1-sensitivity = &lt;span class=&#34;math inline&#34;&gt;\(1-\Pr(Y=1 \mid E=1)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(FN/(FN+TP)\)&lt;/span&gt; = 0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;deriving-ppv-and-npv&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Deriving PPV and NPV&lt;/h2&gt;
&lt;p&gt;The definition of &lt;a href=&#34;https://en.wikipedia.org/wiki/Conditional_probability&#34;&gt;conditional probability&lt;/a&gt; means that, for the conditions &lt;strong&gt;we want&lt;/strong&gt;, we can state:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) = \frac{\Pr(E,Y)}{\Pr(Y)}
\]&lt;/span&gt;
Or, by rearranging:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E,Y) = \Pr(E \mid Y) \Pr(Y)
\]&lt;/span&gt;
Applying the same argument for the conditionals &lt;strong&gt;we have&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y \mid E)\)&lt;/span&gt; (sensitivity and specificity):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(Y,E) = \Pr(Y \mid E) \Pr(E)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;joint probability&lt;/strong&gt; of two events are commutative (unlike conditionals) therefore &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E,Y) = \Pr(Y,E)\)&lt;/span&gt; and we can equate:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) \Pr(Y) = \Pr(Y \mid E) \Pr(E)
\]&lt;/span&gt;
Noting again that we are interested in &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E \mid Y)\)&lt;/span&gt; we can solve:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) = \frac{\Pr(Y \mid E) \Pr(E)}{\Pr(Y)}
\]&lt;/span&gt;
This is Bayes formula.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-calc-PPV-NPV&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Example Calculation&lt;/h2&gt;
&lt;p&gt;Using our example above, here’s want we want, and what we have available:
&lt;span class=&#34;math display&#34;&gt;\[
\Pr(E=1 \mid Y=1) = \frac{ \overbrace{\Pr( Y=1 \mid E=1)}^\text{sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} } 
                    { \underbrace{\Pr(Y=1)}_\text{prob. of +ve decision} }
\]&lt;/span&gt;
We can calculate the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=1)\)&lt;/span&gt;, the unconditional probability of a positive decision, using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_probability&#34;&gt;law of total probability&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Pr(Y=1) =&amp;amp; \overbrace{\Pr( Y=1 \mid E=1)}^\text{sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} + \\
         &amp;amp;\underbrace{\Pr( Y=1 \mid E=0)}_\text{1-specificity} \underbrace{\Pr(E=0)}_\text{1-prevalence}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The calculation step-by-step is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=1) = 0.5 \times 0.02 + (1-0.918) \times (1-0.02) = 0.09\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Substitute sensitivity and prevalence in the numerator:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E=1 \mid Y=1) = \frac{ \overbrace{0.5}^\text{sensitivity} \times \overbrace{0.02}^\text{prevalence} } 
                    { \underbrace{0.09}_\text{prob. of +ve decision} } = 0.11
  \]&lt;/span&gt;
Which delivers the &lt;strong&gt;positive predictive value&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can similarly derive the &lt;strong&gt;negative predictive value&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Pr(E=0 \mid Y=0) = \frac{ \overbrace{\Pr( Y=0 \mid E=0)}^\text{specificity} \overbrace{\Pr(E=0)}^\text{1-prevalence} } 
                    { \underbrace{\Pr(Y=0)}_\text{prob. of -ve decision} }
\]&lt;/span&gt;
And our denominator in this case:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \Pr(Y=0) =&amp;amp; \overbrace{\Pr( Y=0 \mid E=1)}^\text{1-sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} + \\
            &amp;amp; \underbrace{\Pr( Y=0 \mid E=0)}_\text{specificity} \underbrace{\Pr(E=0)}_\text{1-prevalence}
\end{aligned}
\]&lt;/span&gt;
Plugging in the numbers:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=0) = (1-0.5) \times 0.02 + 0.918 \times (1-0.02) = 0.91\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Substitute specificity and prevalence in the numerator:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E=0 \mid Y=0) = \frac{ \overbrace{0.918}^\text{specificity} \times \overbrace{0.98}^\text{1-prevalence} } 
                    { \underbrace{0.91}_\text{prob. of -ve decision} } = 0.987
  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This hypothetical decision system is useful for correctly deciding on negative cases, but performs poorly on identifying positive cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Simulation&lt;/h1&gt;
&lt;p&gt;Now suppose that we have two (or more) clinical tests to help identify patients at risk for a relatively rare but serious event; for example, &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is a relatively cheap and easy-to-administer instrument or questionnaire. &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; is a semi-structured interview or clinical examination which is time consuming, requires expertise to administer and is therefore significantly more costly than &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Further, we have a development sample of 5000 people for which we have the results for &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; and we know who in this sample experienced the serious event.&lt;/p&gt;
&lt;p&gt;We next build a model that attempts to predict the rare, serious event (&lt;span class=&#34;math inline&#34;&gt;\(Y = 1\)&lt;/span&gt;) on the basis of a patient’s &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; results and denote this &lt;span class=&#34;math inline&#34;&gt;\(y = F_{X_1}(x)\)&lt;/span&gt;. Note, no claim is made that this model is well designed.&lt;/p&gt;
&lt;p&gt;Assume the somewhat luxurious position that we have a further 5000 validation cases from the same population – so we can examine the model’s performance on data it was not ‘trained’ on.&lt;/p&gt;
&lt;p&gt;Let’s look at the calibration of the model on the validation sample:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s important to note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; delivers a prediction in the form of a continuous estimate of the absolute probability of the serious event &lt;em&gt;given&lt;/em&gt; the screening instrument &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;there is &lt;strong&gt;no decision rule&lt;/strong&gt; here; so we can’t discuss PPV or NPV&lt;/li&gt;
&lt;li&gt;the model is poorly calibrated: which is unsurprising given the serious event is rare – in the 5000 validation samples there were 240 serious events (&lt;span class=&#34;math inline&#34;&gt;\(E = 1\)&lt;/span&gt;) representing a small prevalence of 0.048&lt;/li&gt;
&lt;li&gt;the model appears to &lt;strong&gt;under estimate&lt;/strong&gt; the probability of a serious event; for example, if the model predicts a probability of a serious event of 0.25, the actual probability is closer to 0.5.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can repeat the same analysis for the other, more costly instrument &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;; as for the cheaper instrument, we train a model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; and then we have access to a validation sample on which we can examine the calibration:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Again, not great calibration.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-decision-rules&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Decision Rules&lt;/h1&gt;
&lt;p&gt;Returning to the idea that Kessler &lt;em&gt;et al&lt;/em&gt; discussed, how can we design a decision rule that makes use of these two tests ?&lt;/p&gt;
&lt;div id=&#34;sec-ROC-curve&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; ROC Curves&lt;/h2&gt;
&lt;p&gt;A common approach to designing a decision rule is to vary a threshold over the output of &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and plot the ROC curve; then, find an “optimal” threshold that maximises the sensitivity/specificity tradeoff.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Setting levels: control = 0, case = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting direction: controls &amp;lt; cases&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The confusion matrix for the decision rule with the threshold = 0.074 shown in the ROC curve above is:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
3953
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
807
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
196
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The usual measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity = 0.8167&lt;/li&gt;
&lt;li&gt;Specificity = 0.8305&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can compute the &lt;em&gt;more&lt;/em&gt; clinically relevant probabilties as follows (as for Section &lt;a href=&#34;#sec-calc-PPV-NPV&#34;&gt;1.2&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Positive predictive value: &lt;span class=&#34;math inline&#34;&gt;\(\Pr( E = 1 \mid Y = 1 )\)&lt;/span&gt; = 0.195&lt;/li&gt;
&lt;li&gt;Negative predictive value: &lt;span class=&#34;math inline&#34;&gt;\(\Pr( E = 0 \mid Y = 0 )\)&lt;/span&gt; = 0.989&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Critically, however, false negatives (44) are catastrophic here because the event, although rare, is serious (i.e. the death of a patient); but &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is correctly identifying a high number (3953) of negative cases correctly.&lt;/p&gt;
&lt;p&gt;As discussed &lt;a href=&#34;https://danwjoyce.netlify.app/post/loss-functions-and-posteriors/&#34;&gt;here&lt;/a&gt; and more persuasively &lt;a href=&#34;https://www.fharrell.com/post/backwards-probs/&#34;&gt;here&lt;/a&gt; sensitivity and specificity do not take account of the loss or utility of the decision and neither do PPV and NPV.&lt;/p&gt;
&lt;p&gt;To understand why this neglect of utility (or loss) is important, take the above confusion matrix and then assume the decision rule declares one additional &lt;strong&gt;false negative&lt;/strong&gt;, so that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of &lt;strong&gt;false negatives&lt;/strong&gt; (FN) = 45&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;actual&lt;/em&gt; number of positive events is of course, unchanged at 240&lt;/li&gt;
&lt;li&gt;so conversely, the &lt;strong&gt;true positive&lt;/strong&gt; rate drops by one so TP = 195&lt;/li&gt;
&lt;li&gt;the revised &lt;strong&gt;sensitivity&lt;/strong&gt; is then 0.8125 – a decrease in decision rule performance of 0.0042&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reverse the experiment, so that the decision rule improves marginally and declares one additional &lt;strong&gt;true positive&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of &lt;strong&gt;true positives&lt;/strong&gt; increases by one, TP = 197&lt;/li&gt;
&lt;li&gt;the number of &lt;strong&gt;false negatives&lt;/strong&gt; decreases by one, FN = 43&lt;/li&gt;
&lt;li&gt;the revised &lt;strong&gt;sensitivity&lt;/strong&gt; is then 0.8208 – an increase in decision rule performance of 0.0041&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The change in performance (the “score”) for one additional correct or incorrectly classified positive case is symmetric and of the order &lt;span class=&#34;math inline&#34;&gt;\(1/N\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size.&lt;/p&gt;
&lt;p&gt;Clearly, an &lt;strong&gt;additional false negative&lt;/strong&gt; should penalise the overall performance score &lt;em&gt;differently&lt;/em&gt; than the reward for an additional true positive.&lt;/p&gt;
&lt;p&gt;Optimising the threshold (decision rule) by maximising the sensitivity-specificity tradeoff (e.g. using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&#34;&gt;Yourdon J statistic&lt;/a&gt;) is not the only method of choosing the threshold and we might for example, choose a decision rule that favours performance of different cells of the confusion matrix. A decision theoretic framework like &lt;strong&gt;net benefit&lt;/strong&gt; allows one systematic treatment.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-curve-analysis&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Decision Curve Analysis&lt;/h1&gt;
&lt;p&gt;Here, we are trying to implement a decision rule whereby a patient is triaged to a more costly “test” (&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;) on the basis of a more available or less costly test &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can adopt a decision-theoretic approach (see previous posts &lt;a href=&#34;https://danwjoyce.netlify.app/post/decisions-and-loss-functions/&#34;&gt;here&lt;/a&gt;) and design a loss (conversely, a utility) function for a decision rule (threshold).&lt;/p&gt;
&lt;p&gt;Assume that we continue to insist on a “hard” decision rule that decides, on the basis of &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;, whether to further investigate (triage to &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;) or, decide that the serious event is unlikely so no further follow-up is necessary and the patient can be discharged.&lt;/p&gt;
&lt;p&gt;In this situation, we can construct the confusion matrix below:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
TN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FN
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
FP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TP
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And then assign a loss to each cell e.g. the loss for a true negative is &lt;span class=&#34;math inline&#34;&gt;\(L_{TN}\)&lt;/span&gt;, for a false negative &lt;span class=&#34;math inline&#34;&gt;\(L_{FN}\)&lt;/span&gt; and so on.&lt;/p&gt;
&lt;p&gt;For a given decision rule (here, the decision rule can be equated with the threshold value &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;) we can compute the &lt;strong&gt;expected loss&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
  L( \tau ) = \frac{1}{N} \left( \#TN \cdot L_{TN} + \#FN \cdot L_{FN} + \#FP \cdot L_{FP} + \#TP \cdot L_{TP} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\#TN\)&lt;/span&gt; is the number of true negatives under the decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; etc. and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size. We then systematically vary &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and choose our final decision rule on the basis of minimum loss.&lt;/p&gt;
&lt;p&gt;The difficulty is that it is often hard to quantify losses (or value, utility) either absolutely or relatively for each cell of the confusion matrix – the example of Kessler &lt;em&gt;et al&lt;/em&gt; examines predictive models for suicide, where a false negative would be catastrophic; is the loss incurred for a false negative 10, 100 or 1000 times ‘worse’ than a true negative ?&lt;/p&gt;
&lt;p&gt;An alternative, proposed by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;Vickers, Van Calster, and Steyerberg&lt;/a&gt; (&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, is to first set up a decision tree representing decisions to intervene / not intervene for the combinations shown in the standard confusion matrix. We then assume that the loss of &lt;strong&gt;not intervening&lt;/strong&gt; when we should (a &lt;strong&gt;false negative&lt;/strong&gt;) is fixed at unity and the loss of a &lt;strong&gt;false positive&lt;/strong&gt; is defined relative to this for a given threshold &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. After some algebra, the loss attributable to a &lt;strong&gt;false positive&lt;/strong&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\tau}{1-\tau}
\]&lt;/span&gt;
Then, the &lt;strong&gt;net benefit&lt;/strong&gt; of a decision rule (value of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;) is then:
&lt;span class=&#34;math display&#34;&gt;\[
NB(\tau) = \frac{\#TP}{N} - \frac{\#FP}{N} \left( \frac{\tau}{1-\tau} \right)
\]&lt;/span&gt;
In this equation, true positives are weighted one, and false positives weighted &lt;span class=&#34;math inline&#34;&gt;\(\tau /( 1-\tau)\)&lt;/span&gt;. As the cost of a false positive is a function of the threshold we can deduce the relative costs. For example, if &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/3\)&lt;/span&gt;, the cost of a false positive is half the cost of a true positive.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;decision curve&lt;/strong&gt; is then the plot of &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau)\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The solid black line is the net benefit of the model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; at each threshold level. The grey horizontal line (at &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau) = 0\)&lt;/span&gt;) is the net benefit of assuming all patients are negative. The black dotted line is the net benefit of the decision rule: “assume all patients are positive and intervene” which is of course wasteful, but offers comparison to the net benefit of each decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. The red solid vertical line shows the threshold located by maximising the sensitivity/specificity tradeoff in Section &lt;a href=&#34;#sec-ROC-curve&#34;&gt;3.1&lt;/a&gt;. Finally, the red dashed line identifies the threshold at which net benefit departs (exceeds) the “assume all positive” line.&lt;/p&gt;
&lt;p&gt;The region for which &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau)\)&lt;/span&gt; is greater than zero are the thresholds for which the model outperforms “assume all patients are negative.” An advantage of decision curve analysis is that one can vary &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and see the relationship between the model performance and a default strategy of assuming everyone requires intervention – the point at which the black solid line departs from the black dotted line.&lt;/p&gt;
&lt;p&gt;Of note, decision curve analysis is not intended to be a method of locating a threshold; in fact, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-vickers2006decision&#34; role=&#34;doc-biblioref&#34;&gt;Vickers and Elkin&lt;/a&gt; (&lt;a href=&#34;#ref-vickers2006decision&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; discuss the method in the context of shared decision making where the clinician &lt;em&gt;and&lt;/em&gt; patient’s prefence for the relative cost of a false positive are factored into deciding the utility of a decision to intervene.&lt;/p&gt;
&lt;p&gt;However, as an experiment, let’s choose a threshold at the point where the net benefit departs from the default “assume all patients are positive and intervene” – shown as the red dotted line in the right panel at 0.01. This results in zero false negatives (serious errors), 240 true positives, 506 true negatives and 4252 false positives.&lt;/p&gt;
&lt;p&gt;To put this in the context of a sequential triage model, 4252 patients who are actually negative would be triaged for the &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; assessment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequential-triage&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Sequential Triage&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;triage-model.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proposal Kessler &lt;em&gt;et al&lt;/em&gt; put forward is a sequential triage model; above, we have sketched (schematically) the two-stage approach described here.&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_1\)&lt;/span&gt; is the total sample (of size &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt;, containing &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_{1}\)&lt;/span&gt; positive and negative cases respectively) who have been assessed using &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and a decision made on the basis of the prediction &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;. Cases are then discharged on the basis of the decision; of those discharged, &lt;strong&gt;serious errors&lt;/strong&gt; and &lt;strong&gt;appropriate discharges&lt;/strong&gt; are analogous to the number of false negative &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_1}\)&lt;/span&gt; and true negative &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1}\)&lt;/span&gt; decisions respectively.&lt;/p&gt;
&lt;p&gt;Those identified as likely positive by the decision &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; form the triaged subset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt;, who proceed to the more resource intensive assessment &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;. A similar decision system &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; with rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; then either discharges or (in this example) recommends admission to hospital.&lt;/p&gt;
&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_2 = N^{+ve}_2 + N^{-ve}_2\)&lt;/span&gt; – the size of triaged set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; depends on the number of &lt;em&gt;actually&lt;/em&gt; positive and negative cases triaged.&lt;/li&gt;
&lt;li&gt;In the sequential arrangement, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; &lt;em&gt;depends&lt;/em&gt; on the performance of &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;. For example, if a case is incorrectly discharged at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; does not have an opportunity to ‘correct’ that error, so: &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_{2} = N^{+ve}_{1} - \#FN_{X_1}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this in mind, we now attempt to define measures of performance in terms &lt;strong&gt;safety&lt;/strong&gt; and &lt;strong&gt;efficiency&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;safety&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Safety&lt;/h2&gt;
&lt;p&gt;From the discussion in Section &lt;a href=&#34;#sec-calc-PPV-NPV&#34;&gt;1.2&lt;/a&gt;, decision systems with favourable NPV (but poor PPV) might be helpful in screening out candidates at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and triaging suspected positive cases to &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define a &lt;strong&gt;safe&lt;/strong&gt; decision system as having these properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;at each stage (i.e. at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;), anyone with a &lt;strong&gt;high&lt;/strong&gt; or &lt;strong&gt;uncertain&lt;/strong&gt; probability of being positive is appropriately triaged&lt;/li&gt;
&lt;li&gt;serious errors are &lt;strong&gt;minimised&lt;/strong&gt; by &lt;em&gt;not&lt;/em&gt; discharging people inappropriately, which means it should minimise &lt;strong&gt;false negatives&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll define the &lt;strong&gt;total safety&lt;/strong&gt; of the system as a function of the number of serious errors made by both &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S_{Total}(\tau_1, \tau_2) &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{N^{+ve}_{1}+N^{+ve}_{2}} \\
                            &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{N^{+ve}_{1}+(N^{+ve}_{1} - \#FN_{X_1})} \\
                            &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{2N^{+ve}_{1} - \#FN_{X_1}}
\end{aligned}
\]&lt;/span&gt;
As a concrete example (with respect to the diagram above and using the same validation set used in the decision curve and ROC analysis above):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_1\)&lt;/span&gt; is of size &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; = 5000 with &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_1\)&lt;/span&gt; = 240 actual positive and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_1\)&lt;/span&gt; = 4760 actual negative cases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After administering &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, using the threshold &lt;span class=&#34;math inline&#34;&gt;\(\tau_1 = 0.10\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; results in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;80 serious errors or inappropriate discharges equal to false negatives, &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;4321 appropriate discharges – equal to the true negatives, &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Resulting in &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; = 599 cases triaged into &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P_2}\)&lt;/span&gt; equating to the sum of false and true positive cases (i.e. those declared positive by &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; = 0.10)&lt;/li&gt;
&lt;li&gt;Of these 599 cases, &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_2\)&lt;/span&gt; = 160 and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_2\)&lt;/span&gt; = 439&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, let’s assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_2 = 0.36\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; applied to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt;. We arrive at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;57 serious errors, equating to &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;430 appropriate discharges, the true negatives &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Resulting in &lt;span class=&#34;math inline&#34;&gt;\(N_3\)&lt;/span&gt; = 112 cases which will be admitted, consisting of &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_3\)&lt;/span&gt; = 103 actually positive cases (appropriate admissions) and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_3\)&lt;/span&gt; = 9 actually negative cases (inappropriate admissions)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Substituting these numbers in the equation above for safety:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S_{Total}(\tau_1, \tau_2) &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{2N^{+ve}_{1} - \#FN_{X_1}} \\
                            &amp;amp;= 1 - \frac{80 + 57}{480 - 80} \\
                            &amp;amp;= 0.6575
\end{aligned}
\]&lt;/span&gt;
If the decisions made by &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; resulted in no serious errors, we would have zero false negatives and &lt;span class=&#34;math inline&#34;&gt;\(S_{Total}\)&lt;/span&gt; would attain a maximum of one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;efficiency&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Efficiency&lt;/h2&gt;
&lt;p&gt;Now consider efficiency defined as the &lt;strong&gt;ratio&lt;/strong&gt; of useful &lt;strong&gt;product&lt;/strong&gt; to &lt;strong&gt;resource&lt;/strong&gt; consumed.&lt;/p&gt;
&lt;p&gt;Here, the denominator – resource consumption – is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all patients &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; will have &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; administered and be passed through the prediction model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;, so resource consumed is &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;a subset of patients declared positive by the decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; are triaged to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; which is composed of &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_2\)&lt;/span&gt; actual positive and negative cases respectively&lt;/li&gt;
&lt;li&gt;all patients in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; are administed &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; – so resource consumed is &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The numerator – useful product – needs elaboration. First consider the efficiency of &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;, which will be perfectly efficient if all actual negative cases are discharged (and &lt;em&gt;do not&lt;/em&gt; end up in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P_2}\)&lt;/span&gt;) and all positive cases are triaged.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  E_{X_1}( \tau_1 ) = \frac{\#TP_{X_1} + \#TN_{X_1}}{N_1}
\]&lt;/span&gt;
Which attains a maximum efficiency of one when &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1} = N^{-ve}_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\#TP_{X_1} = N^{+ve}_{1}\)&lt;/span&gt; (recall that &lt;span class=&#34;math inline&#34;&gt;\(N_1 = N^{+ve}_1 + N^{-ve}_2\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;A similar definition holds for &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  E_{X_2}( \tau_2 ) = \frac{\#TP_{X_2} + \#TN_{X_2}}{N_2}
\]&lt;/span&gt;
And we allow &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; to contribute equally to a total efficiency in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; defined as:
&lt;span class=&#34;math display&#34;&gt;\[
  E_{Total}( \tau_1, \tau_2 ) = \frac{1}{2} \left[ E_{X_1}( \tau_1 ) + E_{X_2}( \tau_2 )  \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-of-sequential-triage&#34; class=&#34;section level2&#34; number=&#34;5.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Performance of Sequential Triage&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plots show safety (left) and efficiency (right) in contours of size 0.1. So, if &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; is less than around 0.05 and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; is less than approximately 1.5, the sequential triage system has overall safety &amp;gt; 0.9.&lt;/p&gt;
&lt;p&gt;However, the tension is that: for the same range of decision thresholds, the efficiency can reach a maximum of 0.747 but this results in upto 31 serious errors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concluding-remarks&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Concluding Remarks&lt;/h1&gt;
&lt;p&gt;A few observations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There is no clear way of robustly setting the decision rules &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; based on performance measures for sequential triage unless one is prepared to accept that efficiency and safety trade-off&lt;/li&gt;
&lt;li&gt;Using decision curve analysis &lt;strong&gt;properly&lt;/strong&gt; invites setting the decision threshold according to a tradeoff that rightly involves how much risk the patient and clinician want to take: i.e. net benefit provides a weight to false positives (harm attributable to intervening when it is unnecessary) as a function of the decision threshold.&lt;/li&gt;
&lt;li&gt;The analyses above were all conducted on the improbably ideal situation where a) we had 5000 exemplars to train a system on, both with measurements/assessments &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; b) we had access to another ‘batch’ of 5000 exemplars to validate the model on which we attempt to define performance and optimise the system’s performance&lt;/li&gt;
&lt;li&gt;When deploying this system, at best, patients arrive in small batches or singularly (certainly not in clusters of 100s or 1000s) – the above analyses are then at best, informative for an actuarial or economic analysis if we were &lt;em&gt;forced&lt;/em&gt; to choose a decision rule to evaluate the triage system’s performance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By their very nature, predictions &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; are probabilistic and discrete thresholds coerce these uncertain forecasts into definitive decisions. It seems unlikely that in the case of rare, serious events anyone would rely on a decision support system that gave discrete answers (and indeed, decision curve analysis emphasises the role of clinician expertise and patient preference in deciding on the intervention threshold)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-altman-pred-values1994&#34; class=&#34;csl-entry&#34;&gt;
Altman, Douglas G, and J Martin Bland. 1994. &lt;span&gt;“Statistics Notes: Diagnostic Tests 2: Predictive Values.”&lt;/span&gt; &lt;em&gt;Bmj&lt;/em&gt; 309 (6947): 102.
&lt;/div&gt;
&lt;div id=&#34;ref-blitzstein2019introduction&#34; class=&#34;csl-entry&#34;&gt;
Blitzstein, Joseph K, and Jessica Hwang. 2019. &lt;em&gt;Introduction to Probability&lt;/em&gt;. &lt;span&gt;CRC&lt;/span&gt; Press.
&lt;/div&gt;
&lt;div id=&#34;ref-kessler2019suicide&#34; class=&#34;csl-entry&#34;&gt;
Kessler, Ronald C, Robert M Bossarte, Alex Luedtke, Alan M Zaslavsky, and Jose R Zubizarreta. 2019. &lt;span&gt;“Suicide Prediction Models: A Critical Review of Recent Research with Recommendations for the Way Forward.”&lt;/span&gt; &lt;em&gt;Molecular Psychiatry&lt;/em&gt;, 1–12.
&lt;/div&gt;
&lt;div id=&#34;ref-vickers2006decision&#34; class=&#34;csl-entry&#34;&gt;
Vickers, Andrew J, and Elena B Elkin. 2006. &lt;span&gt;“Decision Curve Analysis: A Novel Method for Evaluating Prediction Models.”&lt;/span&gt; &lt;em&gt;Medical Decision Making&lt;/em&gt; 26 (6): 565–74.
&lt;/div&gt;
&lt;div id=&#34;ref-Vickers2016&#34; class=&#34;csl-entry&#34;&gt;
Vickers, Andrew J, Ben Van Calster, and Ewout W Steyerberg. 2016. &lt;span&gt;“Net Benefit Approaches to the Evaluation of Prediction Models, Molecular Markers, and Diagnostic Tests.”&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 352. &lt;a href=&#34;https://doi.org/10.1136/bmj.i6&#34;&gt;https://doi.org/10.1136/bmj.i6&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Decisions and Loss Functions - A more clinical focus</title>
      <link>/post/decisions-and-loss-functions/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/decisions-and-loss-functions/</guid>
      <description>
&lt;script src=&#34;/post/decisions-and-loss-functions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/decisions-and-loss-functions/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/decisions-and-loss-functions/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;In the previous post, loss functions where considered in the context of estimating measures of central tendency for distributions. In this post, I want to look at the computation of loss functions in situations that might arise in a clinical predictive model. This is all textbook stuff – see &lt;a href=&#34;#sec-further-reading&#34;&gt;Further Reading&lt;/a&gt; – but I wanted to summarise it in a way I understood when in a year’s time, I wonder what the code does.&lt;/p&gt;
&lt;p&gt;I realised that the notation in the last post was sloppy, so for this post, I’ll adopt the conventions in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The basic setup is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a finite set of available actions, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A} = \{ a_1, a_2, \ldots, a_j \}\)&lt;/span&gt; – in the examples that follow, we’ll restrict our attention to a choice between two actions of “do nothing” or “intervene/treat” respectively but there is no loss of generality in assuming this.&lt;/li&gt;
&lt;li&gt;There are uncertain quantities representing “states of the world” &lt;span class=&#34;math inline&#34;&gt;\(\Theta = \{ \theta_1, \theta_2, \ldots, \theta_i \}\)&lt;/span&gt; about which we can obtain data and that can affect our decision about which action to take. Here, these states will reflect something about a patient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, our task will be to choose the best action from &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A}\)&lt;/span&gt; given information about states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Information about the states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; will come from a putative predictive model: as in the previous post, measurements (the “input” to the model) for a given patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are given to the predictive model &lt;span class=&#34;math inline&#34;&gt;\(Y = F(x)\)&lt;/span&gt; that delivers scores (the “output”) as realisations of &lt;span class=&#34;math inline&#34;&gt;\(Y \in [0,1]\)&lt;/span&gt;. Importantly, for any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we can access samples from the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; (rather than relying on a single point prediction, such as the mean of the posterior).&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Setup&lt;/h1&gt;
&lt;p&gt;To begin with, assume the simplest case of there being two states &lt;span class=&#34;math inline&#34;&gt;\(\Theta = \{ \theta_1, \theta_2 \}\)&lt;/span&gt; which correspond to a patient being “negative” or “positive” (respectively) for some event or outcome. Our repetoire of actions is &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A} = \{ a_1, a_2 \}\)&lt;/span&gt; representing “do nothing” and “intervene/treat” respectively. This only serves a pedagogical need when developing the ideas, not because it represents a principled or sound modelling decision.&lt;/p&gt;
&lt;p&gt;For a single example patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the output of the model suggests the they are most likely negative (i.e. the probability mass is concentrated near zero):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(gridExtra)
library(kableExtra)
library(latex2exp)
library(reshape2)

# globals for presentation
basictheme &amp;lt;- theme_minimal() + 
  theme(axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        plot.title = element_text(size = rel(1.25), face = &amp;quot;bold&amp;quot;, hjust = 0.5 ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3141)
range01 &amp;lt;- function(x){(x-min(x))/(max(x)-min(x))}
samples &amp;lt;- range01( rgamma( 2000, shape = 2, scale = 2) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now require a mapping from the samples &lt;span class=&#34;math inline&#34;&gt;\(y \sim \pi_{F}(Y|x)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; because the domain of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}\)&lt;/span&gt; will be the interval &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; and to get started, we need to “quantise” to two states.&lt;/p&gt;
&lt;p&gt;Define the distribution over states &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\pi_{\Theta}(\theta_{1}) &amp;amp;= \Pr_{\pi_{F}}( Y \leq 0.5 ) \\
\pi_{\Theta}(\theta_{2})  &amp;amp;= \Pr_{\pi_{F}}( Y &amp;gt; 0.5 )
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;So, we basically histogram the samples into two bins either side of 0.5, representing the probability of a patient being negative (&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;) or positive (&lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt;). A terrible idea, which we will reverse later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdfFromSamples &amp;lt;- function(a, b, delta, samples) {
  H &amp;lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) )
  ret &amp;lt;- data.frame(
    mids  = H$mids,
    freq  = H$counts
  )
  ret$P &amp;lt;- ret$freq / sum(ret$freq)
  return(ret)
}

pdf.Y &amp;lt;- pdfFromSamples(0,1,delta = 1/2, samples)
pdf.Y$theta &amp;lt;- factor( c(1,2) )

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

print( pdf.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to our blunt assignment of states to output from the predictive model, the probability the patient is negative is &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_{1})\)&lt;/span&gt; = 0.921 and positive &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_{2})\)&lt;/span&gt; = 0.079.&lt;/p&gt;
&lt;p&gt;With this setup, (two actions, two states) we can “tabulate” the combinations of actions and states (the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cartesian_product&#34;&gt;Cartesian product&lt;/a&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\Theta \times \mathscr{A}\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;loss-actions.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In each “cell” or combination &lt;span class=&#34;math inline&#34;&gt;\((\theta,a)\)&lt;/span&gt; we then assign a &lt;strong&gt;loss&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,a) \leq 0\)&lt;/span&gt; which describes the &lt;strong&gt;cost&lt;/strong&gt; incurred for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; when the state &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; obtains. Generally, we will adopt the convention that losses represent costs or penalties for actions with respect to states.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-loss-matrix&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Example Loss Matrix&lt;/h1&gt;
&lt;p&gt;Equipped with this toy example we assign losses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- c(&amp;quot;a1:&amp;lt;br&amp;gt;do nothing&amp;quot;,&amp;quot;a2:&amp;lt;br&amp;gt;intervene&amp;quot;)
Theta &amp;lt;- c(&amp;quot;&amp;lt;b&amp;gt;theta1:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;negative&amp;quot;, &amp;quot;&amp;lt;b&amp;gt;theta2:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;positive&amp;quot;)

loss.matrix &amp;lt;- matrix( c( 0.0,  -0.5,
                          -1.0, 0 ), 
                       nrow = 2, ncol = 2, byrow = TRUE)
rownames(loss.matrix) &amp;lt;- Theta
colnames(loss.matrix) &amp;lt;- A

knitr::kable( loss.matrix, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a1:&lt;br&gt;do nothing
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a2:&lt;br&gt;intervene
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta1:&lt;/b&gt;&lt;br&gt;negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta2:&lt;/b&gt;&lt;br&gt;positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As a use example, assume we decide &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; (intervene) and the state of the patient turns out to be &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; (negative) we incur a loss of &lt;span class=&#34;math inline&#34;&gt;\(L(\theta_1,a_2) = -0.5\)&lt;/span&gt; to reflect unnecessary costs of e.g. further investigations, inconvenience to the patient etc. If we select &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; and the state is &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; (equating to doing nothing and the patient is negative) we incur zero loss because this was an appropriate action given the circumstances.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-expected-loss&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Bayesian Expected Loss&lt;/h1&gt;
&lt;p&gt;Following &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt;, we define the Bayesian expected loss (BEL) for action &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; with respect to the discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[
\rho(\pi_{\Theta},a_j) = \mathbb{E}_{\pi_{\Theta}} \left[ L(\theta,a_j\right] = \sum_{i}L(\theta_i,a_j)\pi_{\Theta}(\theta_i)
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bayesian expected loss (BEL)
BEL &amp;lt;- function( a, p.pi, loss.matrix ) {
   sum( loss.matrix[ , a ] * p.pi )
}

# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y$P, loss.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the upper bound of the BEL is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-bayes-decision-principal&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Conditional Bayes Decision Principal&lt;/h1&gt;
&lt;p&gt;Having established the BEL for each action, the conditional bayes decision principle (CBD) for deciding on an action &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt; is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;choose &lt;span class=&#34;math inline&#34;&gt;\(a_{j} \in \mathscr{A}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; &lt;strong&gt;minimises&lt;/strong&gt; the BEL : &lt;span class=&#34;math inline&#34;&gt;\(\underset{j}{\mathrm{arg\,max}} \; \rho( \pi_{\Theta}, a_j )\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In code: the resulting vector for &lt;span class=&#34;math inline&#34;&gt;\(\rho( \pi_{\Theta}, a )\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable( rho.A, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rho
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.0790
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.4605
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And the action that minimises the BEL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( min.bel.CBD &amp;lt;- which.max( rho.A$rho ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we find the action 1 (i.e. &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; = “do nothing”) minimises the BEL. This fits with our intuition given the patient is most likely negative: &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_1) &amp;gt; \pi_{\Theta}(\theta_1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;developing-the-loss-function&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Developing the Loss Function&lt;/h1&gt;
&lt;p&gt;Consider a different patient where the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt;, the output of the predictive model, looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samples &amp;lt;- range01( c(rnorm( 1000, mean = 0, sd = 2 ), rnorm( 1000, mean = 10, sd = 3) ) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
In this example, there’s uncertainty about the patient being negative or positive.&lt;/p&gt;
&lt;p&gt;This time, we’ll quantise into &lt;em&gt;three&lt;/em&gt; equal-sized intervals over the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; (again, an unprincipled decision made only for demonstration) and map to three states:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0,1,delta=1/3,samples)
pdf.Y$theta &amp;lt;- factor( seq(1,3,by=1) )

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

print( pdf.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The loss matrix will now be a &lt;span class=&#34;math inline&#34;&gt;\(3 \times 2\)&lt;/span&gt; matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- c(&amp;quot;a1:&amp;lt;br&amp;gt;do nothing&amp;quot;,&amp;quot;a2:&amp;lt;br&amp;gt;intervene&amp;quot;)
Theta &amp;lt;- c(&amp;quot;&amp;lt;b&amp;gt;theta1:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;negative&amp;quot;, 
           &amp;quot;&amp;lt;b&amp;gt;theta2:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;equivocal&amp;quot;,  
           &amp;quot;&amp;lt;b&amp;gt;theta3:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;positive&amp;quot;)

loss.matrix &amp;lt;- matrix( c( 0.0,  -0.5,
                          -0.5,  -0.2,
                          -1.0, 0 ), 
                       nrow = 3, ncol = 2, byrow = TRUE)
rownames(loss.matrix) &amp;lt;- Theta
colnames(loss.matrix) &amp;lt;- A

knitr::kable( loss.matrix, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a1:&lt;br&gt;do nothing
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a2:&lt;br&gt;intervene
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta1:&lt;/b&gt;&lt;br&gt;negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta2:&lt;/b&gt;&lt;br&gt;equivocal
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta3:&lt;/b&gt;&lt;br&gt;positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bearing in mind that states are uncertain, the logic behind this loss matrix is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; (do nothing) : no cost is incurred if the patient is likely negative (&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;). If the patient is most likely positive (&lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt;) and we do nothing, this is evidently the wrong decision and we incur the maximum penalty of -1.0. If there is some equivocation &lt;span class=&#34;math inline&#34;&gt;\((\theta_2\)&lt;/span&gt;) – we penalise by half the maximum cost to discourage doing nothing (equating to loss aversion)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; (intervene) : for &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; we incur a cost (-0.5) for intervening when unnecessary. Naturally, for &lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt;, the correct thing to do is intervene so this has no penalty associated. For the equivocal case, &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt;, we should certainly not ignore these cases but simply intervening (i.e. with zero penalty) is inappropriate. So we incur a small penalty (-0.2) to nudge us away from intervening as the default.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that in designing the loss matrix, we are trying to capture domain knowledge about the &lt;em&gt;deployment&lt;/em&gt; of the model – for example, the loss attached to doing nothing (when there is equivocation about the negative/positive state of the patient) pushes us to be cautious and intervene.&lt;/p&gt;
&lt;p&gt;Let’s look at the resulting BEL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y$P, loss.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable( rho.A, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rho
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.35125
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.30530
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( min.bel.CBD &amp;lt;- which.max( rho.A$rho ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The action that minimises &lt;span class=&#34;math inline&#34;&gt;\(\rho(\pi_{\Theta},a)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; – as can be seen, the probability mass for &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt; (and the associated losses) is driving the decision to intervene i.e. be cautious.&lt;/p&gt;
&lt;p&gt;We can continue introducing more and more granularity in quantising the posterior predictions &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; to arrive at mappings to states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; and then specifying individual losses in the corresponding rows of the loss matrix. Instead, we’ll specify a &lt;strong&gt;loss function&lt;/strong&gt; (although for coding convenience, we’ll continue with a matrix representation).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Sigmoid &amp;lt;- function( x, A, B, m, s ) {
  # x = vector of values
  # A = height of sigmoid
  # B = translation on y axis
  # m = value of x for which Sigmoid() = half max value
  # s = steepness of linear component
  exp.x &amp;lt;- exp( -(x-m)/s )
  return(
    ( A + B * (1+exp.x) ) / (1+exp.x)
  )
}


# plots to compare the quantised states to a more fine-grained version
pdf.Y1 &amp;lt;- pdfFromSamples(0, 1, delta= 1/50, samples)
pdf.plot.Y1 &amp;lt;- ggplot( pdf.Y1, aes( x = mids, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

pdf.Y2 &amp;lt;- pdfFromSamples(0, 1, delta= 1/3, samples)
pdf.Y2$Theta &amp;lt;- factor(c(&amp;quot;theta1&amp;quot;,&amp;quot;theta2&amp;quot;,&amp;quot;theta3&amp;quot;))
pdf.plot.Y2 &amp;lt;- ggplot( pdf.Y2, aes( x = Theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y2$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

# loss functions for a1 and a2
loss.fun.a1 &amp;lt;- Sigmoid(pdf.Y1$mids, A = -1.0, B = 0, m = 0.5, s = 0.15 )
loss.fun.a2 &amp;lt;- Sigmoid(pdf.Y1$mids, A = 0.5, B = -0.5, m = 0.3, s = 0.08 )

# build a tabular version of loss function
loss.fun &amp;lt;- data.frame( Theta = pdf.Y1$mids,
                        L.a1 = loss.fun.a1,
                        L.a2 = loss.fun.a2
                        )

# show the loss function and 3 state quantised loss matrix
loss.fun.plot &amp;lt;- ggplot( loss.fun, aes( x = Theta ) ) +
  geom_line( aes( y = L.a1 ), colour = &amp;quot;#fc8d59&amp;quot;, size = 1) +
  annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.75, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( y = L.a2 ), colour = &amp;quot;#91bfdb&amp;quot;, size = 1 ) + 
  annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.15, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) +
  basictheme

df.loss.matrix &amp;lt;- data.frame( Theta = factor( c(&amp;quot;theta1&amp;quot;,&amp;quot;theta2&amp;quot;,&amp;quot;theta3&amp;quot;) ),
                              L.a1 = loss.matrix[,1],
                              L.a2 = loss.matrix[,2]
                            )

loss.matrix.plot &amp;lt;- ggplot( df.loss.matrix ) +
  geom_line( aes( x = Theta, y = L.a1, group = 1), 
             colour = &amp;quot;#fc8d59&amp;quot;, size = 1) +
  geom_point( aes( x = Theta, y = L.a1, group = 1), 
             colour = &amp;quot;#fc8d59&amp;quot;, size = 4) +
  annotate( &amp;quot;label&amp;quot;, 
            x = 2.8, y = -0.7, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( x = Theta, y = L.a2, group= 1), 
             colour = &amp;quot;#91bfdb&amp;quot;, size = 1.5 ) + 
  geom_point( aes( x = Theta, y = L.a2, group= 1), 
             colour = &amp;quot;#91bfdb&amp;quot;, size = 4 ) + 
  annotate( &amp;quot;label&amp;quot;, 
            x = 2.8, y = -0.2, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) +
  basictheme

grid.arrange( pdf.plot.Y2, pdf.plot.Y1, 
              loss.matrix.plot, loss.fun.plot, 
              nrow = 2, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, we show the three-state loss matrix underneath the distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; (the lines are to emphasise the trend in losses as we proceed from likely negative through positive). On the right, a finer-grained representation of the distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta} \approx \pi_{F}(Y|x)\)&lt;/span&gt; with a sigmoid loss function over &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; interpolating between the points in the loss matrix at the extremes (negative, positive) and midpoint (equivocal). Now, we can effectively use the whole of the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; more directly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the loss function x probability
pdf.Y1$L.a1 &amp;lt;- pdf.Y1$P * loss.fun$L.a1
pdf.Y1$L.a2 &amp;lt;- pdf.Y1$P * loss.fun$L.a2

# product of the posterior and loss function
loss.fun.plot2 &amp;lt;- ggplot( pdf.Y1, aes( x = mids ) ) +
  geom_line( aes( y = L.a1 ), colour = &amp;quot;#fc8d59&amp;quot;, size = 1.5) +
  #annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.75, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( y = L.a2 ), colour = &amp;quot;#91bfdb&amp;quot;, size = 1.5 ) + 
  #annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.15, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) + 
  basictheme

## The actual BEL
# we need a matrix representation of the loss function
loss.fun.matrix &amp;lt;- as.matrix( loss.fun[,2:3] )
colnames( loss.fun.matrix ) &amp;lt;- c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)

# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y1$P, loss.fun.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot.Y1, loss.fun.plot2, bel.plot, ncol = 2, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the top-left panel shows a finer-grained distribution function &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; and the top-right panel shows the loss function for &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; weighted by &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta} \approx \pi_{F}(Y|x)\)&lt;/span&gt; – rather than the &lt;strong&gt;sum&lt;/strong&gt; for each action as in &lt;span class=&#34;math inline&#34;&gt;\(\rho(\theta, a)\)&lt;/span&gt;. This exposes that the Bayesian expected loss of an action is the integral over states (equivalently, the sum for discrete distributions) of the product of the loss function for an action in a certain state and the probability of that state. The bottom-left panel shows the resulting BEL where, as expected, &lt;span class=&#34;math inline&#34;&gt;\(a2\)&lt;/span&gt; minimises &lt;span class=&#34;math inline&#34;&gt;\(\rho(\theta,a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-further-reading&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Further Reading&lt;/h1&gt;
&lt;p&gt;If I were to try this again (rather than trying to piece together an understanding from wikipedia), I would proceed in this order:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-savage1951theory&#34; role=&#34;doc-biblioref&#34;&gt;Savage 1951&lt;/a&gt;)&lt;/span&gt; for foundations/first principles and tutorial approach.&lt;/li&gt;
&lt;li&gt;First four chapters of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt; for a really clear exposition of the core ideas.&lt;/li&gt;
&lt;li&gt;For decision theory in point estimation from the perspective of sciences concerned with &lt;strong&gt;prediction&lt;/strong&gt; and &lt;strong&gt;forecasting&lt;/strong&gt;: &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gneiting2011making&#34; role=&#34;doc-biblioref&#34;&gt;Gneiting 2011&lt;/a&gt;)&lt;/span&gt; provides a comprehensive review&lt;/li&gt;
&lt;li&gt;Risk/decision theory for &lt;strong&gt;classification&lt;/strong&gt; Chapter 2 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-duda2012pattern&#34; role=&#34;doc-biblioref&#34;&gt;Duda, Hart, and Stork 2012&lt;/a&gt;)&lt;/span&gt; and Chapter 1.5 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Foundations in Bayesian principles more generally: Chapter 2 and Appendix B of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bernardo2009bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Bernardo and Smith 2009&lt;/a&gt;)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-berger1985statistical&#34; class=&#34;csl-entry&#34;&gt;
Berger, James O. 1985. &lt;em&gt;Statistical Decision Theory and Bayesian Analysis&lt;/em&gt;. 2nd ed. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-bernardo2009bayesian&#34; class=&#34;csl-entry&#34;&gt;
Bernardo, José M, and Adrian FM Smith. 2009. &lt;em&gt;Bayesian Theory&lt;/em&gt;. Vol. 405. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-duda2012pattern&#34; class=&#34;csl-entry&#34;&gt;
Duda, Richard O, Peter E Hart, and David G Stork. 2012. &lt;em&gt;Pattern Classification&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-gneiting2011making&#34; class=&#34;csl-entry&#34;&gt;
Gneiting, Tilmann. 2011. &lt;span&gt;“Making and Evaluating Point Forecasts.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 106 (494): 746–62.
&lt;/div&gt;
&lt;div id=&#34;ref-savage1951theory&#34; class=&#34;csl-entry&#34;&gt;
Savage, Leonard J. 1951. &lt;span&gt;“The Theory of Statistical Decision.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 46 (253): 55–67.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Loss, Risk and Point Summaries of Posterior Distributions</title>
      <link>/post/loss-functions-and-posteriors/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/loss-functions-and-posteriors/</guid>
      <description>
&lt;script src=&#34;/post/loss-functions-and-posteriors/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/loss-functions-and-posteriors/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/loss-functions-and-posteriors/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;For a while, I’ve been thinking about the deployment of predictive algorithms in clinical decision support. Specifically, about the difference between what we understand about a model’s performance from the publication describing it and how this might be less informative when deployed. In short: what is the value of knowing that a model has good balanced accuracy or a high area under the ROC curve when sat with a patient and using the tool to make a clinical decision.&lt;/p&gt;
&lt;p&gt;Frequently in medical applications of machine learning we see summary measures of performance used to demonstrate the competence of the model typically reported as sensitivities, specificities, balanced accuracy, area under the ROC curve and so on. These measures assume that for an input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; (a patient) we get an output &lt;span class=&#34;math inline&#34;&gt;\(Y = F(x)\)&lt;/span&gt; representing for example, “negative” or “positive” caseness.&lt;/p&gt;
&lt;p&gt;If the system &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is designed to &lt;strong&gt;classify&lt;/strong&gt; patients, then it will deliver discrete ‘yes / no’ answers e.g. &lt;span class=&#34;math inline&#34;&gt;\(Y \in \{0,1\}\)&lt;/span&gt;; canonical examples include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;support-vector machine (SVM)&lt;/a&gt;. Baked-into these algorithms is a &lt;strong&gt;decision rule&lt;/strong&gt; operating over a continuous value; in SVMs, for example, the decision rule classifies each patient by dichotomising the signed distance of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the class-seperating hyperplane. If using a &lt;strong&gt;classifier&lt;/strong&gt; makes sense in the context of it’s clinical deployment you might want the algorithm to report a definitive dichotomised answer and summary measures like sensitivity and specificity (perhaps) make sense.&lt;/p&gt;
&lt;p&gt;Compare this with &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;logistic regression&lt;/a&gt; (often used as if it were a classifier) where the purpose is to estimate the &lt;strong&gt;probability of an event&lt;/strong&gt;, i.e. that a patient is positive or negative. Often in these cases, the decision rule is ‘bolted on’ and that’s when people invoke ROC curves and compute accuracies at an operating threshold that maximises the trade-off between sensitivity and specificity e.g. by &lt;a href=&#34;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&#34;&gt;maximising Youden’s J-statistic&lt;/a&gt;. It’s here that the decision rule and it’s deployment context matter.&lt;/p&gt;
&lt;p&gt;It seems to me that patients and clinicians would probably want more information than a pure classifier provides and most likely would prefer to know the &lt;strong&gt;actual&lt;/strong&gt; continuous score – the “output”&#34; of &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Further, a patient and clinician might want to understand the decision rule and it’s assumptions. For example, assume for a deployed predictive model &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and a new patient, &lt;span class=&#34;math inline&#34;&gt;\(F(x) = 0.79\)&lt;/span&gt; and this represents (or is proportional to) the likelihood of being a positive case. The predictive model being deployed has an operating threshold of &lt;span class=&#34;math inline&#34;&gt;\(0.80\)&lt;/span&gt; for declaring a positive case and this threshold was determined by maximising the trade-off between sensitivity and specificity. Recall that sensitivity is the ratio &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt; and specificity is the ratio &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt;. Amongst other things, I’d want to know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;in determining the operating threshold, were true positives and false negatives given equal weight ? For example, in the context of predicting a rare but serious event, a false negative can be orders-of-magnitude more ‘costly’ than the model correctly determining cases that are true positives or true negatives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;in the example above, &lt;span class=&#34;math inline&#34;&gt;\(F(x) = 0.79\)&lt;/span&gt; is 0.01 below the operating threshold for declaring a &lt;em&gt;positive&lt;/em&gt; case – boundary cases near the operating threshold, demand closer inspection and at least, consideration of the &lt;strong&gt;uncertainty&lt;/strong&gt; in the output of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more elaborate version of this discussion is &lt;a href=&#34;https://github.com/danwjoyce/summ-performance/blob/master/revised_predictive_decisions.pdf&#34;&gt;here&lt;/a&gt; and summarised in a &lt;a href=&#34;https://jamanetwork.com/journals/jamapsychiatry/article-abstract/2758828&#34;&gt;short paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, the problem of ‘declaring’ a prediction can be cast as a problem in statistical &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_theory&#34;&gt;decision theory&lt;/a&gt;. Being only vaguely familiar with similar ideas from signal detection models in psychophysics &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-green1966signal&#34; role=&#34;doc-biblioref&#34;&gt;Green and Swets 1966&lt;/a&gt;)&lt;/span&gt;, I decided to deep-dive into the details of using posterior distributions to arrive at decisions via loss and risk functions. I found the ideas are fairly intuitive and are well described e.g. in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-duda2012pattern&#34; role=&#34;doc-biblioref&#34;&gt;Duda, Hart, and Stork 2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;. It was harder for me to operationalise these; for example, I frequently found explainations that the &lt;span class=&#34;math inline&#34;&gt;\(L_0\)&lt;/span&gt; “zero-one” loss function delivers the mode of a (posterior) distribution, but examples of this in action were harder to come by.&lt;/p&gt;
&lt;div id=&#34;sec-predictions&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Predictions&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;prediction-classification.png&#34; width=&#34;474&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As above, assume we have a model &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; that delivers a continuous score &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an outcome/case/event given some “input” measurement or feature(s), &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; for a patient. Importantly, the model delivers outputs in the form of posterior probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x) = \Pr(Y=y|x)\)&lt;/span&gt;; for example, the probability of being a positive case is &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=1|x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0|x) = 1 - \pi(Y=1|x)\)&lt;/span&gt; being the probability of a negative case.&lt;/p&gt;
&lt;p&gt;So for any patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we will have a posterior distribution – rather than a single &lt;em&gt;point&lt;/em&gt; summary.&lt;/p&gt;
&lt;p&gt;This is an important distinction: after inferring (learning) a model that delivers posterior probabilities, we can then &lt;em&gt;deliberately&lt;/em&gt; design and implement a decision process – in contrast to solving the related problem of &lt;strong&gt;discrimination&lt;/strong&gt; or &lt;strong&gt;classification&lt;/strong&gt;, where we find a direct mapping from each input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a discrete, often binary, output and sacrifice an estimate of uncertainty (see Ch. 1.5 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt; for more detail).&lt;/p&gt;
&lt;p&gt;We are used to seeing &lt;em&gt;point estimates&lt;/em&gt; as outputs from predictive models given some input e.g. &lt;span class=&#34;math inline&#34;&gt;\(\pi( Y = 1 | x) = 0.78\)&lt;/span&gt; and sometimes, with a measure of uncertainty on that output (for example, the standard error on predicted values from &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.glm.html&#34;&gt;&lt;code&gt;predict.glm()&lt;/code&gt;&lt;/a&gt; in &lt;code&gt;R&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This point value is a summary of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_distribution&#34;&gt;posterior distribution&lt;/a&gt; of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (for example, the mean) and represents the output of a decision making process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/h1&gt;
&lt;p&gt;Assume that for some predictive model, we present a single patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and we are able to access the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; i.e. we can obtain samples from the posterior distribution for that patient denoted &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to ensure reproducible result
set.seed(3141)
# simulate a bimodal posterior distribution $\pi(Y|x)$
samples &amp;lt;- range01( c(rnorm( 1000, mean = 0, sd = 1 ), rnorm( 1000, mean = 5, sd = 2) ) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have 2000 samples from &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; stored as &lt;code&gt;samples&lt;/code&gt; (usually, these samples will be from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_predictive_distribution&#34;&gt;posterior predictive distribution&lt;/a&gt; obtained by e.g. MCMC sampling):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round( samples[1:10], 3 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.291 0.130 0.271 0.295 0.294 0.373 0.261 0.232 0.177 0.313&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following code will give us a &lt;code&gt;data.frame&lt;/code&gt; that represents an approximation to the probability distribution function as a lookup table (basically, a histogram):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdfFromSamples &amp;lt;- function(a, b, delta, samples) {
  H &amp;lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) )
  ret &amp;lt;- data.frame(
    mids  = H$mids,
    freq  = H$counts
  )
  ret$P &amp;lt;- ret$freq / sum(ret$freq)
  return(ret)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, we can examine &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; in the region &lt;span class=&#34;math inline&#34;&gt;\(Y \in [0.5,0.6]\)&lt;/span&gt; with a bin-width of &lt;code&gt;delta = 1/50&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0, 1, delta = 1/50, samples )
knitr::kable( pdf.Y[ pdf.Y$mids &amp;gt;= 0.5 &amp;amp; pdf.Y$mids &amp;lt;= 0.6, ], 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE,
              row.names = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
mids
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
freq
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
P
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0285
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
46
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0230
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0285
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0310
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
48
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0240
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With some abuse of notation, we can state that (approximately) &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0.55|x) = 0.0285\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0.59|x) = 0.0240\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-loss&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Loss Functions&lt;/h1&gt;
&lt;p&gt;Consider the task of choosing a summary of the information contained in the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; as a single (point) value. We can see that the score &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; ranges from 0 to 1 and we could potentially pick any one of an infinite number of values as our chosen point summary (of course, some will be meaningful and others less so).&lt;/p&gt;
&lt;p&gt;To make this concrete, we’ll cheat and look-ahead to the answer. One meaningful decision to summarise &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; is to choose the &lt;strong&gt;mode&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0, 1, delta = 1/100, samples )

mode.pdf.Y &amp;lt;- pdf.Y$mids[ order( pdf.Y$freq, decreasing = TRUE) ][1]

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = mids, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  xlab(&amp;quot;Score (Y)&amp;quot;) +
  ylab(TeX(&amp;#39;$\\pi(Y|x)&amp;#39;)) +
  basictheme

mode.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = mode.pdf.Y, colour = &amp;quot;black&amp;quot;, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Mode = &amp;quot;, mode.pdf.Y ), 
            x = mode.pdf.Y, y = 0.01)

print( mode.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now assume we don’t have this information but instead, let &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; be one such candidate decision for the point summary. The actual point summary is &lt;span class=&#34;math inline&#34;&gt;\(y_{j} \in [0,1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define the cost of choosing &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; – where the true value is &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; – as the &lt;strong&gt;loss function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(a_{i},y_{j})\)&lt;/span&gt;. Now, construct a loss function that penalizes any candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; by a single unit if &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; is not equal to &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; (incorrect) and zero if &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; is equal to &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;; this is the so-called “zero-one” loss function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_{0}(a_{i},y_{j}) = \begin{cases}
          0 &amp;amp; \text{if}\ a_{i} = y_{j} \\
          1 &amp;amp; \text{otherwise}
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, we propose an arbitrary candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{1} = 0.555\)&lt;/span&gt; and we want to know the associated loss over the range of possible values of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the range of possible values $y_{j}$
y &amp;lt;- pdf.Y$mids
# our estimate $a_{i}$
a1 &amp;lt;- 0.555

# the L_{0} loss function:
loss0 &amp;lt;- function( y, a ) { ifelse( a == y, 0, 1 ) }

# the loss function evaluated over the range $y_{j}$
l0.ex &amp;lt;- data.frame( mids = pdf.Y$mids, loss.ex1 = loss0( y, a1 ) )

loss.plot &amp;lt;- ggplot( l0.ex, aes( x = l0.ex$mids, y = loss.ex1) ) +
  geom_line( colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$y_{j}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$L_{0}$&amp;#39;)) +
  basictheme

print( loss.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s repeat the same process for two other (arbitrarily chosen) candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{2} = 0.095\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{3} = 0.755\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a2 &amp;lt;- 0.095
a3 &amp;lt;- 0.755

# the loss function evaluated over the range $y_{j}$
l0.ex$loss.ex2 &amp;lt;- loss0( y, a2 )
l0.ex$loss.ex3 &amp;lt;- loss0( y, a3 )

loss.plot &amp;lt;- ggplot( l0.ex ) +
  geom_line( aes( x = mids, y = loss.ex1 ), colour = &amp;quot;#bdbdbd&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{1}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a1, y = 0.25) +
  geom_line( aes( x = mids, y = loss.ex2 ), colour = &amp;quot;#969696&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{2}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a2, y = 0.25) +
  geom_line( aes( x = mids, y = loss.ex3 ), colour = &amp;quot;#636363&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{3}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a3, y = 0.25) +
  xlab(TeX(&amp;#39;$y_{j}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$L_{0}$&amp;#39;)) +
  basictheme

print( loss.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly using the loss function we get ‘spikes’ when our candidates coincide with a value in the range of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But, what we care about is loss associated with a candidate with respect to the probability of each possible value &lt;span class=&#34;math inline&#34;&gt;\(Y=y_j\)&lt;/span&gt;; so we weight the loss associated with each “decision” (each candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;) by the posterior probability of &lt;span class=&#34;math inline&#34;&gt;\(Y = y_{j}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_0(a_{i},y_{j})\pi(y_{j}|x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This leads us to consider the &lt;strong&gt;risk&lt;/strong&gt; associated with each candidate. In essence, we want to know the loss associated with &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; when the true value is &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; weighted by how likely or how frequently we see &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-risk-functions&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Risk Functions&lt;/h1&gt;
&lt;p&gt;We could continue randomly choosing candidates but instead, we’ll be systematic and check all values of &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;. At the same time, we’ll shift representation and instead of plotting the loss function for each candidate against the range of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;, we’ll compute a &lt;strong&gt;risk function&lt;/strong&gt; for each candidate as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34;&gt;expected value&lt;/a&gt; of the loss function evaluated for each &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[L_0(a_{i},y_{j})] = \sum_{j} \underbrace{L_{0}(a_{i},y_{j})}_\text{loss} \underbrace{\pi(y_{j}|x)}_\text{posterior}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, instead of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; on the horizontal axis and &lt;span class=&#34;math inline&#34;&gt;\(L_{0}\)&lt;/span&gt; on the vertical, we instead show candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; on the horizontal with the risk &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[L]\)&lt;/span&gt; on the vertical:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all candidate values for $a_{i}$ 
a &amp;lt;- pdf.Y$mids
# the range of values for $y_{j}$
y &amp;lt;- pdf.Y$mids

pdf.Y$risk.L0 &amp;lt;- rep(NA, nrow(pdf.Y))

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  # with reference to the above equation
  pdf.Y$risk.L0[i] &amp;lt;- sum(                     # sum over j
                        loss0(y, a[i]) *       # loss
                        pdf.Y$P                # posterior
                      ) 
}

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L0  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{0})$&amp;#39;)) +
  basictheme

print(risk.plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-optimal-decision&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Optimal Decision&lt;/h1&gt;
&lt;p&gt;Our question is now: what is the best &lt;strong&gt;action&lt;/strong&gt; – or decision – over our candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; to choose as the point summary given the loss function &lt;span class=&#34;math inline&#34;&gt;\(L_{0}\)&lt;/span&gt; and the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The answer is, the &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; that &lt;strong&gt;minimises&lt;/strong&gt; the risk (expected loss). Implementing this, we arrive at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# find the $a_{i}$ that minimises the risk function
min.risk.L0 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L0 ) ]

risk.plot &amp;lt;- risk.plot + 
    geom_vline( xintercept = min.risk.L0, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Minimum = &amp;quot;, min.risk.L0 ), 
            x = mode.pdf.Y, y = 0.99)
print( risk.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Compare with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.arrange(risk.plot, mode.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result then, can be summarised as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Under the &lt;strong&gt;zero-one loss function&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(L_0\)&lt;/span&gt;, the action/decision &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; (point summary) which minimises the risk function (expected loss) is the &lt;strong&gt;mode&lt;/strong&gt; of the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;other-loss-functions&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Other Loss Functions&lt;/h1&gt;
&lt;p&gt;We can repeat the same process as for section &lt;a href=&#34;#sec-loss&#34;&gt;3&lt;/a&gt; through &lt;a href=&#34;#sec-optimal-decision&#34;&gt;5&lt;/a&gt; with different loss functions.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;linear loss&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L_{1}\)&lt;/span&gt; loss is defined as:
&lt;span class=&#34;math display&#34;&gt;\[
L_{1}(a_{i},y_{j}) = \begin{cases}
          c_{1} |a_{i} - y_{j}| &amp;amp; \text{ if } a_{i} \leq y_{j} \\
          c_{2} |a_{i} - y_{j}| &amp;amp; \text{ if } a_{i} &amp;gt; y_{j} \\
    \end{cases}
\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(c_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_{2}\)&lt;/span&gt; are constants. If &lt;span class=&#34;math inline&#34;&gt;\(c_{1} = c_{2}\)&lt;/span&gt; we arrive at the &lt;strong&gt;median&lt;/strong&gt; of the posterior, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the linear loss function
loss1 &amp;lt;- function( y, a, c1, c2 ) {
  ifelse(
    a &amp;lt;= y, c1 * abs( a - y ),
            c2 * abs( a - y )
  )
}

pdf.Y$risk.L1 &amp;lt;- rep(NA, nrow(pdf.Y))

# set constants equal
c1 &amp;lt;- c2 &amp;lt;- 1

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L1[i] &amp;lt;- sum( 
                        loss1(y, a[i], c1, c2) * 
                        pdf.Y$P
                      ) 
}

# find the minimum
min.lossL &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1 ) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum of the risk function is 0.355:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L1  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{1})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL, colour = &amp;quot;black&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL, 2) ), 
              x = min.lossL, y = 0.4) +
  basictheme

median.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = median( samples ), colour = &amp;quot;black&amp;quot;, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Median = &amp;quot;, round( median(samples), 2 ) ), 
            x = median(samples), y = 0.01)

grid.arrange(risk.plot, median.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More generally, for positive constants, the &lt;span class=&#34;math inline&#34;&gt;\(c_{1}/(c_{1}+c_{2})\)&lt;/span&gt; quantile of the posterior distribution can be found. For example, we can obtain the 25th and 75th percentiles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y$risk.L1_75 &amp;lt;- rep(NA, nrow(pdf.Y))
pdf.Y$risk.L1_25 &amp;lt;- rep(NA, nrow(pdf.Y))

# set constants
q95 &amp;lt;- 0.75
q05 &amp;lt;- 0.25
c1 &amp;lt;- 1
c2.q95 &amp;lt;- (c1/q95) - c1
c2.q05 &amp;lt;- (c1/q05) - c1

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L1_75[i] &amp;lt;- sum( 
                            loss1(y, a[i], c1, c2.q95) * 
                            pdf.Y$P
                         ) 
  pdf.Y$risk.L1_25[i] &amp;lt;- sum( 
                            loss1(y, a[i], c1, c2.q05) * 
                            pdf.Y$P
                         ) 
}

# find the minima
min.lossL_75 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_75 ) ]
min.lossL_25 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_25 ) ]

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L1_75  ), colour = &amp;quot;#1f78b4&amp;quot; ) +
  geom_line( aes( x = mids, y = risk.L1_25  ), colour = &amp;quot;#33a02c&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{1})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL_75, colour = &amp;quot;#1f78b4&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL_75, 3) ), 
              x = min.lossL_75, y = 1.25) +
  
  geom_vline( xintercept = min.lossL_25, colour = &amp;quot;#33a02c&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL_25, 3) ), 
              x = min.lossL_25, y = 1.25) +
  basictheme

fun.q75 &amp;lt;- as.numeric( round( quantile(samples, probs = c(0.75)), 3 ) )
fun.q25 &amp;lt;- as.numeric( round( quantile(samples, probs = c(0.25)), 3 ) )

quantile.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = fun.q75, colour = &amp;quot;#1f78b4&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;75th = &amp;quot;, fun.q75 ), 
              x = fun.q75, y = 0.01) +
      geom_vline( xintercept = fun.q25, colour = &amp;quot;#33a02c&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;25th = &amp;quot;, fun.q25 ), 
              x = fun.q25, y = 0.01)

grid.arrange(risk.plot, quantile.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the top panel of the figure above, the green and blue lines represents the risk using the linear loss function with constants configured to locate the 25th and 75th percentile respectively. The bottom panel shows the same result obtained directly from the &lt;code&gt;quantile()&lt;/code&gt; function operating directly on the raw &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To complete the loss functions for common measures of central tendency, we have the &lt;span class=&#34;math inline&#34;&gt;\(L_{2}\)&lt;/span&gt; &lt;strong&gt;quadratic&lt;/strong&gt; loss function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_{2}(a_{i},y_{j}) = (a_{i} - y_{j})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the result for quadratic loss:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the quadratic loss function
loss2 &amp;lt;- function( y, a ) {
  return( 
    (a - y)^2 
  )
}

pdf.Y$risk.L2 &amp;lt;- rep(NA, nrow(pdf.Y))

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L2[i] &amp;lt;- sum( 
                        loss2(y, a[i]) * 
                        pdf.Y$P
                      ) 
}

# find the minimum
min.lossL2 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L2 ) ]

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L2  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{2})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL2, colour = &amp;quot;black&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL2,3) ), 
              x = min.lossL2, y = 0.25) +
  basictheme

mean.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = mean( samples ), colour = &amp;quot;black&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Mean = &amp;quot;, round( mean( samples ), 3 ) ), 
              x = mean(samples), y = 0.005)

grid.arrange(risk.plot, mean.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The estimate from &lt;code&gt;quantile()&lt;/code&gt; plotted on the bottom panel differs from the top panel (the mean estimated by minimising the risk function) due to the granularity (&lt;code&gt;delta = 1/100&lt;/code&gt;) of the approximation of the distribution function used in the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-duda2012pattern&#34; class=&#34;csl-entry&#34;&gt;
Duda, Richard O, Peter E Hart, and David G Stork. 2012. &lt;em&gt;Pattern Classification&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-green1966signal&#34; class=&#34;csl-entry&#34;&gt;
Green, David Marvin, and John A Swets. 1966. &lt;em&gt;Signal Detection Theory and Psychophysics&lt;/em&gt;. Wiley New York.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Routines and Workflow: Adaptations around the Electronic Health Record</title>
      <link>/post/routines-and-the-ehr/</link>
      <pubDate>Wed, 26 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/post/routines-and-the-ehr/</guid>
      <description>&lt;p&gt;The motivation for this post is a series of discussions I&amp;rsquo;ve had with colleagues over the past few weeks involving the potential for electronic health/medical records (EHRs or EMRs) for improving clinical care and their utility for research.  All of these conversations have been in the context of how we use clinical data to better record and understand patient&amp;rsquo;s state, trajectories, treatments/interventions, adverse events and outcomes.  In this post, I discuss an example of how EHRs are used in a workflow for clinical practice and highlight opportunities for improving EHRs.&lt;/p&gt;
&lt;p&gt;From my perspective, a central theme is how given a set of available tools (including an EHR), my workflow in clinical practice includes &lt;strong&gt;work-arounds&lt;/strong&gt; and &lt;strong&gt;routines&lt;/strong&gt; analogous to what Herbert Simon called &lt;a href=&#34;https://en.wikipedia.org/wiki/Satisficing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;satisficing&amp;rdquo;&lt;/a&gt; &amp;ndash; finding satisfactory (often sub-optimal) solutions to account for the realities of the environment.&lt;/p&gt;
&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;There were some dominant themes emerging from discussions with colleagues around the use of EHRs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;clinical records contained in the EHR represent a kind of &lt;strong&gt;ground-truth&lt;/strong&gt; e.g. about clinical states, trajectories, diagnoses and treatments&lt;/li&gt;
&lt;li&gt;that free-text records in the EHR contain &lt;strong&gt;sufficient signal (relative to noise)&lt;/strong&gt; and that we can develop research tools which can reliably extract usable research content, for example, using natural language processing (by both rule-based as well as statistical/probabilistic methods)&lt;/li&gt;
&lt;li&gt;that &lt;strong&gt;structured data&lt;/strong&gt; in the EHR is routinely collected and can be considered more reliable than free-text &amp;ndash; this includes demographics, coded diagnostic data, structured psychometric instruments/questionnaires/scales and patient-reported outcome measures as well as service utilisation data (e.g. general practitioner contacts, temporal event data representing contact with services such as admissions to hospital)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;non-native data&lt;/strong&gt; (e.g. generated outside the EHR) data is repositoried in the EHR (e.g. clinic letters, admission/discharge summaries) by being uploaded and represent a further source of clinical data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One important and recurring theme in each conversation was that clinician workflow and the situated practice of clinical use of EHRs was unexplored.  This is important because workflow (importantly, the work-arounds that clinicians find and use) impacts on the content of the EHR.  Below, I describe my current workflow with an EHR that I use every day and highlight some of the problems arising and opportunities for developing EHRs in a way that actively assists clinical recording and decision making.&lt;/p&gt;
&lt;p&gt;EHRs are supposed to be an improvement on paper medical records &amp;ndash; my experience suggests this is absolutely true &amp;ndash; but there are new problems that emerge from how these systems are embedded in and used by clinical teams.  As a preview, my principle complaint with EHRs is that they often default to being a &lt;a href=&#34;https://en.wikipedia.org/wiki/Document-oriented_database&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;document-oriented database&lt;/a&gt; but without any of the query, retrieval or data-modelling sophistication. They end up being a &amp;lsquo;dumping ground&amp;rsquo; for recording every kind of clinical activity, in part I suspect, because of their medico-legal status.&lt;/p&gt;
&lt;p&gt;Over the past decade, I&amp;rsquo;ve used 13 different systems (excluding more narrow-functionality, specialty-specific systems e.g. for radiology):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;three different commercial, proprietary EHR systems designed for secondary care in psychiatry/mental health&lt;/li&gt;
&lt;li&gt;three different commercial, proprietary EHRs for acute (general, secondary care) hospitals&lt;/li&gt;
&lt;li&gt;four different emergency department EHRs for acute hospitals (each commercial and proprietary)&lt;/li&gt;
&lt;li&gt;two &amp;ldquo;custom&amp;rdquo; EHRs built in-house for two different secondary care, acute hospitals&lt;/li&gt;
&lt;li&gt;a single proprietary primary care (general practice) EHR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Common to the my experience and use of &lt;strong&gt;all&lt;/strong&gt; of these EHRs, is I have to find a workflow and set of practices around these systems and this has remained relatively constant: All of the intellectual work is done using applications &lt;strong&gt;separate&lt;/strong&gt; from the actual EHR because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;generally, none of the EHRs provide tools for the kinds of &lt;strong&gt;information processing&lt;/strong&gt; that I need in clinical practice (more below)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I have not used &lt;strong&gt;any&lt;/strong&gt; EHR that seamlessly inter-operates with another &amp;ndash; for example, if I want to combine radiology, pathology and my clinical notes, I have to copy-and-paste between applications hoping that data displayed in each application can be made compatible by exchanging it via a &lt;strong&gt;lowest common denominator&lt;/strong&gt; or minimal common exchange format &amp;ndash; which is invariably plain, free-text.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One consequence of the above is that clinicians tend to treat the EHR as a data repository (particularly, for the free-text and non-native format storage) and as a medico-legal document (i.e. if you haven&amp;rsquo;t recorded some activity as an entry on the EHR, then it didn&amp;rsquo;t happen)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;none of the EHRs that I&amp;rsquo;ve used is &lt;strong&gt;engineered to be reliable enough&lt;/strong&gt; that I can trust it &amp;ndash; for example, my current EHR runs as a web application &amp;ndash; but appears to only be guaranteed (tested) to work on a single browser platform; it either crashes frequently (often losing draft or in-progress work) or &amp;lsquo;times out&amp;rsquo; my connection (for governance and information security reasons, because I&amp;rsquo;ve been inactive for a period of around 15 minutes) without a robust draft/auto-save facility.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;example-workflow&#34;&gt;Example Workflow&lt;/h1&gt;
&lt;p&gt;Over the years, I&amp;rsquo;ve developed a workflow which is optimal for me &lt;strong&gt;given&lt;/strong&gt; the above constraints.  My hope is to highlight how these represent work-arounds for the problems highlighted above.  The context here is a weekly out-patient clinic; people attending the clinic are a combination of new referrals (for example, from GPs or other clinicians in secondary care) and people being followed-up in treatment.&lt;/p&gt;
&lt;p&gt;Typically, there are between 6 and 8 patients in a 4 hour clinic; people who are new or I&amp;rsquo;ve not met before are often given 45 minutes to 1 hour appointments and people attenting for &amp;lsquo;check up&amp;rsquo; or treatment monitoring are usually given 30 minute appointments.&lt;/p&gt;
&lt;h2 id=&#34;for-new-patients&#34;&gt;For New Patients&lt;/h2&gt;
&lt;p&gt;As I describe this process, I&amp;rsquo;ll keep a running total the number of unique applications and instances/windows for these applications that I need.&lt;/p&gt;
&lt;p&gt;For people new to the clinic (or that I&amp;rsquo;ve not met before):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;m going to need access to my personal library of prescribing guidelines, literature and instruments/scales/ questionnaires.  I open the file system browser and navigate to my networked storage.  I prefer to do web browsing using a different browser to one our EHR runs on, so I launch this often navigating straight to the British National Formulary (BNF) so it&amp;rsquo;s easily available to me.  That&amp;rsquo;s &lt;strong&gt;two applications&lt;/strong&gt; off the bat.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I launch the EHR (open the default web-browser, navigate to the EHR link) and my goal is to establish a history of the person&amp;rsquo;s contact with services and to try and understand what the historical and current difficulites are.  So we now have &lt;strong&gt;three applications&lt;/strong&gt; open and running.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;My EHR is a multi-tabbed web application where the main display panel reflects the currently-selected tab.  The EHR is organised (roughly) into&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;structured data (outcomes, questionnaires, psychometric scales/instruments; diagnostic coding; demographics)&lt;/li&gt;
&lt;li&gt;the &amp;ldquo;Correspondence&amp;rdquo; section/tab which acts as a repository for any non-native data (scanned documents/letters; investigations including bloods, ECGs, radiology reports from systems not compatible with the EHR; admission/discharge summaries usually in an office application format; email correspondence)&lt;/li&gt;
&lt;li&gt;the free-text &amp;ldquo;Clinical Notes&amp;rdquo; section where most clinical activity is recorded and (importantly) replicates information from the two other sections&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I usually avoid the tabs for &amp;ldquo;Clinical Notes&amp;rdquo; and go straight to a menu at the top of the EHR application where I can opt for the main panel to show me the entire record of these free-text entries.  As this free-text record often replicates and brings together in a single location much of the data recorded in different tabs I prefer to use this as a starting point. I read the free-text entries in chronological order &amp;ndash; as the first point of contact/referral is recorded at the end of this document I read from the bottom-up.  Of note, this is &lt;strong&gt;less time consuming&lt;/strong&gt; than clicking to read each entry individually in the default list format for the &amp;ldquo;Clinical Notes&amp;rdquo;.  Also, when reading a single note/entry individually, the EHR application displays formatting meta-data (e.g. numbered or unordered lists) as &lt;em&gt;plain&lt;/em&gt; text, without intepretation or formatting for display.  Most importantly, I can exploit the browser&amp;rsquo;s free-text search facility to e.g. quickly navigate to keywords like &amp;ldquo;medication&amp;rdquo;, &amp;ldquo;ecg&amp;rdquo;, &amp;ldquo;bloods&amp;rdquo; &amp;ndash; because the EHR web application doesn&amp;rsquo;t have a search facility.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;
&lt;p&gt;I launch another instance of the EHR browser window so I can have access to &lt;strong&gt;other tabs&lt;/strong&gt; in parallel; notably, I want the &amp;ldquo;Correspondence&amp;rdquo; tab which contains scanned documents (usually PDFs, sometimes bitmapped TIFF or GIF images arising from scanned paper documents) including the original referral letter.  Often, ECGs are scanned and placed here also.  So I open a PDF reader so I can view these.  We&amp;rsquo;re at &lt;strong&gt;three applications&lt;/strong&gt; and for the EHR application, &lt;strong&gt;two instances or windows&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I open another browser window, and navigate to our pathology system (a separate web application) where I can find the most recent blood tests, letters from other specialties, radiology reports and records of admissions or clinic attendances to other local acute general hospitals.  I use a separate login identity and password because this system is operated by another NHS Trust.  This brings us to &lt;strong&gt;four applications&lt;/strong&gt; and &lt;strong&gt;two instances&lt;/strong&gt; for the EHR&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While I&amp;rsquo;m reviewing these notes, I open a word processor, and start making brief notes summarising the salient points from the EHR.  This &amp;ldquo;main clinic document&amp;rdquo; is the center-piece of my workflow.  I also copy and paste the NHS number from the EHR (a person&amp;rsquo;s &lt;em&gt;mostly&lt;/em&gt; unique identifier across the NHS), which is formatted as numeric data with the format &amp;ldquo;XXX XXX XXXX&amp;rdquo; &amp;ndash; then I edit out the whitespace because to copy-paste the NHS number to the pathology system, it expects a single, uninterrupted string of numerals.  This is more efficient than trying to search the pathology system on name, date of birth etc.  Now we have &lt;strong&gt;five applications&lt;/strong&gt; and &lt;strong&gt;two instances&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On average, for each clinic appointment, I&amp;rsquo;m spending between 15 minutes to an hour (for very complex cases) working with this workflow by quick-tabbing (ALT+Tab) between applications/windows, making notes, navigating the EHR, copy-pasting, looking up results and so on.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;focus&lt;/strong&gt; of this information gathering exercise is the word processor document (the &amp;ldquo;main clinic document&amp;rdquo;), where I&amp;rsquo;m collating together all information that has direct utility for the forthcoming clinic appointment for that patient.&lt;/p&gt;
&lt;p&gt;Importantly, I&amp;rsquo;m copying and pasting information from the pathology application, the EHR and PDF reader (when the documents are not bitmapped, scanned images &amp;ndash; where in these cases I have to retype the data).  I have my word processor set to default to &amp;lsquo;paste as plain text&amp;rsquo; to avoid formatting problems when e.g. copying tabulated data from the pathology web application to my evolving clinical notes for the clinic appointment.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll keep this workflow &amp;lsquo;open&amp;rsquo; and repeat the process for each person to be seen in clinic.  As I proceed, the &amp;ldquo;main clinic document&amp;rdquo; is a single, growing document which contains page-separated summaries for every person to be seen.  For each person, this typically takes the form of: the person&amp;rsquo;s recent history with health services, referral details, history of and presenting complaint/current difficulites, a history of their investigations/test results to date (or pending), relevant medical history, medications history, side-effects/history of adverse reactions or allergies, personal / social circumstances and some of my initial thoughts about issues that need addressing in clinic, outstanding investigations and an initial management plan.  Most of this information is gathered from the free-text &amp;ldquo;Clinical Notes&amp;rdquo; function of the EHR, navigated using the free-text search facility provided by the browser.&lt;/p&gt;
&lt;h2 id=&#34;for-known-patients&#34;&gt;For &amp;lsquo;Known&amp;rsquo; Patients&lt;/h2&gt;
&lt;p&gt;The above workflow is modified for a person I&amp;rsquo;ve met before:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Instead of reading the entire set of clinical notes from the earliest to the most recent entry, I&amp;rsquo;ll still request the EHR displays all clinical notes, but read from the top (most recent).  I&amp;rsquo;ll review what has happened since I last met with this person for example, notes from social workers, occupational therapists, psychologists, pharmacists, care coordinators.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the second of the EHR browser instances, I&amp;rsquo;ll look in &amp;ldquo;Correspondence&amp;rdquo; where I&amp;rsquo;ll locate my &lt;strong&gt;last clinic letter&lt;/strong&gt;.  I&amp;rsquo;ll open this letter which launches another document window in the word processor application.  From here, I&amp;rsquo;ll copy the &amp;lsquo;proforma&amp;rsquo; part of the letter (which contains demographics, clinicians involved in their care, medication lists and diagnoses) and paste into my evolving &amp;ldquo;main clinic document&amp;rdquo;.  I&amp;rsquo;ll also copy the last &amp;ldquo;Plan&amp;rdquo; we made so I can refer to and update it with any progress notes.  Finally, I&amp;rsquo;ll also copy and paste the last mental state examination and the list of &amp;ldquo;early warning signs&amp;rdquo; (for relapse) which are contained in the clinic letter so I can refer to these during the clinic appointment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When relevant, I&amp;rsquo;ll switch to the browser window containing the EHR &amp;ldquo;Clinical Notes&amp;rdquo; and summarise any investigations which any of the team have placed in the free-text record.  This might be, for example, entries summarising blood results planned from the last appointment, imaging or other assessments that help formulation/diagnosis or will influence management.  Sometimes, I&amp;rsquo;ll go to the separate browser running the pathology web application if I can&amp;rsquo;t locate information or it&amp;rsquo;s summarised too briefly for example, when someone has checked some bloods and written &amp;ldquo;Bloods checked, NAD&amp;rdquo; as a placeholder to record the activity rather than the results themselves.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Also, when relevant or required, I&amp;rsquo;ll need to look at the &amp;ldquo;Correspondence&amp;rdquo; tab to see written communication from other teams or specialties and make notes in the main clinic document so it&amp;rsquo;s all available in one place during the clinic appointment.  It&amp;rsquo;s worth noting that very often, email correspondence between appointments with other professionals are copied into the &amp;ldquo;Clinical Notes&amp;rdquo; free-text to formally record the outcomes of inter-professional or agency working.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, I&amp;rsquo;ll usually open a spreadsheet application and open an existing (or create a new) spreadsheet for each person where I&amp;rsquo;ve previously completed quetionnaires or instruments (usually, these are clinician and patient-reported structured data which can&amp;rsquo;t be easily managed inside the EHR because they aren&amp;rsquo;t &amp;lsquo;standard forms&amp;rsquo;).   These are most often structured, quantitative data on depressive/affective symptoms, tools like the Scale for Assessment of Negative Symptoms (SANS), PANSS and cognitive tests/screening.  These spreadsheets are all located on my personal Trust-based secure storage for compliance with information governance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In total, to prepare for a clinic I&amp;rsquo;ll have &lt;strong&gt;six applications&lt;/strong&gt; running.&lt;/p&gt;
&lt;h1 id=&#34;before-and-during-a-clinic&#34;&gt;Before and During A Clinic&lt;/h1&gt;
&lt;p&gt;I might tidy up my desktop environment a bit before clinic &amp;ndash; for example, closing the PDF reader, one or two browser instances and so on.  Before the clinic gets started, I&amp;rsquo;ll have a separate &lt;strong&gt;calender application&lt;/strong&gt; open (our EHR provides a calender / scheduling tool, but it&amp;rsquo;s not used by clinicians or the administration team) &amp;ndash; this shows me the timing for the clinic, and should reflect same order as the &amp;ldquo;main clinic document&amp;rdquo; in my word processor.  I&amp;rsquo;ll also print any hardcopies of materials I&amp;rsquo;ll need for each patient (for example, I&amp;rsquo;ll get any mandatory or useful clinical questionnaires printed) &amp;ndash; we don&amp;rsquo;t have tools for electronic administration of the clinical instruments and questionnaires I commonly use.&lt;/p&gt;
&lt;p&gt;During clinic, with the person&amp;rsquo;s permission, I&amp;rsquo;ll make short-hand notes about important issues directly into the main clinic document as we go along.  I&amp;rsquo;ll also record the plans / changes to management we agree in clinic including any planned investigations, appointments with other members of the team and important communication to their GP.  I&amp;rsquo;ll collate paperwork completed during the clinic for entry later on (there&amp;rsquo;s not enough time between appointments).  However, between appointments, in the calender application I&amp;rsquo;ll often put in calender reminders / events to prompt me to follow up parts of the plan made during the previous appointment.&lt;/p&gt;
&lt;h1 id=&#34;after-a-clinic&#34;&gt;After A Clinic&lt;/h1&gt;
&lt;p&gt;Typically, I&amp;rsquo;ll go back to the start of the main clinic document, and for each person, complete the following steps to arrive at a summative document which will form the basis of a clinic letter to other professionals and &amp;lsquo;double up&amp;rsquo; as a record to go in the EHR &amp;ldquo;Clinical Notes&amp;rdquo; free-text:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Collate any paperwork (quetionnaires, scales) and manually enter these into a spreadsheet which captures historical and current measures that are important.  For example, the person&amp;rsquo;s currently reported depression symptoms inventory, results from brief cognitive tests and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I&amp;rsquo;ll complete the mental state examination (MSE) for each person seen by expanding on the notes I made during clinic.  While doing this, I compare this to the previous MSE (which are usually pasted in the main clinic document) for that person &amp;ndash; noting any important changes.  I&amp;rsquo;ll end the MSE with an &amp;lsquo;impression&amp;rsquo; which summarises important information on the current MSE and often, highlighting changes with respect to previous MSEs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first part of the clinic letter is then written; summarising the last time / duration since the person was seen and   a narrative description including any issues brought to clinic by the patient, recent issues/events I&amp;rsquo;ve collated from reading the EHR notes (for example, I might highlight any recent investigations). The latter part of the letter will summarise any quantitative (structured) information usually from my spreadsheet-based data &amp;ndash; for example, I might note the current salient items in a symptoms inventory commenting on changes from any previous measures.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The letter concludes with an updated plan, noting any actions to be completed (by whom and when) from this clinic appointment and highlighting anything that remains pending from previous clinic plans.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the end of this process, I usually have between one and three pages of information which will include the clinic letter and then:&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;I will send the letter content for each patient via email to the team&amp;rsquo;s administrators, who will format, electronically sign and place the relevent letter heads onto the document.  They also direct the correspondence out by post or email to the patient and any clinicians indicated in the &amp;lsquo;cc&amp;rsquo; list.  The administrators will also upload the finalised letter to the &amp;ldquo;Correspondence&amp;rdquo; section of a patient&amp;rsquo;s EHR.  This often happens within 24-48 hours depending on how many clinicians are requesting help with administration as well as when the clinic finishes (often, for an afternoon clinic, the post-clinic work will take place after office hours).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I copy the text of the letter into the EHR &amp;ldquo;Clinical Notes&amp;rdquo; &amp;ndash; while, strictly, this should be redundant (as the administrators will evenutally place it on &amp;ldquo;Correspondence&amp;rdquo;) it is often necessary for contemporaneous recording; for example, should the patient be seen out of hours in a crisis by other clinicians.  Often, in doing this, I use the minimal common exchange format &amp;ndash; plain text &amp;ndash; because I can&amp;rsquo;t trust the EHR to format information beyond this.  I can&amp;rsquo;t use graphical or tabular data reliably.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This process is repeated for each person seen in clinic.  It requires &lt;strong&gt;four separate applications&lt;/strong&gt; (word processor, spreadsheet, web browser for the EHR and an email/calender application).  Notice how the intellectual and administrative parts of this work are independent of the EHR; the main clinic document is the focal point, and the other applications allow for collating and summarising data and communicating the output of this process with other team members (notably, the administration team).  Only at the end of this process does the EHR factor in as a repository for a document.&lt;/p&gt;
&lt;h1 id=&#34;time-costs&#34;&gt;Time Costs&lt;/h1&gt;
&lt;p&gt;For the pre-clinic workup:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To prepare for a clinic where everyone is new, I&amp;rsquo;ll need between 2 and 4 hours.  At the end of this, I&amp;rsquo;ll have one document (in the word processor) containing all the relevant information for each patient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When their&amp;rsquo;s a mixture of new and known people, the cumulative information contained in previous clinic letters helps reduce the time &amp;ndash; but it comes in around 2 hours on average.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the post-clinic work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Depending on complexity, I can write and compare MSEs in around 10-20 minutes for each person.  It&amp;rsquo;s worth noting that the MSE should contain enough information to facilitate my comparison / estimate of change as well as communicate with other professionals.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The remainder of the letter takes a further 10-15 minutes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In total, for a four-hour clinic, there&amp;rsquo;s an average of between 2-4 hours of post-clinic work.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;core-components-of-the-workflow&#34;&gt;Core Components of the Workflow&lt;/h1&gt;
&lt;h2 id=&#34;central-working-space&#34;&gt;Central &amp;lsquo;Working Space&amp;rsquo;&lt;/h2&gt;
&lt;p&gt;This role is fulfilled by the &amp;ldquo;main clinic document&amp;rdquo; in the word processing application that collects together information from at least two locations in the EHR (most often, the free-text &amp;ldquo;Clinical Notes&amp;rdquo; and the &amp;ldquo;Correspondence&amp;rdquo; repository), a separate pathology system and previous clinic letters.&lt;/p&gt;
&lt;p&gt;It is required because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the EHR does not provide a facility to collate information together from multiple sources in a &amp;ldquo;workspace&amp;rdquo; or scratch-pad that can be used to display, edit, synthesise information and to begin formulating a record of the clinical encounter. For example, I can&amp;rsquo;t ask the EHR to display my last MSE so I can conveniently compare to the current clinic MSE. Very often, the important clinical information is the &lt;strong&gt;change&lt;/strong&gt; in the clinic MSE &lt;strong&gt;relative&lt;/strong&gt; to the last recorded MSE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In an ideal world, I&amp;rsquo;d like the MSE to be a structured record from which a &amp;lsquo;narrative&amp;rsquo; version (for the clinic letter) can be generated &amp;ndash; this would reduce the time spent (redundantly) re-typing the same thing over and over.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the EHR&amp;rsquo;s method of recording medications or interventions trialled is cumbersome to the point no-one uses it; instead, the free-text &amp;ldquo;Clinical Notes&amp;rdquo; become the primary source of information &amp;ndash; it is, of course, unstructured and subject to idiosyncratic recording between clinicians.  As an example, if I want to compile a history of medications trialled, I have to manually search, using the browser search facility, and copy and paste information to my main clinic document.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;clinic letters and the &amp;ldquo;Correspondence&amp;rdquo; tab (really, just a repository of anything that is &lt;em&gt;not&lt;/em&gt; free text in the &amp;ldquo;Clinical Notes&amp;rdquo; tab) are the best cumulative source of information for the person&amp;rsquo;s trajectory during treatment &amp;ndash; they contain plans (which evolve), describe change in the person&amp;rsquo;s condition, highlight unmet needs and often contain the clinical reasoning that justifies changes in management.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;during clinics, I have &lt;strong&gt;most information in one place&lt;/strong&gt; &amp;ndash; I don&amp;rsquo;t have to rely on other web-based applications which are unstable, don&amp;rsquo;t inter-operate or have cumbersome user interfaces to quickly find the information I need prompt access to (i.e. to minimise disruption to the patient&amp;rsquo;s time during the clinic appointment)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the EHR is not stable enough to rely upon during clinic appointments or to remain stable for the time after a clinic &amp;ndash; in short, if you don&amp;rsquo;t store your notes in a word processor application, you&amp;rsquo;ll likely loose them before you have a chance to finalise them.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;spreadsheets-for-structured--quantitative-data&#34;&gt;Spreadsheets for Structured / Quantitative Data&lt;/h2&gt;
&lt;p&gt;The EHR I work with can record some structured clinical data (for example, important patient-reported outcome measures (PROMs) have their own structured form-based input), but it&amp;rsquo;s hard to locate and find the facility quickly and most importantly, there is no way of visualising patterns and changes in serial measurements.&lt;/p&gt;
&lt;p&gt;As an example, currently, to use PROMs for audit or research on outcomes, there is a research assistant who manually re-enters the same data into spreadsheets and then uses the spreadsheet application to produce visualisations.  So, for one questionnaire, we have a paper form that is entered onto two systems (the EHR and a standalone spreadsheet), by two separate people simply to visualise, understand trends and compile meaningful outcome data.  The team has to repeat a similar exercise when compiling compliance data for national audits &amp;ndash; our EHR doesn&amp;rsquo;t provide functionality that means we don&amp;rsquo;t have to &amp;lsquo;parallel record&amp;rsquo; this information.&lt;/p&gt;
&lt;p&gt;Further, I would have to wait for an EHR implementation team to create a structured form should we decide to use something targetted to a particular clinical population, team/clinic or condition.&lt;/p&gt;
&lt;p&gt;So, the tactical solution of using one&amp;rsquo;s own spreadsheet-based implementation for structured data provides:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a familiar user interface for tabular data entry for questionnaires, PROMs and structured clinical instruments&lt;/li&gt;
&lt;li&gt;quick and low-cost implementation of tools without waiting for a team to implement in the EHR&lt;/li&gt;
&lt;li&gt;a way of visualising patterns / changes over time (imperfectly, because we&amp;rsquo;re really using a generic office application  but it&amp;rsquo;s certainly better than nothing)&lt;/li&gt;
&lt;li&gt;some (limited) flexibility in how this data is then migrated to the &amp;ldquo;main clinic document&amp;rdquo; to form part of the clinic letter / summary of the clinical encounter&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;the-current-role-of-the-ehr&#34;&gt;The Current Role of the EHR&lt;/h1&gt;
&lt;p&gt;As described in my workflow (and this seems familiar to other clinicians I work with):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the &lt;strong&gt;focus&lt;/strong&gt; of the intellectual and administration work is &amp;ndash; almost exclusively &amp;ndash; the word processing application which acts as a common workspace for the whole operation; for each person seen in clinic, the &amp;ldquo;main clinic document&amp;rdquo; summarises recent history (e.g. from previous clinic letters and notes entered onto the free-text &amp;ldquo;Clinical Notes&amp;rdquo; tab), previous plans, recent investigation results and acts as the location where this information and the current clinical encounter come together&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the spreadsheet application runs &amp;lsquo;parallel&amp;rsquo; to facilitate structured data collection that &lt;em&gt;may be provided&lt;/em&gt; in the EHR, but where the utilisation of this data (and exchange of data with the &amp;ldquo;main clinic document&amp;rdquo;) is not provided for.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, notice how the EHR enters this workflow at only two points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;First, in the pre-clinic workup, where it is used as a source repository for a) recent clinical encounters (which are, mostly, composed using the same word processing application because the EHR is unstable) and b) centralised repository of any clinical documentation or communication between professionals, teams and agencies&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second, right at the end of the workflow, copies of the recent clinical encounter are pasted as a free-text entry (in the &amp;ldquo;Clinical Notes&amp;rdquo; tab) and the the clinic letter uploaded to the &amp;ldquo;Correspondence&amp;rdquo; tab.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In what I&amp;rsquo;ve described here, the EHR functions primarily as a document-based storage facility.  What is notably absent is functionality in the EHR that &lt;strong&gt;supports clinical decision making&lt;/strong&gt;.  To compensate, we develop a workflow and supplement functionality from other applications &amp;ndash; more harshly, it&amp;rsquo;s a &lt;a href=&#34;https://en.wikipedia.org/wiki/Kludge&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kludge&lt;/a&gt;.  As a final post-script, notice the frequency with which information and data is &lt;strong&gt;manipulated&lt;/strong&gt; and manually &lt;strong&gt;moved between different applications&lt;/strong&gt; in an attempt to facilitate the actual clinical encounter.  This is largely because of the EHR-imposed need to have a minimal, common exchange format and that happens to be plain text for the most part.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clinical State: Part One - Dynamical Systems</title>
      <link>/post/2021-05-23-clinical-trajectories-pt-one/</link>
      <pubDate>Sun, 12 May 2019 18:00:00 +0100</pubDate>
      <guid>/post/2021-05-23-clinical-trajectories-pt-one/</guid>
      <description>
&lt;script src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;In this series of blogposts, we look at some models of clinical state. The motivation is to document exploratory work with a colleague (Nick Meyer, who runs the &lt;a href=&#34;https://sleepsight.org/&#34;&gt;SleepSight&lt;/a&gt; study) as we try and apply some theoretical ideas – for example &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-nelson2017moving&#34; role=&#34;doc-biblioref&#34;&gt;Nelson et al. 2017&lt;/a&gt;; &lt;a href=&#34;#ref-scheffer2009early&#34; role=&#34;doc-biblioref&#34;&gt;Scheffer et al. 2009&lt;/a&gt;)&lt;/span&gt; – to ‘real-life’ data. This problem is interesting because the psychiatric literature more often than not uses an aggregate measure of either state or trajectory, and sometimes, there is no distinction made between the person’s clinical state, a measurement of this state and an ‘outcome.’ As examples, most will be familiar with measuring the total (aggregate) score over some scale or instrument (e.g. HAMD in depression, PANSS in psychotic disorders). Often, we measure this at two time-points – such as before and after treatment – and describe the outcome as a change in this aggregated score. Sometimes, we plot a time-series of these total scores, and call this a trajectory. However, this results in a loss of information (see &lt;a href=&#34;http://www.danwjoyce.com/clinical-state-models&#34;&gt;here&lt;/a&gt;) and is driven by the requirement to be compatible with standard (or perhaps more accurately, off-the-shelf) procedures for analysing such data (i.e. defining a discrete ‘response’ / ‘no-response’ univariate outcome for investigating the efficacy/effectiveness of an intervention).&lt;/p&gt;
&lt;div id=&#34;basics&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Basics&lt;/h1&gt;
&lt;p&gt;Throughout, we will assume that there are measurements of clinical state, obtained by some instrument, usually with some noise added. Further, for the purposes of this post, we assume that there is some latent process being measured by these instruments and we use clinical symptoms as a concrete example (but the principles generalise to anything that can be measured and taken to represent state). For pedagogical reasons, the easiest example is to consider just two dimensions - for example, in psychosis, we might measure the positive and negative symptom burden.&lt;/p&gt;
&lt;p&gt;To begin, take a time-ordered set of measurements for positive (&lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt;) and negative (&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;) symptoms respectively:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
P &amp;amp;= (29,24,17, \ldots, 12, 11) \\
N &amp;amp;= (26,24,19, \ldots, 22, 25)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Graphically, this looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Individually, we can see that positive symptoms generally decrease over time, but the negative symptoms ‘oscillate.’ Next we define a native &lt;strong&gt;state space&lt;/strong&gt; where instead of treating the two sequences as independent, we form a vector &lt;span class=&#34;math inline&#34;&gt;\(x(t) = (p_t, n_t)\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(p_t \in P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_t \in N\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
x(t) &amp;amp;= \big[ (29,26), (24,24), (17,19), \ldots,(12,22), (11,25)   \big]
\end{aligned}
\]&lt;/span&gt;
So, if we want the state at &lt;span class=&#34;math inline&#34;&gt;\(t=7\)&lt;/span&gt; we get &lt;span class=&#34;math inline&#34;&gt;\(x(7) = (12,22)\)&lt;/span&gt; and similarly, &lt;span class=&#34;math inline&#34;&gt;\(x(2) = (24,24)\)&lt;/span&gt;. Each of these states is naturally a point in two dimensions, visualised as a plane:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, the state space is a finite plane (two-dimensional) which contains &lt;em&gt;all possible&lt;/em&gt; configurations of &lt;span class=&#34;math inline&#34;&gt;\((P,N)\)&lt;/span&gt;, and a single time-ordered sequence of states (numbered 1 through 8, in orange) shows the state &lt;strong&gt;trejectory&lt;/strong&gt; for a single person. We can equip this state space with a &lt;a href=&#34;https://en.wikipedia.org/wiki/Metric_space&#34;&gt;metric&lt;/a&gt; that imports mathematical tools for notions such as the distance between two states. This means we can model the patient’s trajectory in this &lt;strong&gt;native&lt;/strong&gt; state space (preserving information) and only when we absolutely need to, apply mathematical tools to reduce this multi-dimensional representation to a convenient form that enables us to e.g. inspect change or build statistical models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dynamical-system-approach&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Dynamical System Approach&lt;/h1&gt;
&lt;p&gt;As a starting point, &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-nelson2017moving&#34; role=&#34;doc-biblioref&#34;&gt;Nelson et al. 2017&lt;/a&gt;)&lt;/span&gt; consider and survey some theoretical proposals for moving toward dynamic (instead of static) models of the &lt;em&gt;onset&lt;/em&gt; of disorders – notably, they review dynamical systems and network models. Similarly, &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mason2017mood&#34; role=&#34;doc-biblioref&#34;&gt;Mason, Eldar, and Rutledge 2017&lt;/a&gt;)&lt;/span&gt; explore a model of how mood oscillations occur in bipolar disorder and their proposal is superifically similar to a dynamical system with periodic behaviour. The influential work of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-scheffer2009early&#34; role=&#34;doc-biblioref&#34;&gt;Scheffer et al. 2009&lt;/a&gt;)&lt;/span&gt; is also relevant: if one can identify a latent process with a dynamical systems formulation, then a whole theory of &lt;strong&gt;critical transitions&lt;/strong&gt; can be mobilised to explain how perturbations from the system’s equilibirum can ‘break’ the inherent stability of a system leading to a catastrophic change (i.e. relapse). Our starting point here is how &lt;em&gt;operationalize&lt;/em&gt; these ideas.&lt;/p&gt;
&lt;p&gt;Here, we assume that underlying the measured clinical state is some process which behaves according to a putative model. The example used here, and in the literature, is of &lt;strong&gt;damped oscillators&lt;/strong&gt;. A physical analogy helps: imagine a mass attached to a fixed point by a spring. At rest, this system is in equilibrium. If the mass is pulled or pushed (displaced) by a certain amount, the system is moved from equilibrium and will ‘bounce’ up and down with a frequency and amplitude determined by the amount of displacement, the ‘stiffness’ of the spring and any ‘damping’ introduced by the viscosity of the medium. This has been proposed as a model of e.g. mood dysregulation &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-odgers2009capturing&#34; role=&#34;doc-biblioref&#34;&gt;Odgers et al. 2009&lt;/a&gt;)&lt;/span&gt; and symptom trajectory is modelled using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Harmonic_oscillator&#34;&gt;damped oscillator&lt;/a&gt; that is fit to data using for example, regression &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-boker2002method&#34; role=&#34;doc-biblioref&#34;&gt;Boker and Nesselroade 2002&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To begin, we denote the clinical state at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; by &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; (note this is a uni- rather than multi-variate state representation, so for example, consider only the ‘negative symptoms’ plot above). For more discussion of models of damped oscillators, see &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hayek2003&#34; role=&#34;doc-biblioref&#34;&gt;Hayek 2003&lt;/a&gt;)&lt;/span&gt; for an applied physical systems discussion, and for a helpful mathematical tutorial, &lt;a href=&#34;%7Bhttps://www.jirka.org/diffyqs/%7D&#34;&gt;Chapter 2.4&lt;/a&gt; of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Lebl2019diff&#34; role=&#34;doc-biblioref&#34;&gt;Lebl 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A general damped oscillator is described by a linear second-order ordinary differential equation (ODE):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{d^2x}{dt^2} - \zeta \frac{dx}{dt} - \eta x = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With coefficient &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; modelling the ‘decay’ of the oscillations, and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; describing the ‘frequency’ of oscillations.&lt;/p&gt;
&lt;p&gt;To simplify the presentation, we use ‘dot’ notation where &lt;span class=&#34;math inline&#34;&gt;\(\ddot{x}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}\)&lt;/span&gt; are the second and first derivatives respectively:
&lt;span class=&#34;math display&#34;&gt;\[
\ddot{x}(t) - \zeta \dot{x}(t) - \eta x(t) = 0
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rearranging for the second-order term:
&lt;span class=&#34;math display&#34;&gt;\[
\ddot{x}(t) = \zeta \dot{x}(t) + \eta x(t)
\]&lt;/span&gt;
Generally, we only have access to numerical data that we suppose is generated from an underlying damped oscillator process; so we use &lt;a href=&#34;https://en.wikipedia.org/wiki/Numerical_differentiation&#34;&gt;numerical differentiation&lt;/a&gt; to obtain the &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_approximation&#34;&gt;locally-linear&lt;/a&gt; approximation to the derivatives of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To use ODEs as our model, we need to be able to solve them (for example, for fitting and then reconstructing the model for a given set of data). To use off-the-shelf ODE solvers, we need to convert the second order ODE into two first-order equations by writing substitutions:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  y_1 &amp;amp;= x \\
  y_2 &amp;amp;= \dot{x} = \dot{y_1} \\
\end{aligned}
\]&lt;/span&gt;
So :
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \dot{y_2} &amp;amp;= \zeta y_2 + \eta y_1 \\
  \dot{y_1} &amp;amp;= y_2
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The analytic solution for this system is well understood and depends on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; – there are three solutions for &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; depending on whether the system is damped, under-damped or critically damped – &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-Lebl2019diff&#34; role=&#34;doc-biblioref&#34;&gt;Lebl 2019&lt;/a&gt;)&lt;/span&gt; gives a helpful tutorial. However, as we won’t know the parameters in advance, we need to use numerical methods (an ODE solver) reassured that we can plug in any set of parameters to construct and visualise &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulated-example&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Simulated Example&lt;/h1&gt;
&lt;p&gt;We generate some simulated data with the following parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; = -0.1 (the ‘damping’ or ‘amplification’)&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; = -0.5 (the ‘frequency’ of oscillations)&lt;/li&gt;
&lt;li&gt;initial displacement (‘baseline’) value &lt;span class=&#34;math inline&#34;&gt;\(x(0)\)&lt;/span&gt; = 5 and initial ‘velocity’ &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}(0)\)&lt;/span&gt; = -2.5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To emphasise how the system looks when amplifying (rather than damping) the oscilations, we invert the sign: &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; = 0.1 resulting in:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-fitting&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Model Fitting&lt;/h1&gt;
&lt;p&gt;The modelling approach from &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-boker2002method&#34; role=&#34;doc-biblioref&#34;&gt;Boker and Nesselroade 2002&lt;/a&gt;)&lt;/span&gt; – used in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-odgers2009capturing&#34; role=&#34;doc-biblioref&#34;&gt;Odgers et al. 2009&lt;/a&gt;)&lt;/span&gt; – is to treat &lt;span class=&#34;math inline&#34;&gt;\(\ddot{x}\)&lt;/span&gt; as the dependent variable in a linear regression model with independent variables &lt;span class=&#34;math inline&#34;&gt;\(\ddot{x}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;. We note that &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-boker2002method&#34; role=&#34;doc-biblioref&#34;&gt;Boker and Nesselroade 2002&lt;/a&gt;)&lt;/span&gt; intends for their method to fit a population-level model – that is, extracting a common &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; for a group of people’s trajectories such that “When a stable interrelationship between a variable and its own derivatives occurs, the variable is said to exhibit intrinsic dynamics” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-boker2002method&#34; role=&#34;doc-biblioref&#34;&gt;Boker and Nesselroade 2002&lt;/a&gt;)&lt;/span&gt;. We’ll only consider fitting to a single individual here.&lt;/p&gt;
&lt;p&gt;The steps are as follows.&lt;/p&gt;
&lt;div id=&#34;compute-gradients&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Compute Gradients&lt;/h2&gt;
&lt;p&gt;Using numerical approximation, and the simulated damped oscillator data, the columns are &lt;code&gt;x&lt;/code&gt; = &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt;, &lt;code&gt;dx1&lt;/code&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}\)&lt;/span&gt; and &lt;code&gt;dx2&lt;/code&gt; = &lt;span class=&#34;math inline&#34;&gt;\(\ddot{x}\)&lt;/span&gt;:&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Time
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
x
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dx1
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
dx2
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.00000
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.34588
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.21820
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.65412
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.56407
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.11456
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.12815
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-3.11675
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.13715
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.57938
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.28977
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.03432
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-4.70768
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95190
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.91782
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-2.67558
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.54586
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.93726
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.38405
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.82642
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.37767
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2.97725
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.79053
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-1.39512
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;estimate-parameters&#34; class=&#34;section level2&#34; number=&#34;4.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.2&lt;/span&gt; Estimate Parameters&lt;/h2&gt;
&lt;p&gt;We can quickly and easily estimate using the ‘regression’ approach, which will be an ordinary least squares solution. The resulting point estimates &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta}\)&lt;/span&gt;, are displayed below, alongside the actual parameters (i.e. for the simulated damped oscillator above):&lt;/p&gt;
&lt;table class=&#34;table table-striped&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Estimated
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Actual
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Zeta
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.151
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Eta
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.381
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;reconstruct-time-series&#34; class=&#34;section level2&#34; number=&#34;4.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.3&lt;/span&gt; Reconstruct Time Series&lt;/h2&gt;
&lt;p&gt;The final step is to visualise the resulting model by numerically integrating the ODEs again, but this time, using the estimated &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta}\)&lt;/span&gt; to ‘reconstruct’ the time series &lt;span class=&#34;math inline&#34;&gt;\(\hat{x}(t)\)&lt;/span&gt; and compare with the original:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The time series resulting from the estimated model parameters (shown in red) is predictably different – and there are at least two systematic reasons for this:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The numerical approximation of the true derivatives is systematically over or under-estimating the ‘true’ derivatives&lt;/li&gt;
&lt;li&gt;These errors are propogated further by obtaining (essentially) an ordinary-least-squares point estimate of the parameters from the data&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The estimates &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta}\)&lt;/span&gt; are derived from these numerical derivatives, so unsuprisingly they are close (but significantly) different from the ‘theoretical’ or known &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. We can quantify the mean square error between the reconstructed and original time series:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{MSE} \left( \hat{x}(t), x(t) \right) = \frac{1}{N} \sum_{t}  \big[ \hat{x}(t)-x(t) \big]^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the number of time points in the original &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt;. The MSE is then 0.9552. This is useful as a baseline for what follows.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-parameters-by-non-linear-least-squares-optimisation&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Estimating Parameters by Non-Linear Least Squares Optimisation&lt;/h1&gt;
&lt;p&gt;Using an ordinary least squares solution for &lt;span class=&#34;math inline&#34;&gt;\(\ddot{x}(t) \sim \dot{x}(t) + x(t)\)&lt;/span&gt; – we obtained a relatively poor estimate for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta}\)&lt;/span&gt;, and this was reflected in the MSE for the reconstructed (versus original) time series. A more traditional method would be to use a non-linear &lt;a href=&#34;https://en.wikipedia.org/wiki/Optimization_problem&#34;&gt;optimisation algorithm&lt;/a&gt; to search the parameter space of &lt;span class=&#34;math inline&#34;&gt;\((\zeta, \eta)\)&lt;/span&gt;, so we try using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm&#34;&gt;Levenberg-Marquardt&lt;/a&gt; (LM) method. This method finds an estimate for &lt;span class=&#34;math inline&#34;&gt;\((\hat{\zeta}, \hat{\eta})\)&lt;/span&gt; by minimising an &lt;a href=&#34;https://en.wikipedia.org/wiki/Loss_function&#34;&gt;objective function&lt;/a&gt;, which in our case, are the values of the parameters that minimise the sum of squares of the deviations (essentially, the MSE). Like many optimisation methods, we run the risk of locating local (rather than global) minimum – that is, an estimate of &lt;span class=&#34;math inline&#34;&gt;\((\hat{\zeta}, \hat{\eta})\)&lt;/span&gt; which minimises the MSE, but where if we were to ‘explore’ the surface of the MSE over a broader range of parameters values, a better (more global) minimum might be found.&lt;/p&gt;
&lt;p&gt;The LM algorithm is iterative, proceding by gradually refining the estimates &lt;span class=&#34;math inline&#34;&gt;\((\hat{\zeta}, \hat{\eta})\)&lt;/span&gt; from an initial, user specified ‘first estimate.’ If this first “guess” is close to the global minimum the algorithm is more likely to converge to the global solution. It therefore makes sense to use domain knowledge to establish a plausible starting point for the LM search. In our case, we will start the search by initialising the parameter estimate to be negative real numbers which corresponds to our expectation that we will be observing a damped (rather than amplifying) oscillator.&lt;/p&gt;
&lt;p&gt;As we only have two parameters to estimate, we can manually evaluate the MSE by systematically varying &lt;span class=&#34;math inline&#34;&gt;\((\hat{\zeta}, \hat{\eta})\)&lt;/span&gt; over a coarse grid of values to get an idea of what the error surface looks like, and further, we can then extract the best estimate (as we will have evaluated the error at each combination of &lt;span class=&#34;math inline&#34;&gt;\((\hat{\zeta}, \hat{\eta})\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;110%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the left, we see that the error surface on a coarse grid over [-1,0] in steps of 0.05 for &lt;span class=&#34;math inline&#34;&gt;\((\zeta,\eta)\)&lt;/span&gt; shows that the error is very large around &lt;span class=&#34;math inline&#34;&gt;\((0,0)\)&lt;/span&gt; but otherwise appears ‘flat.’ The white lines and dot show the parameter values for the minimum MSE – but at this coarse resolution, we can not see the shape of the error surface near the optimum solution. The panel on the right shows the error surface ‘zoomed’ for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta} \in [-0.15,-0.05]\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta} \in [-0.55,-0.45]\)&lt;/span&gt;, (note the difference in the MSE scale) and we can see that best esimates are &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta} = -0.1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta} = -0.5\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This brute-force method gives us the correct answer, and allows a visualisation of the error surface we expect the LM algorithm to search iteratively. We now compare with the LM solution setting our “initial guess” to &lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta} = -1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta} = -1\)&lt;/span&gt; which corresponds to the bottom-right of the parameter space above.&lt;/p&gt;
&lt;p&gt;And find:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\zeta}\)&lt;/span&gt; = -0.1&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\hat{\eta}\)&lt;/span&gt; = -0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we reconstruct the original time-series to compare:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A much better fit, with an MSE approaching 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;some-real-data&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Some Real Data&lt;/h1&gt;
&lt;p&gt;So far, we’ve been using ‘ideal’ simulated data where there is no measurement error and the underlying (hypothetical) damped oscillating process is in a one-one correspondence with the time series &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt;. Here, we use some data on daily self-reported symptoms from Nick Meyer’s &lt;a href=&#34;https://sleepsight.org/&#34;&gt;SleepSight&lt;/a&gt; study. Nick’s data is a natural fit for the state-space models espoused at the start, but to apply a dynamical system model, we need to start small (with one variable). We pick one symptom (self-reported sleep duration) for an examplar participant, and then scale and center the data, before detrending (i.e. removing any linear ‘drift’ in the time series). We then add a &lt;a href=&#34;https://en.wikipedia.org/wiki/Local_regression&#34;&gt;lowess&lt;/a&gt; smoother (shown in red) to try and expose any underlying pattern:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note the somewhat periodic behaviour but there is no clear frequency or progression of damping of oscillations, so it is unlikely that a ‘pure’ damped oscillator will fit. Nonetheless, we use the LM method to try and fit a damped oscillator:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Resulting in a poorly fitting model. One way to understand this is that to look at the &lt;strong&gt;phase plane&lt;/strong&gt; for the system. First, take our first simulated oscillator:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
On the left, we have &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; on the vertical axis as time progresses. On the right, we plot &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}(t)\)&lt;/span&gt; on the vertical and &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; on the horizontal axes (using the ‘mass on a spring’ analogy, we are looking at the relationship between the velocity and displacement). The purple line is our original oscillator, and the orange line the &lt;em&gt;same&lt;/em&gt; system (&lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;), but with &lt;em&gt;different&lt;/em&gt; initial values (i.e. the initial state is &lt;span class=&#34;math inline&#34;&gt;\(x(t) = 2\)&lt;/span&gt; with initial ‘velocity’ &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}(t) = 1.5\)&lt;/span&gt;). The right panel shows the phase plane – the evolution of the &lt;span class=&#34;math inline&#34;&gt;\(x(t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}(t)\)&lt;/span&gt; over time. Notice how (despite different initial values) the two damped oscillators converge to a stable &lt;a href=&#34;https://en.wikipedia.org/wiki/Attractor&#34;&gt;attractor&lt;/a&gt; in the middle (which corresponds to the equilibrium state of the system around as &lt;span class=&#34;math inline&#34;&gt;\(t \rightarrow 100\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;If we look at the behaviour of the real (sleep duration) data using the numerical derivatives:
&lt;img src=&#34;/post/2021-05-23-clinical-trajectories-pt-one/index_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The colour gradient shows time so we can compare the left and right panels: we can see that while the system tends towards a region around &lt;span class=&#34;math inline&#34;&gt;\(x(t) = 0.2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\dot{x}(t) = 0\)&lt;/span&gt;, it is not stable and the trajectory diverges rather than converging (in contrast to the simulated damped oscillator). The difference in behaviours shown by the phase planes for the real data and the idealised, simulated data (from an actual damped oscillator) tell us why the model fit was so poor.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;directions&#34; class=&#34;section level1&#34; number=&#34;7&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;7&lt;/span&gt; Directions&lt;/h1&gt;
&lt;p&gt;The dynamical systems framework is appealing because it provides a model for a latent process which might underly measured / observed data. It requires a model (e.g. a damped oscillator) and a method to fit the data. If the model fits the data, we then import a whole theory that enables us to understand and test the qualitative properties of the model – for example, in terms of stability, attractors and critical transitions &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-scheffer2009early&#34; role=&#34;doc-biblioref&#34;&gt;Scheffer et al. 2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The examples used in this post are all linear systems but there is evidently more complexity to the real data we examined – bluntly, a single damped oscillator cannot model the dynamics of self-reported sleep in our application (and it might be naive to assume that it would). As we alluded to at the start, we would prefer to treat individual variables as components of a larger system – and we have not explored this here (in part, because systems of &lt;em&gt;coupled&lt;/em&gt; oscillators require a more sophisticated analysis), instead focusing on principles and how they apply to data.&lt;/p&gt;
&lt;p&gt;In future posts, we’ll try different approaches that inherit the ideas of state spaces, but attempt to model them without such strong assumptions about the dynamics.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-boker2002method&#34; class=&#34;csl-entry&#34;&gt;
Boker, Steven M, and John R Nesselroade. 2002. &lt;span&gt;“A Method for Modeling the Intrinsic Dynamics of Intraindividual Variability: Recovering the Parameters of Simulated Oscillators in Multi-Wave Panel Data.”&lt;/span&gt; &lt;em&gt;Multivariate Behavioral Research&lt;/em&gt; 37 (1): 127–60.
&lt;/div&gt;
&lt;div id=&#34;ref-hayek2003&#34; class=&#34;csl-entry&#34;&gt;
Hayek, Sabih I. 2003. &lt;span&gt;“Mechanical Vibration and Damping.”&lt;/span&gt; In &lt;em&gt;Digital Encyclopedia of Applied Physics&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1002/3527600434.eap231&#34;&gt;https://doi.org/10.1002/3527600434.eap231&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-Lebl2019diff&#34; class=&#34;csl-entry&#34;&gt;
Lebl, Jiří. 2019. &lt;em&gt;Notes on Diffy Qs&lt;/em&gt;. &lt;a href=&#34;https://www.jirka.org/diffyqs/&#34;&gt;https://www.jirka.org/diffyqs/&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-mason2017mood&#34; class=&#34;csl-entry&#34;&gt;
Mason, Liam, Eran Eldar, and Robb B Rutledge. 2017. &lt;span&gt;“Mood Instability and Reward Dysregulation—a Neurocomputational Model of Bipolar Disorder.”&lt;/span&gt; &lt;em&gt;JAMA Psychiatry&lt;/em&gt; 74 (12): 1275–76.
&lt;/div&gt;
&lt;div id=&#34;ref-nelson2017moving&#34; class=&#34;csl-entry&#34;&gt;
Nelson, Barnaby, Patrick D McGorry, Marieke Wichers, Johanna TW Wigman, and Jessica A Hartmann. 2017. &lt;span&gt;“Moving from Static to Dynamic Models of the Onset of Mental Disorder: A Review.”&lt;/span&gt; &lt;em&gt;JAMA Psychiatry&lt;/em&gt; 74 (5): 528–34.
&lt;/div&gt;
&lt;div id=&#34;ref-odgers2009capturing&#34; class=&#34;csl-entry&#34;&gt;
Odgers, Candice L, Edward P Mulvey, Jennifer L Skeem, William Gardner, Charles W Lidz, and Carol Schubert. 2009. &lt;span&gt;“Capturing the Ebb and Flow of Psychiatric Symptoms with Dynamical Systems Models.”&lt;/span&gt; &lt;em&gt;American Journal of Psychiatry&lt;/em&gt; 166 (5): 575–82.
&lt;/div&gt;
&lt;div id=&#34;ref-scheffer2009early&#34; class=&#34;csl-entry&#34;&gt;
Scheffer, Marten, Jordi Bascompte, William A Brock, Victor Brovkin, Stephen R Carpenter, Vasilis Dakos, Hermann Held, Egbert H Van Nes, Max Rietkerk, and George Sugihara. 2009. &lt;span&gt;“Early-Warning Signals for Critical Transitions.”&lt;/span&gt; &lt;em&gt;Nature&lt;/em&gt; 461 (7260): 53.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Alternatives to Percentage Change</title>
      <link>/post/2021-05-23-symm-percent/add_symm_change_score/</link>
      <pubDate>Thu, 18 Apr 2019 19:47:56 +0100</pubDate>
      <guid>/post/2021-05-23-symm-percent/add_symm_change_score/</guid>
      <description>&lt;p&gt;The motivation for looking at this topic was primarily my naivety and
getting counter-intuitive results when I was looking at serial measures
of response to treatment. In particular, I wanted to use outcome
measures based on published conventions in the literature for threshold
response (the problems with this approach are a topic for another day).
I found some pointers on Frank Harrell&amp;rsquo;s blog on the topic of
&lt;a href=&#34;http://www.fharrell.com/post/percent/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percentages&lt;/a&gt;, which mirrored
some of the discussion in Chapter 8 of (Senn 2008) which looks at
assumptions of additvity in treatment effects.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s the context in clinical trials in psychiatry: To define a
clinically-meaningful outcome (e.g. to a treatment/intervention) the
literature frequently uses &lt;em&gt;percentage change from baseline&lt;/em&gt; &amp;ndash; for
example (Leucht et al. 2009; Munk-Jørgensen 2011) in the context of
PANSS scores in psychosis. I&amp;rsquo;ll avoid the debate about why this may not
be appropriate for statistical modelling (e.g. using change from
baseline severity rather than using the baseline as a covariate in a
statistical model).&lt;/p&gt;
&lt;p&gt;To make this concrete, let A be the first measurement, and B the second
(i.e. pre-treatment and post-treatment respectively, or baseline and
endpoint). Leaving aside the correction to ensure minimum symptoms
severity is 0, Leucht&amp;rsquo;s formula for &lt;strong&gt;percentage reduction from baseline
PANSS&lt;/strong&gt; is essentially:&lt;/p&gt;
&lt;p&gt;$$
100 \times \frac{A - B}{A}
$$&lt;/p&gt;
&lt;p&gt;To make the example concrete, one of the criteria for defining
treatment resistance in schizophrenia (Howes et al. 2016) is: &amp;ldquo;&lt;strong&gt;less
than 20% symptom reduction during a prospective trial&lt;/strong&gt;&amp;rdquo; (of treatment)
using Leucht &lt;em&gt;et al&lt;/em&gt;&amp;rsquo;s formula (above). Re-wording this (to make it
compatible with Leucht): failure of a treatment trial is that the
&lt;strong&gt;percentage reduction from baseline is less than 20%&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;percentages&#34;&gt;Percentages&lt;/h1&gt;
&lt;p&gt;The obvious reason for favouring percentage change from baseline is that
improvement (or worsening) is relative the patient&amp;rsquo;s initial level of
symptom severity. Assuming higher scores represents higher symptom
burden, a patient moving from a baseline A = 97 to an endpoint B = 77
represents a 20% change for that patient. A patient starting from a more
modest symptom burden, say A = 65, improving to B = 52 similarly
represents a 20% improvement.&lt;/p&gt;
&lt;p&gt;However, percentages are not &lt;strong&gt;symmetric&lt;/strong&gt; &amp;ndash; for example, if the first
measurement is A = 97, and the second (post-treatment) improved score is
B = 67 we have a &lt;strong&gt;percentage reduction&lt;/strong&gt; of
$$
100 \times \frac{97-67}{97} \approx 31 \%
$$&lt;/p&gt;
&lt;p&gt;Switching this around, for another patient who gets &lt;strong&gt;worse&lt;/strong&gt; by exactly
the same absolute amount after treatment: A = 67 and B = 97, yields
$$
100 \times \frac{67-97}{67} \approx -45 \%
$$&lt;/p&gt;
&lt;p&gt;Notice that both the sign and magnitude of the change are different,
but the absolute change in units is 30 in both cases.&lt;/p&gt;
&lt;p&gt;The next problem &amp;ndash; percentages are not &lt;strong&gt;additive&lt;/strong&gt;; assume a 30%
&lt;strong&gt;improvement&lt;/strong&gt; from an initial score of A = 100 &amp;ndash; the endpoint
(post-treatment) would be B = 70:&lt;/p&gt;
&lt;p&gt;$$
100 \times \frac{100 - 70}{100} = 30\%
$$&lt;/p&gt;
&lt;p&gt;Now assume we follow the same patient from B to another time point, C,
and they&amp;rsquo;ve gotten worse, returning to their baseline of 100:
$$
100 \times \frac{70 - 100}{70} \approx -42.9\%
$$&lt;/p&gt;
&lt;p&gt;On absolute scale units we have a series of measurements A = 100, B =
70 and C = 100, but we have seen a percentage change of 30% (A to B)
followed by a percentage change of -42.9% (B to C) despite A = C. In
other words, if we take the baseline A, add the change from A to B and
the change from B to C, we should have &lt;strong&gt;zero net change&lt;/strong&gt; from the
baseline, A to the last measurement C:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
A + (A-B) + (B-C) &amp;amp;= 100 + (100 - 70) + (70 - 100) \\\&lt;br&gt;
&amp;amp;= 100 + 30 - 30 \\\&lt;br&gt;
&amp;amp;= 100
\end{aligned}
$$
But if we use percentages, this is not the case; assuming that A
represents 100% &amp;ndash; the baseline level of symptom severity &amp;ndash; and abusing
notation, we let (A-B) stand for the percentage change from A to B, and
similarly for (B-C):
$$
\begin{aligned}
A + (A-B) + (B-C) &amp;amp;= 100 + 30 - 42.9 \\\&lt;br&gt;
&amp;amp;= 87.1\%
\end{aligned}
$$
Percentage change from baseline is intuitive because it allows for a
familiar and uniform representation of change &lt;em&gt;relative&lt;/em&gt; to the
patient&amp;rsquo;s baseline, but it&amp;rsquo;s counter-intuitive because it is asymmetric
and non-additive.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a graphical representation: let the baseline (A) and endpoint (B)
values range from 0 to 100 respectively &amp;ndash; the interactive graph below
shows the behaviour of percentage change as both A and B vary:&lt;/p&gt;
&lt;iframe width=&#34;900&#34; height=&#34;800&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;//plot.ly/~danwjoyce/11.embed&#34;&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;sympercent&#34;&gt;Sympercent&lt;/h1&gt;
&lt;p&gt;The idea proposed in (Cole 2000), neatly illustrated in (T. J. Cole and
Altman 2017b; T. J. Cole and Altman 2017a) is to exploit a property of
natural logarithms where because
$$
\ln(A)−\ln(B)=\ln(A/B)
$$
the ratio A divided by B can be expressed as a sum, retaining
additivity and symmetry, and it turns out, approximating a &amp;lsquo;percentage&amp;rsquo;
change. The proposed name for this measure is &amp;ldquo;sympercent&amp;rdquo; and (Cole
2000) gave it the symbol &lt;strong&gt;s%&lt;/strong&gt; and the formula
$$
100 × [\ln(A)−\ln(B)]
$$
Repeating the examples above; first of all A = 97, B = 67 which gave
us a 31% (percent) improvement instead gives us the sympercent change:&lt;/p&gt;
&lt;p&gt;$$
100 × [\ln(97)−\ln(67)] ≈ 37 s\%
$$&lt;/p&gt;
&lt;p&gt;A different numerical value (37, versus 31). Note, however, that we
gain symmetry &amp;ndash; so with A = 67 and B = 97 (worsening symptoms from A to
B):&lt;/p&gt;
&lt;p&gt;$$
100 × [\ln(67)−\ln(97)] ≈ −37 s\%
$$&lt;/p&gt;
&lt;p&gt;The magnitude is the same (37) but the sign changes to represent
worsening, rather than improvement.&lt;/p&gt;
&lt;p&gt;Repeating the additivity example; improvement from A = 100 to B = 70 but
with subsequent worsening back to C = 100. We&amp;rsquo;ll drop the factor of 100
to simplify presentation:
$$
\begin{aligned}
\ln(A) + \big[ \ln(A)-\ln(B) \big] + \big[\ln(B)-\ln(C) \big] &amp;amp;= 4.61 + 0.36 - 0.36  \\\&lt;br&gt;
&amp;amp;= 4.61
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;That is, a net change of zero from the baseline.&lt;/p&gt;
&lt;p&gt;Switching to an alternative representation affords a more interpretable
and intuitive measure of change that preserves the idea of change
&lt;strong&gt;relative&lt;/strong&gt; to the patient&amp;rsquo;s baseline.&lt;/p&gt;
&lt;p&gt;Below, the behaviour of sympercent is shown over the range [0,100],
analogous to the percent change shown above:&lt;/p&gt;
&lt;iframe width=&#34;900&#34; height=&#34;800&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; src=&#34;//plot.ly/~danwjoyce/13.embed&#34;&gt;&lt;/iframe&gt;
&lt;h1 id=&#34;the-original-problem&#34;&gt;The Original Problem&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s apply the &amp;lsquo;sympercent&amp;rsquo; idea for the problem originally introduced
&amp;ndash; a threshold of 20% improvement for symptoms.&lt;/p&gt;
&lt;p&gt;To simplify presentation, we drop the factor 100 and instead, work in
fractions of 1 (i.e. rather 20% we say 0.2 or 1/5). If we require at
least a 20% reduction in symptom scores to qualify as a response, then
we are saying:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{A-B}{A} &amp;amp;= 0.2 \\\&lt;br&gt;
A-B &amp;amp;= 0.2 \times A
\end{aligned}
$$
i.e. the difference between symptom scores at time points A and B
should be a fraction (0.2, or one-fifth) of the baseline A:&lt;/p&gt;
&lt;p&gt;Reusing our first example, where A = 97:
$$
\begin{aligned}
97-B = 0.2 \times 97
\end{aligned}
$$
So, for symptoms to have improved by at least 20% from a baseline of A
= 97, the symptom score at B should be 77.6 or lower.&lt;/p&gt;
&lt;p&gt;What would this reduction look like in s% (sympercents)?&lt;/p&gt;
&lt;p&gt;If A = 97, and we know a 20% reduction (or 0.2) would represent B = 77.6 then:&lt;/p&gt;
&lt;p&gt;$$
100 × [\ln(97)−\ln(77.6)] ≈ 22.3 s\%
$$&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;Cole, Tim J, and Douglas G Altman. 2017a. “Statistics Notes: Percentage
Differences, Symmetry, and Natural Logarithms.” &lt;em&gt;BMJ&lt;/em&gt; 358. British
Medical Journal Publishing Group: j3683.&lt;/p&gt;
&lt;p&gt;———. 2017b. “Statistics Notes: What Is a Percentage Difference?” &lt;em&gt;BMJ&lt;/em&gt;
358. British Medical Journal Publishing Group: j3663.&lt;/p&gt;
&lt;p&gt;Cole, TJ. 2000. “Sympercents: Symmetric Percentage Differences on the
100 log&lt;sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;/sub&gt; Scale Simplify the Presentation of Log Transformed
Data.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 19 (22). Wiley Online Library: 3109–25.&lt;/p&gt;
&lt;p&gt;Howes, Oliver D, Rob McCutcheon, Ofer Agid, Andrea De Bartolomeis, Nico
JM Van Beveren, Michael L Birnbaum, Michael AP Bloomfield, et al. 2016.
“Treatment-Resistant Schizophrenia: Treatment Response and Resistance in
Psychosis (Trrip) Working Group Consensus Guidelines on Diagnosis and
Terminology.” &lt;em&gt;American Journal of Psychiatry&lt;/em&gt; 174 (3). Am Psychiatric
Assoc: 216–29.&lt;/p&gt;
&lt;p&gt;Leucht, S, JM Davis, RR Engel, W Kissling, and JM Kane. 2009.
“Definitions of Response and Remission in Schizophrenia: Recommendations
for Their Use and Their Presentation.” &lt;em&gt;Acta Psychiatrica Scandinavica&lt;/em&gt;
119. Wiley Online Library: 7–14.&lt;/p&gt;
&lt;p&gt;Munk-Jørgensen, Povl. 2011. “Corrigendum.” &lt;em&gt;Acta Psychiatrica
Scandinavica&lt;/em&gt; 124 (1): 82–82.
doi:&lt;a href=&#34;https://doi.org/10.1111/j.1600-0447.2011.01720.x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;10.1111/j.1600-0447.2011.01720.x&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Senn, Stephen S. 2008. &lt;em&gt;Statistical Issues in Drug Development&lt;/em&gt;. Vol.
69. John Wiley &amp;amp; Sons.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
