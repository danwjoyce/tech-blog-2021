<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tensors | Dan W Joyce</title>
    <link>/category/tensors/</link>
      <atom:link href="/category/tensors/index.xml" rel="self" type="application/rss+xml" />
    <description>tensors</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 17 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hub05f02e644906e3d80c1c494250cac2e_12120_512x512_fill_lanczos_center_2.png</url>
      <title>tensors</title>
      <link>/category/tensors/</link>
    </image>
    
    <item>
      <title>Tensor Decompositions and Noisy Graphs</title>
      <link>/post/tensor-connectivity-matrices/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/tensor-connectivity-matrices/</guid>
      <description>
&lt;script src=&#34;/post/tensor-connectivity-matrices/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Here’s the problem set-up: we have a group of diseases/disorders/conditions (A,B,C and D) and associated known treatments (RxA, RxB, RxC and RxD). We want to build &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)#Definitions&#34;&gt;graphs&lt;/a&gt; that represent associations between the conditions and treatments for a group of patients. We are given time-ordered data (from their medical records) that records a sequence of recorded episodes (but not the precise dates of each) of both conditions and treatments. Ultimately, we want to discover patterns of common treatments and conditions to derive some understanding of multimorbidity.&lt;/p&gt;
&lt;p&gt;An example:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tod-1.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The circles represent mentions of disorders and the squares treatment episodes. Here, we can see someone with a life-long condition (A) for which they had continuous treatment (or at least, at the resolution of recording, whenever A was mentioned, RxA was also). At some point, they develop an episode of condition B and on the second occurrence, they’re prescribed treatment RxB. And so on. We can see that we could represent each time-ordered series as a binary indicator vector, for instance condition B = (0,0,1,1,0,0,1,1,1) and treatment RxD = (0,0,0,0,0,1,1,0,0).&lt;/p&gt;
&lt;p&gt;We can construct a graph where nodes/vertices are the conditions and treatments, and the edges between represent relationships. In this example, A and RxA &lt;em&gt;always&lt;/em&gt; co-occur together so if an edge represents co-occurence, this would be a very strong association. Another method of constructing the graph is to systematically time-lag each condition/treatment vector and see if there’s a consistent pattern of e.g. condition B always preceding RxB. To do this, we use pairwise &lt;a href=&#34;https://en.wikipedia.org/wiki/Transfer_entropy&#34;&gt;transfer entropy&lt;/a&gt; (which generalises the notion of Granger temporality) for each pair of conditions and treatments.&lt;/p&gt;
&lt;p&gt;If we do this for the above time-ordered data, we get the following directed graph (note, squares are treatments and circles conditions):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-tod-1.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This graph can be parsimoniously represented as an adjacency matrix: rows and columns represent conditions/treatments and the &lt;span class=&#34;math inline&#34;&gt;\((i,j)\)&lt;/span&gt;th element is the strength of association (i.e. that condition/treatment &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; precedes condition/treatment &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;). For the graph above, the adjacency matrix looks like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;conn-matrix-1.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Importantly, it’s an asymmetric directed graph, so the direction (as well as the strength) of associations matter. Ultimately, for this blogpost, it doesn’t really matter how the graph is constructed, just that we have a bunch of graphs represented as adjacency matrices.&lt;/p&gt;
&lt;p&gt;For this simulation, we built 4 example time-ordered sequences of conditions/treatments and then derived the adjacency matrices as before, resulting in the following 4 exemplar graphs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;four-exemplar-graphs.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Then, we derive 25 noisy variations of each examplar, giving us 100 samples as follows:
&lt;img src=&#34;all-connect-matrix.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now assume that we are given the 100 noisy samples and we want to discover underlying graphs (which of course, in this simulated example, we know is 4 and we know what they look like). We can see glimpses of the original 4 exemplars in the table of samples, but if we take the “average” graph (adjacency matrix) we’d get this:
&lt;img src=&#34;avrg-connect-matrix.png&#34; width=&#34;60%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_partition&#34;&gt;Graph partitioning&lt;/a&gt; methods try and tease out collections (or communities) of nodes/edges that form discrete graphs by selecting and cutting edges. One approach to this is spectral partitioning where first, we derive the Laplacian matrix (from the adjacency matrix) and perform eigen- or singular value decomposition (SVD) on it – to me, this seems like magic: you can carve up a graph (represented as an adjacency matrix) using linear algebra. I recently learned about non-negative matrix factorisation from a paper by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;Hassaine et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; on multimorbidity in clinical data which prompted me to read the original paper &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;Lee and Seung 1999&lt;/a&gt;)&lt;/span&gt; and a bit like SVD, seemed like more magic to me. Further, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;Hassaine et al.&lt;/a&gt; (&lt;a href=&#34;#ref-hassaine2020untangling&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; described using tensors – collections of matrices – for a similar task.&lt;/p&gt;
&lt;p&gt;So here we have our problem: given a bunch of 100 noisy adjacency matrices above, extract the underlying ‘exemplar’ or prototype graphs. The case presented here is not dissimilar to ideas in image processing – each matrix is a noisy image and we want to find some underlying latent structure. If each of the 100 noisy adjacency matrices is a sample, it seems logical to ‘stack’ them up and see what can be learned from this ‘stack.’ A series of matrices can be stacked up into a multidimensional array structure (a tensor) and there’s a literature on how to perform decomposition (akin to SVD) on tensors. So you can see how the logic proceeds: if matrix decompositions (like SVD) can can be used to partition a single graph (connectivity matrix), then maybe tensors can help tease out common structure in a stack of adjacency matrices.&lt;/p&gt;
&lt;div id=&#34;tensors&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Tensors&lt;/h1&gt;
&lt;p&gt;There are plenty of great online resources on &lt;a href=&#34;https://en.wikipedia.org/wiki/Tensor#Definition&#34;&gt;tensors&lt;/a&gt; and multilinear algerbra. One source that I found spectacularly helpful was &lt;a href=&#34;https://www.alexejgossmann.com/&#34;&gt;Alexej Gossman’s&lt;/a&gt; tutorial on Tucker and Rank-1 decompositions, not least because it serves as an introduction to programming with tensors in R. A frequently-cited and comprehensive tutorial paper is &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt; which contains the formal details alluded to here and from which we borrow notation. Much of what follows are notes to help me remember the core concepts.&lt;/p&gt;
&lt;p&gt;A tensor is essentially a multi-dimensional array – much the same as the notion of a multi-dimensional array of numbers in programmning languages – and generalises familiar objects like vectors and matrices.&lt;/p&gt;
&lt;p&gt;A tensor has an &lt;strong&gt;order&lt;/strong&gt; which equates the number of dimensions of the array.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;order one&lt;/strong&gt; tensor is a vector &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v} = \left[ v_1, v_2, \ldots, v_{I_1} \right]\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt; elements (i.e. is of length &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt;). A vector of real numbers is denoted &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{v} \in \mathbb{R}^{I_1}\)&lt;/span&gt; (and in what follows, all our tensors will contain real elements).&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;order two&lt;/strong&gt; tensor is a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{I_1 \times I_2}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(I_1\)&lt;/span&gt; rows and &lt;span class=&#34;math inline&#34;&gt;\(I_2\)&lt;/span&gt; columns.&lt;/p&gt;
&lt;p&gt;Before getting carried away, a simple motivating example. Take the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{2 \times 3}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M &amp;lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE )
M&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we want the first &lt;em&gt;row&lt;/em&gt; of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt; we’ll write this as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{1:}\)&lt;/span&gt; which we read as “set the row index &lt;span class=&#34;math inline&#34;&gt;\(i_1 = 1\)&lt;/span&gt; and retrieve &lt;em&gt;all&lt;/em&gt; columns” yielding (1, 2, 3).
Similarly, if we want the third &lt;em&gt;column&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt; we’ll write this as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{:3}\)&lt;/span&gt;, to be read as “set the column index &lt;span class=&#34;math inline&#34;&gt;\(i_2 = 3\)&lt;/span&gt; and retrieve all rows” yielding (3, 6).&lt;/p&gt;
&lt;div id=&#34;modes-and-fibres&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Modes and Fibres&lt;/h2&gt;
&lt;p&gt;Translating to the language of tensors; instead of discussing rows or columns, we generalise to &lt;strong&gt;fibres&lt;/strong&gt; and &lt;strong&gt;modes&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a matrix is a 2nd order tensor with indices &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;index &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; refers to the &lt;strong&gt;first mode&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;index &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt; refers to the &lt;strong&gt;second mode&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;mode-1 fibres&lt;/strong&gt; of the matrix above are the columns denoted by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{:j}\)&lt;/span&gt; which are (1, 4), (2, 5) and (3, 6),&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;mode-2 fibres&lt;/strong&gt; are the rows of the matrix above, denoted by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}_{i:}\)&lt;/span&gt; which are (1, 2, 3) and (4, 5, 6)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fibres of a tensor are obtained by ‘fixing’ all but one of the mode indices.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;third-order-tensors-and-slices&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Third Order Tensors and Slices&lt;/h2&gt;
&lt;p&gt;Now we come to &lt;strong&gt;third order&lt;/strong&gt; tensors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}} \in \mathbb{R}^{I_1 \times I_2 \times I_3}\)&lt;/span&gt; representing a “cuboid” of elements with three modes and indices ranging from &lt;span class=&#34;math inline&#34;&gt;\(i_1 = 1 \ldots I_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(i_2 = 1 \ldots I_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i_3 = 1 \ldots I_3\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s illustrate this by first of all defining two matrices of size &lt;span class=&#34;math inline&#34;&gt;\(2 \times 3\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X1 &amp;lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE )
X2 &amp;lt;- matrix( c(7,8,9,10,11,12), nrow = 2, ncol = 3, byrow = TRUE )
print(X1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(X2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now, glue them together in the third mode with &lt;code&gt;X1&lt;/code&gt; at the front and &lt;code&gt;X2&lt;/code&gt; behind:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- as.tensor( abind( X1, X2, along = 3 ) )
print(X@data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By “stacking” &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; we’ve built the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}} \in \mathbb{R}^{2 \times 3 \times 2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This illustrates &lt;strong&gt;slices&lt;/strong&gt; – by fixing all the indices except two, we obtain a matrix that represents a ‘cut’ through the tensor in two of the modes. The &lt;strong&gt;frontal slices&lt;/strong&gt; are &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::k}\)&lt;/span&gt; and are simply the two matrices &lt;code&gt;X1&lt;/code&gt; and &lt;code&gt;X2&lt;/code&gt; in the code above. Note that we use the R package &lt;a href=&#34;https://cran.r-project.org/package=rTensor&#34;&gt;&lt;code&gt;rTensor&lt;/code&gt;&lt;/a&gt; which provides a class for tensor representations.&lt;/p&gt;
&lt;p&gt;We can slice the tensor on different modes, obtaining &lt;strong&gt;horizontal slices&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{i::}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[1,,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    7
## [2,]    2    8
## [3,]    3    9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[2,,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    4   10
## [2,]    5   11
## [3,]    6   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And &lt;strong&gt;lateral slices&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{:j:}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,1,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    7
## [2,]    4   10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,2,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    2    8
## [2,]    5   11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X[,3,]@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    3    9
## [2,]    6   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re not going to need more than 3rd order tensors for what follows, so we won’t generalise further.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;matricization-and-vectorization-of-tensors&#34; class=&#34;section level2&#34; number=&#34;1.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.3&lt;/span&gt; Matricization and Vectorization of Tensors&lt;/h2&gt;
&lt;p&gt;Order 3 tensors are cuboids and this is intutive. But if the order is higher, it becomes very hard to visualise, manipulate and define operations on tensors.&lt;/p&gt;
&lt;p&gt;One simple transformation is to &lt;strong&gt;vectorize&lt;/strong&gt; the tensor, which simply means ‘flattening’ the tensor into a vector following some convention for how elements are “read out” by systematically varying the indices with respect to each other (e.g. &lt;span class=&#34;math inline&#34;&gt;\(i_1\)&lt;/span&gt; varies slower than &lt;span class=&#34;math inline&#34;&gt;\(i_2\)&lt;/span&gt;). We denote this operation &lt;span class=&#34;math inline&#34;&gt;\(\text{vec}(\mathcal{X})\)&lt;/span&gt; which gives:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rTensor::vec(X)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  1  4  2  5  3  6  7 10  8 11  9 12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that this look like a vector of the column fibres of each frontal slice – i.e. (1, 4)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt;, (2, 5)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt;, (3, 6)&lt;span class=&#34;math inline&#34;&gt;\(^T\)&lt;/span&gt; … concatenated together.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Matricization&lt;/strong&gt; is the transforming of a tensor into a matrix represention by &lt;strong&gt;unfolding&lt;/strong&gt; the tensor along a mode. This helps us visualise as well as understand operations on tensors. The sources to understand this in detail are &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader&lt;/a&gt; (&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-bader2006algorithm&#34; role=&#34;doc-biblioref&#34;&gt;Bader and Kolda&lt;/a&gt; (&lt;a href=&#34;#ref-bader2006algorithm&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;n-mode matricization&lt;/strong&gt; of a tensor yields a matrix with columns being the mode-n fibres of the tensor.&lt;/p&gt;
&lt;p&gt;Take our &lt;span class=&#34;math inline&#34;&gt;\(2 \times 3 \times 2\)&lt;/span&gt; tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we unfold the tensor along the &lt;em&gt;first&lt;/em&gt; mode we get the two frontal slices (matrices) &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{::2}\)&lt;/span&gt; concatenated side-by-side as a matrix &lt;span class=&#34;math inline&#34;&gt;\(\left[ \mathbf{X}_{::1} \mid \mathbf{X}_{::2} \right]\)&lt;/span&gt; and we denote this with a bracketed subscript indicating the mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  2 6 
## Data: 
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    7    8    9
## [2,]    4    5    6   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And if we unfold on the second mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(2)}\)&lt;/span&gt; we get the transposed frontal slices arranged side-by-side &lt;span class=&#34;math inline&#34;&gt;\(\left[ \mathbf{X}^{\intercal}_{::1} \mid \mathbf{X}^{\intercal}_{::2} \right]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  3 4 
## Data: 
##      [,1] [,2] [,3] [,4]
## [1,]    1    4    7   10
## [2,]    2    5    8   11
## [3,]    3    6    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, unfolding in the third mode &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}_{(3)}\)&lt;/span&gt; we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;k_unfold(X, m = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Numeric Tensor of 2 Modes
## Modes:  2 6 
## Data: 
##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    4    2    5    3    6
## [2,]    7   10    8   11    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which is the matrix :
&lt;span class=&#34;math display&#34;&gt;\[
  \begin{bmatrix}
    \text{vec}(\mathcal{X}_{::1})^T \\
    \text{vec}(\mathcal{X}_{::2})^T
  \end{bmatrix}
\]&lt;/span&gt;
That is, the first and second rows are the vectorization of the first and second frontal slices respectively:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rbind( vec(X[,,1]), vec(X[,,2] ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    4    2    5    3    6
## [2,]    7   10    8   11    9   12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;tensor-matrix-products&#34; class=&#34;section level2&#34; number=&#34;1.4&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.4&lt;/span&gt; Tensor-Matrix Products&lt;/h2&gt;
&lt;p&gt;An important operation is the product of a tensor and a matrix along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;th mode, or the &lt;strong&gt;n-mode product&lt;/strong&gt;. Restricting out attention to order 3 tensors (“cubes”) – we multiply the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}^{I_1 \times I_2 \times I_3}\)&lt;/span&gt; with a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{J \times I_n}\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is mode 1, 2 or 3.&lt;/p&gt;
&lt;p&gt;To my mind, the definition of the n-mode product is most easily understood in terms of matricization (unfolding), matrix products and the folding the result back into a tensor. The expression for the element-wise calculation is given in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt; pp. 460.&lt;/p&gt;
&lt;p&gt;For example, take the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q} \in \mathbb{R}^{3 \times 2}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Q &amp;lt;- matrix( c(1,1,2,2,3,3), nrow = 3, ncol = 2, byrow = TRUE )
print(Q)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    1    1
## [2,]    2    2
## [3,]    3    3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To multiply &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; by mode &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; of the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; we perform this sequence of operations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(n)}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; mode unfolding of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Compute the matrix product &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(n)} = \mathbf{Q} \mathbf{X}_{(n)}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Fold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(n)}\)&lt;/span&gt; along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note, this only makes sense if the dimensions of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; are conformable with the unfolding of the tensor i.e. that the number of columns in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; equals &lt;span class=&#34;math inline&#34;&gt;\(I_n\)&lt;/span&gt;, the size of mode &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; in the tensor&lt;/p&gt;
&lt;p&gt;Let’s do this for the first mode of our example. Here’s our tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(X@data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    1    2    3
## [2,]    4    5    6
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]    7    8    9
## [2,]   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Mode 1 of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; is of size &lt;span class=&#34;math inline&#34;&gt;\(I_1 = 2\)&lt;/span&gt; and our matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}\)&lt;/span&gt; has two columns, so we’re good.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Unfold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\mathcal{X}}\)&lt;/span&gt; along the first mode &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# mode to take product with
n &amp;lt;- 1
X_1 &amp;lt;- k_unfold( X, n )@data
print(X_1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    1    2    3    7    8    9
## [2,]    4    5    6   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the matrix product &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(1)} = \mathbf{Q} \mathbf{X}_{(1)}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_1_prime &amp;lt;- Q %*% X_1
print( X_1_prime )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4] [,5] [,6]
## [1,]    5    7    9   17   19   21
## [2,]   10   14   18   34   38   42
## [3,]   15   21   27   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Fold &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X&amp;#39;}_{(1)}\)&lt;/span&gt; along the &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;-th mode&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute the required modes for the resulting folded tensor
Q.dims &amp;lt;- dim(Q)
modes.X_prime &amp;lt;- X@modes
modes.X_prime[n] &amp;lt;- Q.dims[1]
# fold
Q_by_X.1 &amp;lt;- k_fold( X_1_prime, n, modes.X_prime )
print( Q_by_X.1@data )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    5    7    9
## [2,]   10   14   18
## [3,]   15   21   27
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   17   19   21
## [2,]   34   38   42
## [3,]   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Naturally, there’s a function to do this for us in one simple step:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rTensor::ttm( X, Q, m = 1)@data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## , , 1
## 
##      [,1] [,2] [,3]
## [1,]    5    7    9
## [2,]   10   14   18
## [3,]   15   21   27
## 
## , , 2
## 
##      [,1] [,2] [,3]
## [1,]   17   19   21
## [2,]   34   38   42
## [3,]   51   57   63&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix-decompositions&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Matrix Decompositions&lt;/h1&gt;
&lt;p&gt;We can now venture into the decompositions, or factorizations, of matrices and tensors.&lt;/p&gt;
&lt;div id=&#34;singular-value-decomposition&#34; class=&#34;section level2&#34; number=&#34;2.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.1&lt;/span&gt; Singular Value Decomposition&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(314159)
A &amp;lt;- matrix(  c(1,0,0,1,
                1,0,0,1,
                1,1,1,1,
                1,0,0,1,
                1,0,0,1), ncol = 4, nrow = 5, byrow = TRUE)


A &amp;lt;- A + runif( 4*5, -0.1, 0.3)

UDV &amp;lt;- svd(A)
U &amp;lt;- UDV$u
D &amp;lt;- diag(UDV$d)
V &amp;lt;- UDV$v

recon.4 &amp;lt;- U[,1:4] %*% D[1:4,1:4] %*% t(V[,1:4])
recon.2 &amp;lt;- U[,1:2] %*% D[1:2,1:2] %*% t(V[,1:2])

par(mfrow=c(1,3),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(A), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Original&amp;quot; )
    image( t(recon.4), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;k=4&amp;quot; )
    image( t(recon.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;k=2&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;
This diagram shows on the left, an original image (a sort of noisy “H” shape) represented by a matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} \in \mathbb{R}^{5 \times 4}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We subject this to a &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular valued decomposition&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbf{M} = \mathbf{UDV}^{\intercal}
\]&lt;/span&gt;
where the factor matrices are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(D\)&lt;/span&gt; is the diagonal matrix of singular values, with largest value in the top left, descending to the smallest along the diagonal:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(D)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          [,1]     [,2]      [,3]      [,4]
## [1,] 3.504158 0.000000 0.0000000 0.0000000
## [2,] 0.000000 1.323071 0.0000000 0.0000000
## [3,] 0.000000 0.000000 0.2577888 0.0000000
## [4,] 0.000000 0.000000 0.0000000 0.1509413&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is a matrix of &lt;span class=&#34;math inline&#34;&gt;\(5 \times 4\)&lt;/span&gt; left singular vectors of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(U)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]         [,3]        [,4]
## [1,] -0.4365701 -0.2339371  0.813531383 -0.01200304
## [2,] -0.4120556 -0.2359644 -0.512436719  0.37115595
## [3,] -0.5058560  0.8584828 -0.003890147  0.06573130
## [4,] -0.3961206 -0.1703899 -0.263178631 -0.86250248
## [5,] -0.4762509 -0.3515234 -0.079354732  0.33744341&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is a matrix of &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; right singular vectors of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(V)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            [,1]       [,2]        [,3]        [,4]
## [1,] -0.6951074 -0.2107143 -0.68594850 -0.04358657
## [2,] -0.1943603  0.5802217 -0.03149829  0.79030037
## [3,] -0.2231139  0.7589987  0.03175303 -0.61084605
## [4,] -0.6551869 -0.2070343  0.72627423  0.01981508&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now ‘reconstruct’ an approximation to the original image &lt;span class=&#34;math inline&#34;&gt;\(\widetilde{\mathbf{M}} = \mathbf{UDV^{\intercal}}\)&lt;/span&gt; and this is shown in the middle picture with &lt;span class=&#34;math inline&#34;&gt;\(k = 4\)&lt;/span&gt; denoting that we use all 4 of the singular vectors.&lt;/p&gt;
&lt;p&gt;The “magic” alluded to earlier is when we use a &lt;strong&gt;truncated&lt;/strong&gt; version of the SVD – take the first &lt;span class=&#34;math inline&#34;&gt;\(1 \ldots k\)&lt;/span&gt; singular vectors and use these to reconstruct the image:
&lt;span class=&#34;math display&#34;&gt;\[
  \widetilde{\mathbf{M}} = \mathbf{U}_{:k} \mathbf{D}_{kk} \mathbf{V}^{\intercal}_{:k}
\]&lt;/span&gt;
With &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;, we get the third image on the right; a reconstructed image using half the vectors (i.e. compressed) which additionally helps de-noise the original image.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-negative-matrix-factorisation&#34; class=&#34;section level2&#34; number=&#34;2.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;2.2&lt;/span&gt; Non-Negative Matrix Factorisation&lt;/h2&gt;
&lt;p&gt;Another decomposition (factorisation) is the &lt;strong&gt;non-negative&lt;/strong&gt; matrix factorisation or NMF (see &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;Lee and Seung&lt;/a&gt; (&lt;a href=&#34;#ref-lee1999learning&#34; role=&#34;doc-biblioref&#34;&gt;1999&lt;/a&gt;)&lt;/span&gt; for details). Here, we factor:
&lt;span class=&#34;math display&#34;&gt;\[
  \mathbf{M} = \mathbf{WH}^{\intercal}
\]&lt;/span&gt;
subject to the constraint that the factor matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt; are non-negative (cf. the matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{U}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}\)&lt;/span&gt; in SVD as shown above). This results in an algorithm for estimating the factor matrices which is iterative for each element of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt;. The remarkable feature of NMF is that the resulting factor matrices have a component-parts interpretation as follows; the columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}\)&lt;/span&gt; are &lt;strong&gt;encodings&lt;/strong&gt; and columns of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}\)&lt;/span&gt; are &lt;strong&gt;weights&lt;/strong&gt; that map encodings to weighted-sums in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This means, we can use a truncated NMF decomposition to partition “component parts” of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As an example, with the same noisy “H” image, we apply NMF truncating to &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt; and reconstruct:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nmf.1 &amp;lt;- NMF(A, J=2)
W &amp;lt;- nmf.1$U
H &amp;lt;- nmf.1$V

recon.nmf &amp;lt;- W %*% t(H)


par(mfrow=c(1,2),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(A), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Original&amp;quot; )
    image( t(recon.nmf), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;NMF (k = 2)&amp;quot; )    &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result is very similar to SVD. The “magic” here is in the following trick – take each column of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W}_{:i}\)&lt;/span&gt; and multiply it by the corresponding column of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}^{\intercal}_{:i}\)&lt;/span&gt; and we get:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recon.nmf.1 &amp;lt;- W[,1] %*% t(H[,1])
recon.nmf.2 &amp;lt;- W[,2] %*% t(H[,2])
recon.nmf.sum &amp;lt;- recon.nmf.1 + recon.nmf.2

par(mfrow=c(1,3),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)
    image( t(recon.nmf.1), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (1st column)&amp;quot; )    
    image( t(recon.nmf.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (2nd column)&amp;quot; )
    image( t(recon.nmf.sum), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Sum&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;
So we see that the columns of the factor matrices capture component parts of the source matrix and reconstruction is the sum of those component parts. This is a direct consequence of imposing the constraint that the elements of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{M} = \mathbf{WH}^{\intercal}\)&lt;/span&gt; must be non-negative.&lt;/p&gt;
&lt;p&gt;We can repeat the experiment with &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(314150)
nmf.2 &amp;lt;- NMF(A, J=3,  algorithm = &amp;quot;Frobenius&amp;quot;)
W &amp;lt;- nmf.2$U
H &amp;lt;- nmf.2$V

recon.nmf &amp;lt;- W %*% t(H)

recon.nmf.1 &amp;lt;- W[,1] %*% t(H[,1])
recon.nmf.2 &amp;lt;- W[,2] %*% t(H[,2])
recon.nmf.3 &amp;lt;- W[,3] %*% t(H[,3])
recon.nmf.sum &amp;lt;- recon.nmf.1 + recon.nmf.2 + recon.nmf.3

par(mfrow=c(1,4),
      mar = c(0.5,0.5,2,0.5),
      pty = &amp;quot;s&amp;quot;)

    image( t(recon.nmf.1), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (1st column)&amp;quot; )    
    image( t(recon.nmf.2), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (2nd column)&amp;quot; )
    image( t(recon.nmf.3), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;W x H (3rd column)&amp;quot; )
    image( t(recon.nmf.sum), xaxt = &amp;quot;n&amp;quot;, yaxt = &amp;quot;n&amp;quot;, col = viridis(256), main = &amp;quot;Sum&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/tensor-connectivity-matrices/index_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;
Notice now that the three components are the right and left vertical ‘bars’ and the one horizontal bar. The problem is, with &lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt; the solution found by NMF is stable (i.e. it always extracts a component with the two vertical bars, and one hortizontal) but with &lt;span class=&#34;math inline&#34;&gt;\(k=3\)&lt;/span&gt; the solution varied significantly between runs of the iterative NMF algorithm; sometimes local optima are found which make sense, other times we get less meaningful decompositions. These variable results are not shown here because we fix the pseudo-random number generator seed for reproducibility.&lt;/p&gt;
&lt;p&gt;One of the problems with applying NMF is finding the right &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; that reflects the desired component-parts decomposition in the problem domain.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tensor-decomposition&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Tensor Decomposition&lt;/h1&gt;
&lt;p&gt;For a survey of tensor decomposition techniques, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader&lt;/a&gt; (&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;2009&lt;/a&gt;)&lt;/span&gt; is the go-to source. Here, we’ll focus on one decomposition that mirrors NMF for tensors, the &lt;strong&gt;non-negative Tucker decomposition&lt;/strong&gt; (NTD) for mode-3 tensors. Instead of having a target matrix, decomposed by multiplying factor matrices together, we have a more complex problem of factorising a cuboid. Recall our example: 100 noisy samples derived from 4 ‘prototypical’ graph adjacency matrices.&lt;/p&gt;
&lt;p&gt;With tensors, the intuitions from matrix decomposition apply, but instead of dealing with products of matrices, we now have a more complex situation shown below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tucker-schematic.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The Tucker decomposition is then:
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{X} = \mathcal{G} \times_1 \mathbf{A} \times_2 \mathbf{B} \times_3 \mathbf{C} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; is the tensor of dimensions &lt;span class=&#34;math inline&#34;&gt;\(I_1 \times I_2 \times I_3\)&lt;/span&gt; which for our application represents the &lt;span class=&#34;math inline&#34;&gt;\(8 \times 8\)&lt;/span&gt; adjacency matrices stacked in the third mode to build a tensor of size &lt;span class=&#34;math inline&#34;&gt;\(8 \times 8 \times 100\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{G}\)&lt;/span&gt; is the &lt;strong&gt;core tensor&lt;/strong&gt; which has dimensions &lt;span class=&#34;math inline&#34;&gt;\(R_1 \times R_2 \times R_3\)&lt;/span&gt; and is smaller than the dimensions of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt; where each &lt;span class=&#34;math inline&#34;&gt;\(R_j \leq I_j\)&lt;/span&gt; – therefore, the core tensor “compresses” the information in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are the &lt;strong&gt;factor matrices&lt;/strong&gt; which have dimensions &lt;span class=&#34;math inline&#34;&gt;\(I_1 \times R_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(I_2 \times R_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_3 \times R_3\)&lt;/span&gt; respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By analogy with principal components analysis, the factor matrices &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are the principal components in each of the modes and the core tensor represents the interactions between the components &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kolda2009tensor&#34; role=&#34;doc-biblioref&#34;&gt;Kolda and Bader 2009&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The iterative algorithm for computing the factor matrices and core tensor subject to non-negativity constraints can be found in in Chapter 7 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cichocki2009nonnegative&#34; role=&#34;doc-biblioref&#34;&gt;Cichocki et al. 2009&lt;/a&gt;)&lt;/span&gt; and connections to latent structure and statistics can be found in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-shashua2005non&#34; role=&#34;doc-biblioref&#34;&gt;Shashua and Hazan 2005&lt;/a&gt;)&lt;/span&gt;. Here, we use the &lt;a href=&#34;https://cran.r-project.org/package=nnTensor&#34;&gt;&lt;code&gt;nnTensor&lt;/code&gt;&lt;/a&gt; package implementation for NTD.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;application&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Application&lt;/h1&gt;
&lt;p&gt;Let’s return to the example we started with, recovering adjacency matrices from 100 noisy samples.&lt;/p&gt;
&lt;p&gt;The code for what follows is computationally expensive, so is posted seperately on GITHUB LINK.&lt;/p&gt;
&lt;div id=&#34;finding-an-appropriate-rank-for-the-core-tensor&#34; class=&#34;section level2&#34; number=&#34;4.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;4.1&lt;/span&gt; Finding an Appropriate Rank for the Core Tensor&lt;/h2&gt;
&lt;p&gt;The example of NMF on the “H” image above demonstrates that knowing &lt;em&gt;in advance&lt;/em&gt; what the expected components are helps decide on the truncation (or reduced rank) to use. In the tensor example of 100 adjacency matrices, assume we don’t know that there are four prototypical adjacency matrices in advance. We need to decide on the size of the core tensor (and the corresponding factor matrices) without this information.&lt;/p&gt;
&lt;p&gt;Recall also that the core tensor is a compressed representation of the data in each of the three modes.&lt;/p&gt;
&lt;p&gt;Here’s the approach; we use NTD to obtain a compressed representation and treat the factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; (corresponding to the third mode of the tensor &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{X}\)&lt;/span&gt;) as a projection (i.e. dimensionality reduction) to locate a candidate number of clusters for all 100 samples.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute a low-rank NTD with &lt;span class=&#34;math inline&#34;&gt;\(R_1 = R_2 = 8\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(R_3 = 3\)&lt;/span&gt; – so, compressing over the third mode (the ‘stacking’ of the adjacency matrices) but leaving the modes corresponding to the adjacency matrix rows/columns uncompressed. Reducing on &lt;span class=&#34;math inline&#34;&gt;\(R_3 = 3\)&lt;/span&gt; is somewhat arbitrary, but allows us to visualise the resulting factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; in Step 2&lt;/li&gt;
&lt;li&gt;Take the factor matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C} \in \mathbb{R}^{100 \times 3}\)&lt;/span&gt; which represents each of the 100 adjacency matrices projected in a space &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^3\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Perform &lt;em&gt;k&lt;/em&gt;-means clustering in &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}^3\)&lt;/span&gt; with multiple re-starts over a range of candidate cluster numbers (e.g. 2 through to 10 clusters); for each number of clusters, use the Gap statistic to ascertain the optimal number of clusters.&lt;/li&gt;
&lt;li&gt;Repeat steps 1–3 a number of times (say, 20) recording the optimum number of clusters located in Step 3&lt;/li&gt;
&lt;li&gt;Take the mode number of clusters &lt;span class=&#34;math inline&#34;&gt;\(\#c\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We repeat steps 1–3 because (as for NMF) NTD uses an iterative algorithm not guaranteed to find a global optimum and often, NTD locates quite different factorisations for different initialisations.&lt;/p&gt;
&lt;p&gt;On our example data, here’s what we obtain:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;r3-clusters.png&#34; width=&#34;80%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each plot is one of three projections of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; (it looks a lot cooler as a 3D plot but it’s harder to see the clusters clearly).&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bader2006algorithm&#34; class=&#34;csl-entry&#34;&gt;
Bader, Brett W, and Tamara G Kolda. 2006. &lt;span&gt;“Algorithm 862: MATLAB Tensor Classes for Fast Algorithm Prototyping.”&lt;/span&gt; &lt;em&gt;ACM Transactions on Mathematical Software (TOMS)&lt;/em&gt; 32 (4): 635–53.
&lt;/div&gt;
&lt;div id=&#34;ref-cichocki2009nonnegative&#34; class=&#34;csl-entry&#34;&gt;
Cichocki, Andrzej, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. 2009. &lt;em&gt;Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-hassaine2020untangling&#34; class=&#34;csl-entry&#34;&gt;
Hassaine, Abdelaali, Gholamreza Salimi-Khorshidi, Dexter Canoy, and Kazem Rahimi. 2020. &lt;span&gt;“Untangling the Complexity of Multimorbidity with Machine Learning.”&lt;/span&gt; &lt;em&gt;Mechanisms of Ageing and Development&lt;/em&gt; 190: 111325. &lt;a href=&#34;https://doi.org/10.1016/j.mad.2020.111325&#34;&gt;https://doi.org/10.1016/j.mad.2020.111325&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kolda2009tensor&#34; class=&#34;csl-entry&#34;&gt;
Kolda, Tamara G, and Brett W Bader. 2009. &lt;span&gt;“Tensor Decompositions and Applications.”&lt;/span&gt; &lt;em&gt;SIAM Review&lt;/em&gt; 51 (3): 455–500.
&lt;/div&gt;
&lt;div id=&#34;ref-lee1999learning&#34; class=&#34;csl-entry&#34;&gt;
Lee, Daniel D, and H Sebastian Seung. 1999. &lt;span&gt;“Learning the Parts of Objects by Non-Negative Matrix Factorization.”&lt;/span&gt; &lt;em&gt;Nature&lt;/em&gt; 401 (6755): 788–91.
&lt;/div&gt;
&lt;div id=&#34;ref-shashua2005non&#34; class=&#34;csl-entry&#34;&gt;
Shashua, Amnon, and Tamir Hazan. 2005. &lt;span&gt;“Non-Negative Tensor Factorization with Applications to Statistics and Computer Vision.”&lt;/span&gt; In &lt;em&gt;Proceedings of the 22nd International Conference on Machine Learning&lt;/em&gt;, 792–99.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
