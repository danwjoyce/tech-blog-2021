<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>risk | Dan W Joyce</title>
    <link>/tag/risk/</link>
      <atom:link href="/tag/risk/index.xml" rel="self" type="application/rss+xml" />
    <description>risk</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 22 Jul 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hub05f02e644906e3d80c1c494250cac2e_12120_512x512_fill_lanczos_center_2.png</url>
      <title>risk</title>
      <link>/tag/risk/</link>
    </image>
    
    <item>
      <title>Sequential Clinical Decision Making</title>
      <link>/post/2021-05-23-sequential-triage/</link>
      <pubDate>Wed, 22 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/2021-05-23-sequential-triage/</guid>
      <description>
&lt;script src=&#34;/post/2021-05-23-sequential-triage/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/2021-05-23-sequential-triage/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/2021-05-23-sequential-triage/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;In clinical decision making for serious but rare events, there has been discussion about how to use predictive models as tools in decision making.&lt;/p&gt;
&lt;p&gt;One example is in decision making for assessment and treatment in people at risk of suicide. A systematic review &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-kessler2019suicide&#34; role=&#34;doc-biblioref&#34;&gt;Kessler et al.&lt;/a&gt; (&lt;a href=&#34;#ref-kessler2019suicide&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; of “suicide prediction models” cites the very real concern that the low positive predictive value (PPV) of current state-of-the-art models renders them at least clinically useless and at worst obviously dangerous. Most published models, however, attempt to predict the absolute risk of suicide based on some feature data (i.e. covariates, independent variables or predictors) for individuals – that is, these models attempt to identify people at risk of suicide. A central tennet of Kessler &lt;em&gt;et al&lt;/em&gt;’s argument is that &lt;strong&gt;net benefit&lt;/strong&gt; – rather than positive predictive value – is the appropriate decision-theoretic framework and in effect, predictive models might be better used as tools for screening out cases (of course, their argument and analysis is far more detailed but this is what I’m focusing on here). Kessler &lt;em&gt;et al&lt;/em&gt; describe how to improve the clinical utility of suicide prediction models by embedding them in a clinical triaging system and using thresholds for intervening (or not) derived from &lt;a href=&#34;https://www.mskcc.org/departments/epidemiology-biostatistics/biostatistics/decision-curve-analysis&#34;&gt;decision curve analysis&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;Vickers, Van Calster, and Steyerberg&lt;/a&gt; (&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Kessler &lt;em&gt;et al&lt;/em&gt;’s proposal is that if there is a high prevalence of &lt;em&gt;negative&lt;/em&gt; cases in routine clinical practice, then such a staged triaging system would enable scarce (and often, intrusive) clinical resources to be directed towards cases which are uncertain. In this post, we consider positive and negative predictive value, net benefit as well as examining a sequential triage model of clinical decision support.&lt;/p&gt;
&lt;div id=&#34;predictive-values&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Predictive Values&lt;/h1&gt;
&lt;p&gt;With an assumed representative sample of a population, let &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; be the output of the decision rule/system, and &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; be whether or not the event occurred:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(Y = 1\)&lt;/span&gt; represents the decision that a case is &lt;strong&gt;positive&lt;/strong&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Y = 0\)&lt;/span&gt; represents a &lt;strong&gt;negative&lt;/strong&gt; decision&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(E = 1\)&lt;/span&gt; represents the serious event &lt;strong&gt;occuring&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(E = 0\)&lt;/span&gt; that it did not&lt;/li&gt;
&lt;/ul&gt;
Consider the following hypothetical confusion matrix for a decision system on a representative validation sample of 1000 people:
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
900
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
80
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are 20 serious events in 1000 cases. The model gives a correct decision for 900/980 negative events (true negatives, TN) and decides that 80 negative cases are in fact positve (false positives; FP). The model performs poorly on decisions with positive cases; it decides 10/20 positive events are positive (true positives, TP) and makes the potentially catastrophic decision that 10/20 positives are in fact negatives (false negatives, FN).&lt;/p&gt;
&lt;p&gt;So, we find that :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity = &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 1 \mid E = 1)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt; = 0.5; the probability that the decision was positive, given the event was positive&lt;/li&gt;
&lt;li&gt;Specificity = &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 0 \mid E = 0)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt; = 0.918; the probability that the decision was negative, given the event was negative&lt;/li&gt;
&lt;li&gt;The prevalence of the serious event is 0.02&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As noted in the Altman and Bland classic &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-altman-pred-values1994&#34; role=&#34;doc-biblioref&#34;&gt;Altman and Bland 1994&lt;/a&gt;)&lt;/span&gt;, “the whole point of a diagnostic test is to use it to make a diagnosis, so we need to know the probability that the test will give the correct diagnosis” and sensitivity and specificity of the test (here, the decision rule) aren’t sufficient.&lt;/p&gt;
&lt;p&gt;The important point is, in a clinical situation, we are interested in the conditional probabilities &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E \mid Y)\)&lt;/span&gt; (the probability of the event given the decision rule output).&lt;/p&gt;
&lt;p&gt;However, we only have the conditionals &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y \mid E )\)&lt;/span&gt; and conditional probabilities do not commute so &lt;span class=&#34;math inline&#34;&gt;\(\Pr( Y \mid E) \neq \Pr( E \mid Y)\)&lt;/span&gt;. Failure to recognise this difference is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Prosecutor%27s_fallacy&#34;&gt;prosecutor’s fallacy&lt;/a&gt; or the fallacy of the &lt;a href=&#34;https://rationalwiki.org/wiki/Confusion_of_the_inverse&#34;&gt;transposed conditional&lt;/a&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-blitzstein2019introduction&#34; role=&#34;doc-biblioref&#34;&gt;Blitzstein and Hwang 2019&lt;/a&gt;)&lt;/span&gt; Chapter 2.8.&lt;/p&gt;
&lt;p&gt;We will need to enumerate the probabilities of other conditions (i.e. states of &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;), so:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 1 \mid E = 0)\)&lt;/span&gt; is the false positive rate, or 1-specificity = &lt;span class=&#34;math inline&#34;&gt;\(1-\Pr(Y=0 \mid E=0)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(FP/(FP+TN)\)&lt;/span&gt; = 0.082&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y = 0 \mid E = 1)\)&lt;/span&gt; is the false negative rate, or 1-sensitivity = &lt;span class=&#34;math inline&#34;&gt;\(1-\Pr(Y=1 \mid E=1)\)&lt;/span&gt; = &lt;span class=&#34;math inline&#34;&gt;\(FN/(FN+TP)\)&lt;/span&gt; = 0.5&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;deriving-ppv-and-npv&#34; class=&#34;section level2&#34; number=&#34;1.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.1&lt;/span&gt; Deriving PPV and NPV&lt;/h2&gt;
&lt;p&gt;The definition of &lt;a href=&#34;https://en.wikipedia.org/wiki/Conditional_probability&#34;&gt;conditional probability&lt;/a&gt; means that, for the conditions &lt;strong&gt;we want&lt;/strong&gt;, we can state:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) = \frac{\Pr(E,Y)}{\Pr(Y)}
\]&lt;/span&gt;
Or, by rearranging:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E,Y) = \Pr(E \mid Y) \Pr(Y)
\]&lt;/span&gt;
Applying the same argument for the conditionals &lt;strong&gt;we have&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y \mid E)\)&lt;/span&gt; (sensitivity and specificity):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(Y,E) = \Pr(Y \mid E) \Pr(E)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;joint probability&lt;/strong&gt; of two events are commutative (unlike conditionals) therefore &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E,Y) = \Pr(Y,E)\)&lt;/span&gt; and we can equate:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) \Pr(Y) = \Pr(Y \mid E) \Pr(E)
\]&lt;/span&gt;
Noting again that we are interested in &lt;span class=&#34;math inline&#34;&gt;\(\Pr(E \mid Y)\)&lt;/span&gt; we can solve:
&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E \mid Y) = \frac{\Pr(Y \mid E) \Pr(E)}{\Pr(Y)}
\]&lt;/span&gt;
This is Bayes formula.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-calc-PPV-NPV&#34; class=&#34;section level2&#34; number=&#34;1.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;1.2&lt;/span&gt; Example Calculation&lt;/h2&gt;
&lt;p&gt;Using our example above, here’s want we want, and what we have available:
&lt;span class=&#34;math display&#34;&gt;\[
\Pr(E=1 \mid Y=1) = \frac{ \overbrace{\Pr( Y=1 \mid E=1)}^\text{sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} } 
                    { \underbrace{\Pr(Y=1)}_\text{prob. of +ve decision} }
\]&lt;/span&gt;
We can calculate the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=1)\)&lt;/span&gt;, the unconditional probability of a positive decision, using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Law_of_total_probability&#34;&gt;law of total probability&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\Pr(Y=1) =&amp;amp; \overbrace{\Pr( Y=1 \mid E=1)}^\text{sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} + \\
         &amp;amp;\underbrace{\Pr( Y=1 \mid E=0)}_\text{1-specificity} \underbrace{\Pr(E=0)}_\text{1-prevalence}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The calculation step-by-step is:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=1) = 0.5 \times 0.02 + (1-0.918) \times (1-0.02) = 0.09\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Substitute sensitivity and prevalence in the numerator:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E=1 \mid Y=1) = \frac{ \overbrace{0.5}^\text{sensitivity} \times \overbrace{0.02}^\text{prevalence} } 
                    { \underbrace{0.09}_\text{prob. of +ve decision} } = 0.11
  \]&lt;/span&gt;
Which delivers the &lt;strong&gt;positive predictive value&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We can similarly derive the &lt;strong&gt;negative predictive value&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\Pr(E=0 \mid Y=0) = \frac{ \overbrace{\Pr( Y=0 \mid E=0)}^\text{specificity} \overbrace{\Pr(E=0)}^\text{1-prevalence} } 
                    { \underbrace{\Pr(Y=0)}_\text{prob. of -ve decision} }
\]&lt;/span&gt;
And our denominator in this case:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  \Pr(Y=0) =&amp;amp; \overbrace{\Pr( Y=0 \mid E=1)}^\text{1-sensitivity} \overbrace{\Pr(E=1)}^\text{prevalence} + \\
            &amp;amp; \underbrace{\Pr( Y=0 \mid E=0)}_\text{specificity} \underbrace{\Pr(E=0)}_\text{1-prevalence}
\end{aligned}
\]&lt;/span&gt;
Plugging in the numbers:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Compute the denominator &lt;span class=&#34;math inline&#34;&gt;\(\Pr(Y=0) = (1-0.5) \times 0.02 + 0.918 \times (1-0.02) = 0.91\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Substitute specificity and prevalence in the numerator:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  \Pr(E=0 \mid Y=0) = \frac{ \overbrace{0.918}^\text{specificity} \times \overbrace{0.98}^\text{1-prevalence} } 
                    { \underbrace{0.91}_\text{prob. of -ve decision} } = 0.987
  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This hypothetical decision system is useful for correctly deciding on negative cases, but performs poorly on identifying positive cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Simulation&lt;/h1&gt;
&lt;p&gt;Now suppose that we have two (or more) clinical tests to help identify patients at risk for a relatively rare but serious event; for example, &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is a relatively cheap and easy-to-administer instrument or questionnaire. &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; is a semi-structured interview or clinical examination which is time consuming, requires expertise to administer and is therefore significantly more costly than &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Further, we have a development sample of 5000 people for which we have the results for &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; and we know who in this sample experienced the serious event.&lt;/p&gt;
&lt;p&gt;We next build a model that attempts to predict the rare, serious event (&lt;span class=&#34;math inline&#34;&gt;\(Y = 1\)&lt;/span&gt;) on the basis of a patient’s &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; results and denote this &lt;span class=&#34;math inline&#34;&gt;\(y = F_{X_1}(x)\)&lt;/span&gt;. Note, no claim is made that this model is well designed.&lt;/p&gt;
&lt;p&gt;Assume the somewhat luxurious position that we have a further 5000 validation cases from the same population – so we can examine the model’s performance on data it was not ‘trained’ on.&lt;/p&gt;
&lt;p&gt;Let’s look at the calibration of the model on the validation sample:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s important to note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;model&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; delivers a prediction in the form of a continuous estimate of the absolute probability of the serious event &lt;em&gt;given&lt;/em&gt; the screening instrument &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;there is &lt;strong&gt;no decision rule&lt;/strong&gt; here; so we can’t discuss PPV or NPV&lt;/li&gt;
&lt;li&gt;the model is poorly calibrated: which is unsurprising given the serious event is rare – in the 5000 validation samples there were 240 serious events (&lt;span class=&#34;math inline&#34;&gt;\(E = 1\)&lt;/span&gt;) representing a small prevalence of 0.048&lt;/li&gt;
&lt;li&gt;the model appears to &lt;strong&gt;under estimate&lt;/strong&gt; the probability of a serious event; for example, if the model predicts a probability of a serious event of 0.25, the actual probability is closer to 0.5.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can repeat the same analysis for the other, more costly instrument &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;; as for the cheaper instrument, we train a model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; and then we have access to a validation sample on which we can examine the calibration:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Again, not great calibration.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-decision-rules&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Decision Rules&lt;/h1&gt;
&lt;p&gt;Returning to the idea that Kessler &lt;em&gt;et al&lt;/em&gt; discussed, how can we design a decision rule that makes use of these two tests ?&lt;/p&gt;
&lt;div id=&#34;sec-ROC-curve&#34; class=&#34;section level2&#34; number=&#34;3.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;3.1&lt;/span&gt; ROC Curves&lt;/h2&gt;
&lt;p&gt;A common approach to designing a decision rule is to vary a threshold over the output of &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and plot the ROC curve; then, find an “optimal” threshold that maximises the sensitivity/specificity tradeoff.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Setting levels: control = 0, case = 1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Setting direction: controls &amp;lt; cases&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The confusion matrix for the decision rule with the threshold = 0.074 shown in the ROC curve above is:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
3953
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
44
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;width: 6em; &#34;&gt;
807
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
196
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The usual measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sensitivity = 0.8167&lt;/li&gt;
&lt;li&gt;Specificity = 0.8305&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can compute the &lt;em&gt;more&lt;/em&gt; clinically relevant probabilties as follows (as for Section &lt;a href=&#34;#sec-calc-PPV-NPV&#34;&gt;1.2&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Positive predictive value: &lt;span class=&#34;math inline&#34;&gt;\(\Pr( E = 1 \mid Y = 1 )\)&lt;/span&gt; = 0.195&lt;/li&gt;
&lt;li&gt;Negative predictive value: &lt;span class=&#34;math inline&#34;&gt;\(\Pr( E = 0 \mid Y = 0 )\)&lt;/span&gt; = 0.989&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Critically, however, false negatives (44) are catastrophic here because the event, although rare, is serious (i.e. the death of a patient); but &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; is correctly identifying a high number (3953) of negative cases correctly.&lt;/p&gt;
&lt;p&gt;As discussed &lt;a href=&#34;https://danwjoyce.netlify.app/post/loss-functions-and-posteriors/&#34;&gt;here&lt;/a&gt; and more persuasively &lt;a href=&#34;https://www.fharrell.com/post/backwards-probs/&#34;&gt;here&lt;/a&gt; sensitivity and specificity do not take account of the loss or utility of the decision and neither do PPV and NPV.&lt;/p&gt;
&lt;p&gt;To understand why this neglect of utility (or loss) is important, take the above confusion matrix and then assume the decision rule declares one additional &lt;strong&gt;false negative&lt;/strong&gt;, so that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of &lt;strong&gt;false negatives&lt;/strong&gt; (FN) = 45&lt;/li&gt;
&lt;li&gt;the &lt;em&gt;actual&lt;/em&gt; number of positive events is of course, unchanged at 240&lt;/li&gt;
&lt;li&gt;so conversely, the &lt;strong&gt;true positive&lt;/strong&gt; rate drops by one so TP = 195&lt;/li&gt;
&lt;li&gt;the revised &lt;strong&gt;sensitivity&lt;/strong&gt; is then 0.8125 – a decrease in decision rule performance of 0.0042&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reverse the experiment, so that the decision rule improves marginally and declares one additional &lt;strong&gt;true positive&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the number of &lt;strong&gt;true positives&lt;/strong&gt; increases by one, TP = 197&lt;/li&gt;
&lt;li&gt;the number of &lt;strong&gt;false negatives&lt;/strong&gt; decreases by one, FN = 43&lt;/li&gt;
&lt;li&gt;the revised &lt;strong&gt;sensitivity&lt;/strong&gt; is then 0.8208 – an increase in decision rule performance of 0.0041&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The change in performance (the “score”) for one additional correct or incorrectly classified positive case is symmetric and of the order &lt;span class=&#34;math inline&#34;&gt;\(1/N\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size.&lt;/p&gt;
&lt;p&gt;Clearly, an &lt;strong&gt;additional false negative&lt;/strong&gt; should penalise the overall performance score &lt;em&gt;differently&lt;/em&gt; than the reward for an additional true positive.&lt;/p&gt;
&lt;p&gt;Optimising the threshold (decision rule) by maximising the sensitivity-specificity tradeoff (e.g. using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&#34;&gt;Yourdon J statistic&lt;/a&gt;) is not the only method of choosing the threshold and we might for example, choose a decision rule that favours performance of different cells of the confusion matrix. A decision theoretic framework like &lt;strong&gt;net benefit&lt;/strong&gt; allows one systematic treatment.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;decision-curve-analysis&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Decision Curve Analysis&lt;/h1&gt;
&lt;p&gt;Here, we are trying to implement a decision rule whereby a patient is triaged to a more costly “test” (&lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;) on the basis of a more available or less costly test &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can adopt a decision-theoretic approach (see previous posts &lt;a href=&#34;https://danwjoyce.netlify.app/post/decisions-and-loss-functions/&#34;&gt;here&lt;/a&gt;) and design a loss (conversely, a utility) function for a decision rule (threshold).&lt;/p&gt;
&lt;p&gt;Assume that we continue to insist on a “hard” decision rule that decides, on the basis of &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;, whether to further investigate (triage to &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;) or, decide that the serious event is unlikely so no further follow-up is necessary and the patient can be discharged.&lt;/p&gt;
&lt;p&gt;In this situation, we can construct the confusion matrix below:&lt;/p&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;empty-cells: hide;border-bottom:hidden;&#34; colspan=&#34;1&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; &#34; colspan=&#34;2&#34;&gt;
&lt;div style=&#34;border-bottom: 1px solid #ddd; padding-bottom: 5px; &#34;&gt;
Event (E)
&lt;/div&gt;
&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
0
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
1
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr grouplength=&#34;2&#34;&gt;
&lt;td colspan=&#34;3&#34; style=&#34;border-bottom: 1px solid;&#34;&gt;
&lt;strong&gt;Decision (Y)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
TN
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
FN
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;padding-left: 4em;width: 6em; &#34; indentlevel=&#34;2&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;width: 6em; &#34;&gt;
FP
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
TP
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And then assign a loss to each cell e.g. the loss for a true negative is &lt;span class=&#34;math inline&#34;&gt;\(L_{TN}\)&lt;/span&gt;, for a false negative &lt;span class=&#34;math inline&#34;&gt;\(L_{FN}\)&lt;/span&gt; and so on.&lt;/p&gt;
&lt;p&gt;For a given decision rule (here, the decision rule can be equated with the threshold value &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;) we can compute the &lt;strong&gt;expected loss&lt;/strong&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
  L( \tau ) = \frac{1}{N} \left( \#TN \cdot L_{TN} + \#FN \cdot L_{FN} + \#FP \cdot L_{FP} + \#TP \cdot L_{TP} \right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\#TN\)&lt;/span&gt; is the number of true negatives under the decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; etc. and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is the sample size. We then systematically vary &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and choose our final decision rule on the basis of minimum loss.&lt;/p&gt;
&lt;p&gt;The difficulty is that it is often hard to quantify losses (or value, utility) either absolutely or relatively for each cell of the confusion matrix – the example of Kessler &lt;em&gt;et al&lt;/em&gt; examines predictive models for suicide, where a false negative would be catastrophic; is the loss incurred for a false negative 10, 100 or 1000 times ‘worse’ than a true negative ?&lt;/p&gt;
&lt;p&gt;An alternative, proposed by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;Vickers, Van Calster, and Steyerberg&lt;/a&gt; (&lt;a href=&#34;#ref-Vickers2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;, is to first set up a decision tree representing decisions to intervene / not intervene for the combinations shown in the standard confusion matrix. We then assume that the loss of &lt;strong&gt;not intervening&lt;/strong&gt; when we should (a &lt;strong&gt;false negative&lt;/strong&gt;) is fixed at unity and the loss of a &lt;strong&gt;false positive&lt;/strong&gt; is defined relative to this for a given threshold &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. After some algebra, the loss attributable to a &lt;strong&gt;false positive&lt;/strong&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\tau}{1-\tau}
\]&lt;/span&gt;
Then, the &lt;strong&gt;net benefit&lt;/strong&gt; of a decision rule (value of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;) is then:
&lt;span class=&#34;math display&#34;&gt;\[
NB(\tau) = \frac{\#TP}{N} - \frac{\#FP}{N} \left( \frac{\tau}{1-\tau} \right)
\]&lt;/span&gt;
In this equation, true positives are weighted one, and false positives weighted &lt;span class=&#34;math inline&#34;&gt;\(\tau /( 1-\tau)\)&lt;/span&gt;. As the cost of a false positive is a function of the threshold we can deduce the relative costs. For example, if &lt;span class=&#34;math inline&#34;&gt;\(\tau = 1/3\)&lt;/span&gt;, the cost of a false positive is half the cost of a true positive.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;decision curve&lt;/strong&gt; is then the plot of &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau)\)&lt;/span&gt; against &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The solid black line is the net benefit of the model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; at each threshold level. The grey horizontal line (at &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau) = 0\)&lt;/span&gt;) is the net benefit of assuming all patients are negative. The black dotted line is the net benefit of the decision rule: “assume all patients are positive and intervene” which is of course wasteful, but offers comparison to the net benefit of each decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;. The red solid vertical line shows the threshold located by maximising the sensitivity/specificity tradeoff in Section &lt;a href=&#34;#sec-ROC-curve&#34;&gt;3.1&lt;/a&gt;. Finally, the red dashed line identifies the threshold at which net benefit departs (exceeds) the “assume all positive” line.&lt;/p&gt;
&lt;p&gt;The region for which &lt;span class=&#34;math inline&#34;&gt;\(NB(\tau)\)&lt;/span&gt; is greater than zero are the thresholds for which the model outperforms “assume all patients are negative.” An advantage of decision curve analysis is that one can vary &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and see the relationship between the model performance and a default strategy of assuming everyone requires intervention – the point at which the black solid line departs from the black dotted line.&lt;/p&gt;
&lt;p&gt;Of note, decision curve analysis is not intended to be a method of locating a threshold; in fact, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-vickers2006decision&#34; role=&#34;doc-biblioref&#34;&gt;Vickers and Elkin&lt;/a&gt; (&lt;a href=&#34;#ref-vickers2006decision&#34; role=&#34;doc-biblioref&#34;&gt;2006&lt;/a&gt;)&lt;/span&gt; discuss the method in the context of shared decision making where the clinician &lt;em&gt;and&lt;/em&gt; patient’s prefence for the relative cost of a false positive are factored into deciding the utility of a decision to intervene.&lt;/p&gt;
&lt;p&gt;However, as an experiment, let’s choose a threshold at the point where the net benefit departs from the default “assume all patients are positive and intervene” – shown as the red dotted line in the right panel at 0.01. This results in zero false negatives (serious errors), 240 true positives, 506 true negatives and 4252 false positives.&lt;/p&gt;
&lt;p&gt;To put this in the context of a sequential triage model, 4252 patients who are actually negative would be triaged for the &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; assessment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sequential-triage&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Sequential Triage&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;triage-model.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The proposal Kessler &lt;em&gt;et al&lt;/em&gt; put forward is a sequential triage model; above, we have sketched (schematically) the two-stage approach described here.&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_1\)&lt;/span&gt; is the total sample (of size &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt;, containing &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_{1}\)&lt;/span&gt; positive and negative cases respectively) who have been assessed using &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and a decision made on the basis of the prediction &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;. Cases are then discharged on the basis of the decision; of those discharged, &lt;strong&gt;serious errors&lt;/strong&gt; and &lt;strong&gt;appropriate discharges&lt;/strong&gt; are analogous to the number of false negative &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_1}\)&lt;/span&gt; and true negative &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1}\)&lt;/span&gt; decisions respectively.&lt;/p&gt;
&lt;p&gt;Those identified as likely positive by the decision &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; form the triaged subset &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; of size &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt;, who proceed to the more resource intensive assessment &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;. A similar decision system &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; with rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; then either discharges or (in this example) recommends admission to hospital.&lt;/p&gt;
&lt;p&gt;Note that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N_2 = N^{+ve}_2 + N^{-ve}_2\)&lt;/span&gt; – the size of triaged set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; depends on the number of &lt;em&gt;actually&lt;/em&gt; positive and negative cases triaged.&lt;/li&gt;
&lt;li&gt;In the sequential arrangement, &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; &lt;em&gt;depends&lt;/em&gt; on the performance of &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;. For example, if a case is incorrectly discharged at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; does not have an opportunity to ‘correct’ that error, so: &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_{2} = N^{+ve}_{1} - \#FN_{X_1}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this in mind, we now attempt to define measures of performance in terms &lt;strong&gt;safety&lt;/strong&gt; and &lt;strong&gt;efficiency&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;safety&#34; class=&#34;section level2&#34; number=&#34;5.1&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.1&lt;/span&gt; Safety&lt;/h2&gt;
&lt;p&gt;From the discussion in Section &lt;a href=&#34;#sec-calc-PPV-NPV&#34;&gt;1.2&lt;/a&gt;, decision systems with favourable NPV (but poor PPV) might be helpful in screening out candidates at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and triaging suspected positive cases to &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define a &lt;strong&gt;safe&lt;/strong&gt; decision system as having these properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;at each stage (i.e. at &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt;), anyone with a &lt;strong&gt;high&lt;/strong&gt; or &lt;strong&gt;uncertain&lt;/strong&gt; probability of being positive is appropriately triaged&lt;/li&gt;
&lt;li&gt;serious errors are &lt;strong&gt;minimised&lt;/strong&gt; by &lt;em&gt;not&lt;/em&gt; discharging people inappropriately, which means it should minimise &lt;strong&gt;false negatives&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We’ll define the &lt;strong&gt;total safety&lt;/strong&gt; of the system as a function of the number of serious errors made by both &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S_{Total}(\tau_1, \tau_2) &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{N^{+ve}_{1}+N^{+ve}_{2}} \\
                            &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{N^{+ve}_{1}+(N^{+ve}_{1} - \#FN_{X_1})} \\
                            &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{2N^{+ve}_{1} - \#FN_{X_1}}
\end{aligned}
\]&lt;/span&gt;
As a concrete example (with respect to the diagram above and using the same validation set used in the decision curve and ROC analysis above):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_1\)&lt;/span&gt; is of size &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; = 5000 with &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_1\)&lt;/span&gt; = 240 actual positive and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_1\)&lt;/span&gt; = 4760 actual negative cases&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After administering &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt;, using the threshold &lt;span class=&#34;math inline&#34;&gt;\(\tau_1 = 0.10\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; results in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;80 serious errors or inappropriate discharges equal to false negatives, &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;4321 appropriate discharges – equal to the true negatives, &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Resulting in &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt; = 599 cases triaged into &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P_2}\)&lt;/span&gt; equating to the sum of false and true positive cases (i.e. those declared positive by &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; = 0.10)&lt;/li&gt;
&lt;li&gt;Of these 599 cases, &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_2\)&lt;/span&gt; = 160 and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_2\)&lt;/span&gt; = 439&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, let’s assume &lt;span class=&#34;math inline&#34;&gt;\(\tau_2 = 0.36\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; applied to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt;. We arrive at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;57 serious errors, equating to &lt;span class=&#34;math inline&#34;&gt;\(\#FN_{X_2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;430 appropriate discharges, the true negatives &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_2}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Resulting in &lt;span class=&#34;math inline&#34;&gt;\(N_3\)&lt;/span&gt; = 112 cases which will be admitted, consisting of &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_3\)&lt;/span&gt; = 103 actually positive cases (appropriate admissions) and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_3\)&lt;/span&gt; = 9 actually negative cases (inappropriate admissions)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Substituting these numbers in the equation above for safety:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  S_{Total}(\tau_1, \tau_2) &amp;amp;= 1 - \frac{\#FN_{X_1}+\#FN_{X_2}}{2N^{+ve}_{1} - \#FN_{X_1}} \\
                            &amp;amp;= 1 - \frac{80 + 57}{480 - 80} \\
                            &amp;amp;= 0.6575
\end{aligned}
\]&lt;/span&gt;
If the decisions made by &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; resulted in no serious errors, we would have zero false negatives and &lt;span class=&#34;math inline&#34;&gt;\(S_{Total}\)&lt;/span&gt; would attain a maximum of one.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;efficiency&#34; class=&#34;section level2&#34; number=&#34;5.2&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.2&lt;/span&gt; Efficiency&lt;/h2&gt;
&lt;p&gt;Now consider efficiency defined as the &lt;strong&gt;ratio&lt;/strong&gt; of useful &lt;strong&gt;product&lt;/strong&gt; to &lt;strong&gt;resource&lt;/strong&gt; consumed.&lt;/p&gt;
&lt;p&gt;Here, the denominator – resource consumption – is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all patients &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt; will have &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; administered and be passed through the prediction model &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt;, so resource consumed is &lt;span class=&#34;math inline&#34;&gt;\(N_1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;a subset of patients declared positive by the decision rule &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; are triaged to &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; which is composed of &lt;span class=&#34;math inline&#34;&gt;\(N^{+ve}_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N^{-ve}_2\)&lt;/span&gt; actual positive and negative cases respectively&lt;/li&gt;
&lt;li&gt;all patients in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P}_2\)&lt;/span&gt; are administed &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; – so resource consumed is &lt;span class=&#34;math inline&#34;&gt;\(N_2\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The numerator – useful product – needs elaboration. First consider the efficiency of &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt;, which will be perfectly efficient if all actual negative cases are discharged (and &lt;em&gt;do not&lt;/em&gt; end up in &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{P_2}\)&lt;/span&gt;) and all positive cases are triaged.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  E_{X_1}( \tau_1 ) = \frac{\#TP_{X_1} + \#TN_{X_1}}{N_1}
\]&lt;/span&gt;
Which attains a maximum efficiency of one when &lt;span class=&#34;math inline&#34;&gt;\(\#TN_{X_1} = N^{-ve}_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\#TP_{X_1} = N^{+ve}_{1}\)&lt;/span&gt; (recall that &lt;span class=&#34;math inline&#34;&gt;\(N_1 = N^{+ve}_1 + N^{-ve}_2\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;A similar definition holds for &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  E_{X_2}( \tau_2 ) = \frac{\#TP_{X_2} + \#TN_{X_2}}{N_2}
\]&lt;/span&gt;
And we allow &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; to contribute equally to a total efficiency in the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; defined as:
&lt;span class=&#34;math display&#34;&gt;\[
  E_{Total}( \tau_1, \tau_2 ) = \frac{1}{2} \left[ E_{X_1}( \tau_1 ) + E_{X_2}( \tau_2 )  \right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;performance-of-sequential-triage&#34; class=&#34;section level2&#34; number=&#34;5.3&#34;&gt;
&lt;h2&gt;&lt;span class=&#34;header-section-number&#34;&gt;5.3&lt;/span&gt; Performance of Sequential Triage&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/2021-05-23-sequential-triage/index_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The plots show safety (left) and efficiency (right) in contours of size 0.1. So, if &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; is less than around 0.05 and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; is less than approximately 1.5, the sequential triage system has overall safety &amp;gt; 0.9.&lt;/p&gt;
&lt;p&gt;However, the tension is that: for the same range of decision thresholds, the efficiency can reach a maximum of 0.747 but this results in upto 31 serious errors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;concluding-remarks&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Concluding Remarks&lt;/h1&gt;
&lt;p&gt;A few observations:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There is no clear way of robustly setting the decision rules &lt;span class=&#34;math inline&#34;&gt;\(\tau_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau_2\)&lt;/span&gt; based on performance measures for sequential triage unless one is prepared to accept that efficiency and safety trade-off&lt;/li&gt;
&lt;li&gt;Using decision curve analysis &lt;strong&gt;properly&lt;/strong&gt; invites setting the decision threshold according to a tradeoff that rightly involves how much risk the patient and clinician want to take: i.e. net benefit provides a weight to false positives (harm attributable to intervening when it is unnecessary) as a function of the decision threshold.&lt;/li&gt;
&lt;li&gt;The analyses above were all conducted on the improbably ideal situation where a) we had 5000 exemplars to train a system on, both with measurements/assessments &lt;span class=&#34;math inline&#34;&gt;\(X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_2\)&lt;/span&gt; b) we had access to another ‘batch’ of 5000 exemplars to validate the model on which we attempt to define performance and optimise the system’s performance&lt;/li&gt;
&lt;li&gt;When deploying this system, at best, patients arrive in small batches or singularly (certainly not in clusters of 100s or 1000s) – the above analyses are then at best, informative for an actuarial or economic analysis if we were &lt;em&gt;forced&lt;/em&gt; to choose a decision rule to evaluate the triage system’s performance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By their very nature, predictions &lt;span class=&#34;math inline&#34;&gt;\(F_{X_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(F_{X_2}\)&lt;/span&gt; are probabilistic and discrete thresholds coerce these uncertain forecasts into definitive decisions. It seems unlikely that in the case of rare, serious events anyone would rely on a decision support system that gave discrete answers (and indeed, decision curve analysis emphasises the role of clinician expertise and patient preference in deciding on the intervention threshold)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-altman-pred-values1994&#34; class=&#34;csl-entry&#34;&gt;
Altman, Douglas G, and J Martin Bland. 1994. &lt;span&gt;“Statistics Notes: Diagnostic Tests 2: Predictive Values.”&lt;/span&gt; &lt;em&gt;Bmj&lt;/em&gt; 309 (6947): 102.
&lt;/div&gt;
&lt;div id=&#34;ref-blitzstein2019introduction&#34; class=&#34;csl-entry&#34;&gt;
Blitzstein, Joseph K, and Jessica Hwang. 2019. &lt;em&gt;Introduction to Probability&lt;/em&gt;. &lt;span&gt;CRC&lt;/span&gt; Press.
&lt;/div&gt;
&lt;div id=&#34;ref-kessler2019suicide&#34; class=&#34;csl-entry&#34;&gt;
Kessler, Ronald C, Robert M Bossarte, Alex Luedtke, Alan M Zaslavsky, and Jose R Zubizarreta. 2019. &lt;span&gt;“Suicide Prediction Models: A Critical Review of Recent Research with Recommendations for the Way Forward.”&lt;/span&gt; &lt;em&gt;Molecular Psychiatry&lt;/em&gt;, 1–12.
&lt;/div&gt;
&lt;div id=&#34;ref-vickers2006decision&#34; class=&#34;csl-entry&#34;&gt;
Vickers, Andrew J, and Elena B Elkin. 2006. &lt;span&gt;“Decision Curve Analysis: A Novel Method for Evaluating Prediction Models.”&lt;/span&gt; &lt;em&gt;Medical Decision Making&lt;/em&gt; 26 (6): 565–74.
&lt;/div&gt;
&lt;div id=&#34;ref-Vickers2016&#34; class=&#34;csl-entry&#34;&gt;
Vickers, Andrew J, Ben Van Calster, and Ewout W Steyerberg. 2016. &lt;span&gt;“Net Benefit Approaches to the Evaluation of Prediction Models, Molecular Markers, and Diagnostic Tests.”&lt;/span&gt; &lt;em&gt;BMJ&lt;/em&gt; 352. &lt;a href=&#34;https://doi.org/10.1136/bmj.i6&#34;&gt;https://doi.org/10.1136/bmj.i6&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Decisions and Loss Functions - A more clinical focus</title>
      <link>/post/decisions-and-loss-functions/</link>
      <pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/decisions-and-loss-functions/</guid>
      <description>
&lt;script src=&#34;/post/decisions-and-loss-functions/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/decisions-and-loss-functions/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/decisions-and-loss-functions/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;In the previous post, loss functions where considered in the context of estimating measures of central tendency for distributions. In this post, I want to look at the computation of loss functions in situations that might arise in a clinical predictive model. This is all textbook stuff – see &lt;a href=&#34;#sec-further-reading&#34;&gt;Further Reading&lt;/a&gt; – but I wanted to summarise it in a way I understood when in a year’s time, I wonder what the code does.&lt;/p&gt;
&lt;p&gt;I realised that the notation in the last post was sloppy, so for this post, I’ll adopt the conventions in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The basic setup is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a finite set of available actions, &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A} = \{ a_1, a_2, \ldots, a_j \}\)&lt;/span&gt; – in the examples that follow, we’ll restrict our attention to a choice between two actions of “do nothing” or “intervene/treat” respectively but there is no loss of generality in assuming this.&lt;/li&gt;
&lt;li&gt;There are uncertain quantities representing “states of the world” &lt;span class=&#34;math inline&#34;&gt;\(\Theta = \{ \theta_1, \theta_2, \ldots, \theta_i \}\)&lt;/span&gt; about which we can obtain data and that can affect our decision about which action to take. Here, these states will reflect something about a patient.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then, our task will be to choose the best action from &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A}\)&lt;/span&gt; given information about states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Information about the states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; will come from a putative predictive model: as in the previous post, measurements (the “input” to the model) for a given patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; are given to the predictive model &lt;span class=&#34;math inline&#34;&gt;\(Y = F(x)\)&lt;/span&gt; that delivers scores (the “output”) as realisations of &lt;span class=&#34;math inline&#34;&gt;\(Y \in [0,1]\)&lt;/span&gt;. Importantly, for any &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we can access samples from the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; (rather than relying on a single point prediction, such as the mean of the posterior).&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Setup&lt;/h1&gt;
&lt;p&gt;To begin with, assume the simplest case of there being two states &lt;span class=&#34;math inline&#34;&gt;\(\Theta = \{ \theta_1, \theta_2 \}\)&lt;/span&gt; which correspond to a patient being “negative” or “positive” (respectively) for some event or outcome. Our repetoire of actions is &lt;span class=&#34;math inline&#34;&gt;\(\mathscr{A} = \{ a_1, a_2 \}\)&lt;/span&gt; representing “do nothing” and “intervene/treat” respectively. This only serves a pedagogical need when developing the ideas, not because it represents a principled or sound modelling decision.&lt;/p&gt;
&lt;p&gt;For a single example patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, the output of the model suggests the they are most likely negative (i.e. the probability mass is concentrated near zero):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
library(gridExtra)
library(kableExtra)
library(latex2exp)
library(reshape2)

# globals for presentation
basictheme &amp;lt;- theme_minimal() + 
  theme(axis.title = element_text(size = 14),
        axis.text = element_text(size = 12),
        plot.title = element_text(size = rel(1.25), face = &amp;quot;bold&amp;quot;, hjust = 0.5 ))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(3141)
range01 &amp;lt;- function(x){(x-min(x))/(max(x)-min(x))}
samples &amp;lt;- range01( rgamma( 2000, shape = 2, scale = 2) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We now require a mapping from the samples &lt;span class=&#34;math inline&#34;&gt;\(y \sim \pi_{F}(Y|x)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; because the domain of &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}\)&lt;/span&gt; will be the interval &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; and to get started, we need to “quantise” to two states.&lt;/p&gt;
&lt;p&gt;Define the distribution over states &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\pi_{\Theta}(\theta_{1}) &amp;amp;= \Pr_{\pi_{F}}( Y \leq 0.5 ) \\
\pi_{\Theta}(\theta_{2})  &amp;amp;= \Pr_{\pi_{F}}( Y &amp;gt; 0.5 )
\end{aligned}\]&lt;/span&gt;
&lt;p&gt;So, we basically histogram the samples into two bins either side of 0.5, representing the probability of a patient being negative (&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;) or positive (&lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt;). A terrible idea, which we will reverse later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdfFromSamples &amp;lt;- function(a, b, delta, samples) {
  H &amp;lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) )
  ret &amp;lt;- data.frame(
    mids  = H$mids,
    freq  = H$counts
  )
  ret$P &amp;lt;- ret$freq / sum(ret$freq)
  return(ret)
}

pdf.Y &amp;lt;- pdfFromSamples(0,1,delta = 1/2, samples)
pdf.Y$theta &amp;lt;- factor( c(1,2) )

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

print( pdf.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;According to our blunt assignment of states to output from the predictive model, the probability the patient is negative is &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_{1})\)&lt;/span&gt; = 0.921 and positive &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_{2})\)&lt;/span&gt; = 0.079.&lt;/p&gt;
&lt;p&gt;With this setup, (two actions, two states) we can “tabulate” the combinations of actions and states (the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cartesian_product&#34;&gt;Cartesian product&lt;/a&gt;: &lt;span class=&#34;math inline&#34;&gt;\(\Theta \times \mathscr{A}\)&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;loss-actions.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In each “cell” or combination &lt;span class=&#34;math inline&#34;&gt;\((\theta,a)\)&lt;/span&gt; we then assign a &lt;strong&gt;loss&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(\theta,a) \leq 0\)&lt;/span&gt; which describes the &lt;strong&gt;cost&lt;/strong&gt; incurred for taking action &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; when the state &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; obtains. Generally, we will adopt the convention that losses represent costs or penalties for actions with respect to states.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-loss-matrix&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Example Loss Matrix&lt;/h1&gt;
&lt;p&gt;Equipped with this toy example we assign losses:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- c(&amp;quot;a1:&amp;lt;br&amp;gt;do nothing&amp;quot;,&amp;quot;a2:&amp;lt;br&amp;gt;intervene&amp;quot;)
Theta &amp;lt;- c(&amp;quot;&amp;lt;b&amp;gt;theta1:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;negative&amp;quot;, &amp;quot;&amp;lt;b&amp;gt;theta2:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;positive&amp;quot;)

loss.matrix &amp;lt;- matrix( c( 0.0,  -0.5,
                          -1.0, 0 ), 
                       nrow = 2, ncol = 2, byrow = TRUE)
rownames(loss.matrix) &amp;lt;- Theta
colnames(loss.matrix) &amp;lt;- A

knitr::kable( loss.matrix, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a1:&lt;br&gt;do nothing
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a2:&lt;br&gt;intervene
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta1:&lt;/b&gt;&lt;br&gt;negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta2:&lt;/b&gt;&lt;br&gt;positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As a use example, assume we decide &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; (intervene) and the state of the patient turns out to be &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; (negative) we incur a loss of &lt;span class=&#34;math inline&#34;&gt;\(L(\theta_1,a_2) = -0.5\)&lt;/span&gt; to reflect unnecessary costs of e.g. further investigations, inconvenience to the patient etc. If we select &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; and the state is &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; (equating to doing nothing and the patient is negative) we incur zero loss because this was an appropriate action given the circumstances.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-expected-loss&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Bayesian Expected Loss&lt;/h1&gt;
&lt;p&gt;Following &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt;, we define the Bayesian expected loss (BEL) for action &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; with respect to the discrete distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[
\rho(\pi_{\Theta},a_j) = \mathbb{E}_{\pi_{\Theta}} \left[ L(\theta,a_j\right] = \sum_{i}L(\theta_i,a_j)\pi_{\Theta}(\theta_i)
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Bayesian expected loss (BEL)
BEL &amp;lt;- function( a, p.pi, loss.matrix ) {
   sum( loss.matrix[ , a ] * p.pi )
}

# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y$P, loss.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that the upper bound of the BEL is zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conditional-bayes-decision-principal&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Conditional Bayes Decision Principal&lt;/h1&gt;
&lt;p&gt;Having established the BEL for each action, the conditional bayes decision principle (CBD) for deciding on an action &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt; is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;choose &lt;span class=&#34;math inline&#34;&gt;\(a_{j} \in \mathscr{A}\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(a_j\)&lt;/span&gt; &lt;strong&gt;minimises&lt;/strong&gt; the BEL : &lt;span class=&#34;math inline&#34;&gt;\(\underset{j}{\mathrm{arg\,max}} \; \rho( \pi_{\Theta}, a_j )\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In code: the resulting vector for &lt;span class=&#34;math inline&#34;&gt;\(\rho( \pi_{\Theta}, a )\)&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable( rho.A, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rho
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.0790
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.4605
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;And the action that minimises the BEL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( min.bel.CBD &amp;lt;- which.max( rho.A$rho ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the example above, we find the action 1 (i.e. &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; = “do nothing”) minimises the BEL. This fits with our intuition given the patient is most likely negative: &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}(\theta_1) &amp;gt; \pi_{\Theta}(\theta_1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;developing-the-loss-function&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Developing the Loss Function&lt;/h1&gt;
&lt;p&gt;Consider a different patient where the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt;, the output of the predictive model, looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samples &amp;lt;- range01( c(rnorm( 1000, mean = 0, sd = 2 ), rnorm( 1000, mean = 10, sd = 3) ) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
In this example, there’s uncertainty about the patient being negative or positive.&lt;/p&gt;
&lt;p&gt;This time, we’ll quantise into &lt;em&gt;three&lt;/em&gt; equal-sized intervals over the range &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt; (again, an unprincipled decision made only for demonstration) and map to three states:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0,1,delta=1/3,samples)
pdf.Y$theta &amp;lt;- factor( seq(1,3,by=1) )

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

print( pdf.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The loss matrix will now be a &lt;span class=&#34;math inline&#34;&gt;\(3 \times 2\)&lt;/span&gt; matrix:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;A &amp;lt;- c(&amp;quot;a1:&amp;lt;br&amp;gt;do nothing&amp;quot;,&amp;quot;a2:&amp;lt;br&amp;gt;intervene&amp;quot;)
Theta &amp;lt;- c(&amp;quot;&amp;lt;b&amp;gt;theta1:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;negative&amp;quot;, 
           &amp;quot;&amp;lt;b&amp;gt;theta2:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;equivocal&amp;quot;,  
           &amp;quot;&amp;lt;b&amp;gt;theta3:&amp;lt;/b&amp;gt;&amp;lt;br&amp;gt;positive&amp;quot;)

loss.matrix &amp;lt;- matrix( c( 0.0,  -0.5,
                          -0.5,  -0.2,
                          -1.0, 0 ), 
                       nrow = 3, ncol = 2, byrow = TRUE)
rownames(loss.matrix) &amp;lt;- Theta
colnames(loss.matrix) &amp;lt;- A

knitr::kable( loss.matrix, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a1:&lt;br&gt;do nothing
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
a2:&lt;br&gt;intervene
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta1:&lt;/b&gt;&lt;br&gt;negative
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta2:&lt;/b&gt;&lt;br&gt;equivocal
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.5
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.2
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
&lt;b&gt;theta3:&lt;/b&gt;&lt;br&gt;positive
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-1.0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Bearing in mind that states are uncertain, the logic behind this loss matrix is as follows:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; (do nothing) : no cost is incurred if the patient is likely negative (&lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt;). If the patient is most likely positive (&lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt;) and we do nothing, this is evidently the wrong decision and we incur the maximum penalty of -1.0. If there is some equivocation &lt;span class=&#34;math inline&#34;&gt;\((\theta_2\)&lt;/span&gt;) – we penalise by half the maximum cost to discourage doing nothing (equating to loss aversion)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; (intervene) : for &lt;span class=&#34;math inline&#34;&gt;\(\theta_1\)&lt;/span&gt; we incur a cost (-0.5) for intervening when unnecessary. Naturally, for &lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt;, the correct thing to do is intervene so this has no penalty associated. For the equivocal case, &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt;, we should certainly not ignore these cases but simply intervening (i.e. with zero penalty) is inappropriate. So we incur a small penalty (-0.2) to nudge us away from intervening as the default.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Notice that in designing the loss matrix, we are trying to capture domain knowledge about the &lt;em&gt;deployment&lt;/em&gt; of the model – for example, the loss attached to doing nothing (when there is equivocation about the negative/positive state of the patient) pushes us to be cautious and intervene.&lt;/p&gt;
&lt;p&gt;Let’s look at the resulting BEL:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y$P, loss.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;knitr::kable( rho.A, 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE, 
              position = &amp;#39;c&amp;#39;,
              escape = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
A
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
rho
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.35125
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
a2
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
-0.30530
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print( min.bel.CBD &amp;lt;- which.max( rho.A$rho ) )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The action that minimises &lt;span class=&#34;math inline&#34;&gt;\(\rho(\pi_{\Theta},a)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; – as can be seen, the probability mass for &lt;span class=&#34;math inline&#34;&gt;\(\theta_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_3\)&lt;/span&gt; (and the associated losses) is driving the decision to intervene i.e. be cautious.&lt;/p&gt;
&lt;p&gt;We can continue introducing more and more granularity in quantising the posterior predictions &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; to arrive at mappings to states &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; and then specifying individual losses in the corresponding rows of the loss matrix. Instead, we’ll specify a &lt;strong&gt;loss function&lt;/strong&gt; (although for coding convenience, we’ll continue with a matrix representation).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Sigmoid &amp;lt;- function( x, A, B, m, s ) {
  # x = vector of values
  # A = height of sigmoid
  # B = translation on y axis
  # m = value of x for which Sigmoid() = half max value
  # s = steepness of linear component
  exp.x &amp;lt;- exp( -(x-m)/s )
  return(
    ( A + B * (1+exp.x) ) / (1+exp.x)
  )
}


# plots to compare the quantised states to a more fine-grained version
pdf.Y1 &amp;lt;- pdfFromSamples(0, 1, delta= 1/50, samples)
pdf.plot.Y1 &amp;lt;- ggplot( pdf.Y1, aes( x = mids, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

pdf.Y2 &amp;lt;- pdfFromSamples(0, 1, delta= 1/3, samples)
pdf.Y2$Theta &amp;lt;- factor(c(&amp;quot;theta1&amp;quot;,&amp;quot;theta2&amp;quot;,&amp;quot;theta3&amp;quot;))
pdf.plot.Y2 &amp;lt;- ggplot( pdf.Y2, aes( x = Theta, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  scale_x_discrete(labels = pdf.Y2$theta ) +
  xlab(TeX(&amp;quot;$\\theta&amp;quot;)) +
  ylab(TeX(&amp;#39;$\\pi(\\theta)&amp;#39;)) +
  basictheme

# loss functions for a1 and a2
loss.fun.a1 &amp;lt;- Sigmoid(pdf.Y1$mids, A = -1.0, B = 0, m = 0.5, s = 0.15 )
loss.fun.a2 &amp;lt;- Sigmoid(pdf.Y1$mids, A = 0.5, B = -0.5, m = 0.3, s = 0.08 )

# build a tabular version of loss function
loss.fun &amp;lt;- data.frame( Theta = pdf.Y1$mids,
                        L.a1 = loss.fun.a1,
                        L.a2 = loss.fun.a2
                        )

# show the loss function and 3 state quantised loss matrix
loss.fun.plot &amp;lt;- ggplot( loss.fun, aes( x = Theta ) ) +
  geom_line( aes( y = L.a1 ), colour = &amp;quot;#fc8d59&amp;quot;, size = 1) +
  annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.75, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( y = L.a2 ), colour = &amp;quot;#91bfdb&amp;quot;, size = 1 ) + 
  annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.15, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) +
  basictheme

df.loss.matrix &amp;lt;- data.frame( Theta = factor( c(&amp;quot;theta1&amp;quot;,&amp;quot;theta2&amp;quot;,&amp;quot;theta3&amp;quot;) ),
                              L.a1 = loss.matrix[,1],
                              L.a2 = loss.matrix[,2]
                            )

loss.matrix.plot &amp;lt;- ggplot( df.loss.matrix ) +
  geom_line( aes( x = Theta, y = L.a1, group = 1), 
             colour = &amp;quot;#fc8d59&amp;quot;, size = 1) +
  geom_point( aes( x = Theta, y = L.a1, group = 1), 
             colour = &amp;quot;#fc8d59&amp;quot;, size = 4) +
  annotate( &amp;quot;label&amp;quot;, 
            x = 2.8, y = -0.7, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( x = Theta, y = L.a2, group= 1), 
             colour = &amp;quot;#91bfdb&amp;quot;, size = 1.5 ) + 
  geom_point( aes( x = Theta, y = L.a2, group= 1), 
             colour = &amp;quot;#91bfdb&amp;quot;, size = 4 ) + 
  annotate( &amp;quot;label&amp;quot;, 
            x = 2.8, y = -0.2, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) +
  basictheme

grid.arrange( pdf.plot.Y2, pdf.plot.Y1, 
              loss.matrix.plot, loss.fun.plot, 
              nrow = 2, ncol = 2 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;90%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, we show the three-state loss matrix underneath the distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; (the lines are to emphasise the trend in losses as we proceed from likely negative through positive). On the right, a finer-grained representation of the distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta} \approx \pi_{F}(Y|x)\)&lt;/span&gt; with a sigmoid loss function over &lt;span class=&#34;math inline&#34;&gt;\(\Theta\)&lt;/span&gt; interpolating between the points in the loss matrix at the extremes (negative, positive) and midpoint (equivocal). Now, we can effectively use the whole of the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi_{F}(Y|x)\)&lt;/span&gt; more directly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the loss function x probability
pdf.Y1$L.a1 &amp;lt;- pdf.Y1$P * loss.fun$L.a1
pdf.Y1$L.a2 &amp;lt;- pdf.Y1$P * loss.fun$L.a2

# product of the posterior and loss function
loss.fun.plot2 &amp;lt;- ggplot( pdf.Y1, aes( x = mids ) ) +
  geom_line( aes( y = L.a1 ), colour = &amp;quot;#fc8d59&amp;quot;, size = 1.5) +
  #annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.75, label = &amp;quot;a1&amp;quot; ) + 
  geom_line( aes( y = L.a2 ), colour = &amp;quot;#91bfdb&amp;quot;, size = 1.5 ) + 
  #annotate( &amp;quot;label&amp;quot;, x = 0.9, y = -0.15, label = &amp;quot;a2&amp;quot; ) +
  ylab(&amp;quot;Loss&amp;quot;) +
  xlab(&amp;quot;\nTheta&amp;quot;) + 
  basictheme

## The actual BEL
# we need a matrix representation of the loss function
loss.fun.matrix &amp;lt;- as.matrix( loss.fun[,2:3] )
colnames( loss.fun.matrix ) &amp;lt;- c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)

# compute BEL for each action a:
rho.A &amp;lt;- data.frame( 
  A = factor(c(&amp;quot;a1&amp;quot;,&amp;quot;a2&amp;quot;)),
  rho = rep(NA,2)
)

# for each action
for( j in 1:2 ) {
  rho.A$rho[j] &amp;lt;- BEL( j, pdf.Y1$P, loss.fun.matrix )
}

bel.plot &amp;lt;- ggplot( rho.A, aes(x = A, y = rho) ) +
  geom_col( fill = &amp;quot;#d6604d&amp;quot; ) + 
  basictheme

grid.arrange( pdf.plot.Y1, loss.fun.plot2, bel.plot, ncol = 2, nrow = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/decisions-and-loss-functions/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the top-left panel shows a finer-grained distribution function &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta}\)&lt;/span&gt; and the top-right panel shows the loss function for &lt;span class=&#34;math inline&#34;&gt;\(a_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_2\)&lt;/span&gt; weighted by &lt;span class=&#34;math inline&#34;&gt;\(\pi_{\Theta} \approx \pi_{F}(Y|x)\)&lt;/span&gt; – rather than the &lt;strong&gt;sum&lt;/strong&gt; for each action as in &lt;span class=&#34;math inline&#34;&gt;\(\rho(\theta, a)\)&lt;/span&gt;. This exposes that the Bayesian expected loss of an action is the integral over states (equivalently, the sum for discrete distributions) of the product of the loss function for an action in a certain state and the probability of that state. The bottom-left panel shows the resulting BEL where, as expected, &lt;span class=&#34;math inline&#34;&gt;\(a2\)&lt;/span&gt; minimises &lt;span class=&#34;math inline&#34;&gt;\(\rho(\theta,a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-further-reading&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Further Reading&lt;/h1&gt;
&lt;p&gt;If I were to try this again (rather than trying to piece together an understanding from wikipedia), I would proceed in this order:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start with &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-savage1951theory&#34; role=&#34;doc-biblioref&#34;&gt;Savage 1951&lt;/a&gt;)&lt;/span&gt; for foundations/first principles and tutorial approach.&lt;/li&gt;
&lt;li&gt;First four chapters of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-berger1985statistical&#34; role=&#34;doc-biblioref&#34;&gt;Berger 1985&lt;/a&gt;)&lt;/span&gt; for a really clear exposition of the core ideas.&lt;/li&gt;
&lt;li&gt;For decision theory in point estimation from the perspective of sciences concerned with &lt;strong&gt;prediction&lt;/strong&gt; and &lt;strong&gt;forecasting&lt;/strong&gt;: &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-gneiting2011making&#34; role=&#34;doc-biblioref&#34;&gt;Gneiting 2011&lt;/a&gt;)&lt;/span&gt; provides a comprehensive review&lt;/li&gt;
&lt;li&gt;Risk/decision theory for &lt;strong&gt;classification&lt;/strong&gt; Chapter 2 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-duda2012pattern&#34; role=&#34;doc-biblioref&#34;&gt;Duda, Hart, and Stork 2012&lt;/a&gt;)&lt;/span&gt; and Chapter 1.5 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Foundations in Bayesian principles more generally: Chapter 2 and Appendix B of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bernardo2009bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Bernardo and Smith 2009&lt;/a&gt;)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-berger1985statistical&#34; class=&#34;csl-entry&#34;&gt;
Berger, James O. 1985. &lt;em&gt;Statistical Decision Theory and Bayesian Analysis&lt;/em&gt;. 2nd ed. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-bernardo2009bayesian&#34; class=&#34;csl-entry&#34;&gt;
Bernardo, José M, and Adrian FM Smith. 2009. &lt;em&gt;Bayesian Theory&lt;/em&gt;. Vol. 405. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-duda2012pattern&#34; class=&#34;csl-entry&#34;&gt;
Duda, Richard O, Peter E Hart, and David G Stork. 2012. &lt;em&gt;Pattern Classification&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-gneiting2011making&#34; class=&#34;csl-entry&#34;&gt;
Gneiting, Tilmann. 2011. &lt;span&gt;“Making and Evaluating Point Forecasts.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 106 (494): 746–62.
&lt;/div&gt;
&lt;div id=&#34;ref-savage1951theory&#34; class=&#34;csl-entry&#34;&gt;
Savage, Leonard J. 1951. &lt;span&gt;“The Theory of Statistical Decision.”&lt;/span&gt; &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 46 (253): 55–67.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Loss, Risk and Point Summaries of Posterior Distributions</title>
      <link>/post/loss-functions-and-posteriors/</link>
      <pubDate>Sat, 21 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/loss-functions-and-posteriors/</guid>
      <description>
&lt;script src=&#34;/post/loss-functions-and-posteriors/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/loss-functions-and-posteriors/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/loss-functions-and-posteriors/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;For a while, I’ve been thinking about the deployment of predictive algorithms in clinical decision support. Specifically, about the difference between what we understand about a model’s performance from the publication describing it and how this might be less informative when deployed. In short: what is the value of knowing that a model has good balanced accuracy or a high area under the ROC curve when sat with a patient and using the tool to make a clinical decision.&lt;/p&gt;
&lt;p&gt;Frequently in medical applications of machine learning we see summary measures of performance used to demonstrate the competence of the model typically reported as sensitivities, specificities, balanced accuracy, area under the ROC curve and so on. These measures assume that for an input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; (a patient) we get an output &lt;span class=&#34;math inline&#34;&gt;\(Y = F(x)\)&lt;/span&gt; representing for example, “negative” or “positive” caseness.&lt;/p&gt;
&lt;p&gt;If the system &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; is designed to &lt;strong&gt;classify&lt;/strong&gt; patients, then it will deliver discrete ‘yes / no’ answers e.g. &lt;span class=&#34;math inline&#34;&gt;\(Y \in \{0,1\}\)&lt;/span&gt;; canonical examples include the &lt;a href=&#34;https://en.wikipedia.org/wiki/Support-vector_machine&#34;&gt;support-vector machine (SVM)&lt;/a&gt;. Baked-into these algorithms is a &lt;strong&gt;decision rule&lt;/strong&gt; operating over a continuous value; in SVMs, for example, the decision rule classifies each patient by dichotomising the signed distance of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; from the class-seperating hyperplane. If using a &lt;strong&gt;classifier&lt;/strong&gt; makes sense in the context of it’s clinical deployment you might want the algorithm to report a definitive dichotomised answer and summary measures like sensitivity and specificity (perhaps) make sense.&lt;/p&gt;
&lt;p&gt;Compare this with &lt;a href=&#34;https://en.wikipedia.org/wiki/Logistic_regression&#34;&gt;logistic regression&lt;/a&gt; (often used as if it were a classifier) where the purpose is to estimate the &lt;strong&gt;probability of an event&lt;/strong&gt;, i.e. that a patient is positive or negative. Often in these cases, the decision rule is ‘bolted on’ and that’s when people invoke ROC curves and compute accuracies at an operating threshold that maximises the trade-off between sensitivity and specificity e.g. by &lt;a href=&#34;https://en.wikipedia.org/wiki/Youden%27s_J_statistic&#34;&gt;maximising Youden’s J-statistic&lt;/a&gt;. It’s here that the decision rule and it’s deployment context matter.&lt;/p&gt;
&lt;p&gt;It seems to me that patients and clinicians would probably want more information than a pure classifier provides and most likely would prefer to know the &lt;strong&gt;actual&lt;/strong&gt; continuous score – the “output”&#34; of &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Further, a patient and clinician might want to understand the decision rule and it’s assumptions. For example, assume for a deployed predictive model &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; and a new patient, &lt;span class=&#34;math inline&#34;&gt;\(F(x) = 0.79\)&lt;/span&gt; and this represents (or is proportional to) the likelihood of being a positive case. The predictive model being deployed has an operating threshold of &lt;span class=&#34;math inline&#34;&gt;\(0.80\)&lt;/span&gt; for declaring a positive case and this threshold was determined by maximising the trade-off between sensitivity and specificity. Recall that sensitivity is the ratio &lt;span class=&#34;math inline&#34;&gt;\(TP/(TP+FN)\)&lt;/span&gt; and specificity is the ratio &lt;span class=&#34;math inline&#34;&gt;\(TN/(TN+FP)\)&lt;/span&gt;. Amongst other things, I’d want to know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;in determining the operating threshold, were true positives and false negatives given equal weight ? For example, in the context of predicting a rare but serious event, a false negative can be orders-of-magnitude more ‘costly’ than the model correctly determining cases that are true positives or true negatives.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;in the example above, &lt;span class=&#34;math inline&#34;&gt;\(F(x) = 0.79\)&lt;/span&gt; is 0.01 below the operating threshold for declaring a &lt;em&gt;positive&lt;/em&gt; case – boundary cases near the operating threshold, demand closer inspection and at least, consideration of the &lt;strong&gt;uncertainty&lt;/strong&gt; in the output of &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A more elaborate version of this discussion is &lt;a href=&#34;https://github.com/danwjoyce/summ-performance/blob/master/revised_predictive_decisions.pdf&#34;&gt;here&lt;/a&gt; and summarised in a &lt;a href=&#34;https://jamanetwork.com/journals/jamapsychiatry/article-abstract/2758828&#34;&gt;short paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, the problem of ‘declaring’ a prediction can be cast as a problem in statistical &lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_theory&#34;&gt;decision theory&lt;/a&gt;. Being only vaguely familiar with similar ideas from signal detection models in psychophysics &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-green1966signal&#34; role=&#34;doc-biblioref&#34;&gt;Green and Swets 1966&lt;/a&gt;)&lt;/span&gt;, I decided to deep-dive into the details of using posterior distributions to arrive at decisions via loss and risk functions. I found the ideas are fairly intuitive and are well described e.g. in &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-duda2012pattern&#34; role=&#34;doc-biblioref&#34;&gt;Duda, Hart, and Stork 2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;. It was harder for me to operationalise these; for example, I frequently found explainations that the &lt;span class=&#34;math inline&#34;&gt;\(L_0\)&lt;/span&gt; “zero-one” loss function delivers the mode of a (posterior) distribution, but examples of this in action were harder to come by.&lt;/p&gt;
&lt;div id=&#34;sec-predictions&#34; class=&#34;section level1&#34; number=&#34;1&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;1&lt;/span&gt; Predictions&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;prediction-classification.png&#34; width=&#34;474&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As above, assume we have a model &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; that delivers a continuous score &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; for an outcome/case/event given some “input” measurement or feature(s), &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; for a patient. Importantly, the model delivers outputs in the form of posterior probabilities &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x) = \Pr(Y=y|x)\)&lt;/span&gt;; for example, the probability of being a positive case is &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=1|x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0|x) = 1 - \pi(Y=1|x)\)&lt;/span&gt; being the probability of a negative case.&lt;/p&gt;
&lt;p&gt;So for any patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, we will have a posterior distribution – rather than a single &lt;em&gt;point&lt;/em&gt; summary.&lt;/p&gt;
&lt;p&gt;This is an important distinction: after inferring (learning) a model that delivers posterior probabilities, we can then &lt;em&gt;deliberately&lt;/em&gt; design and implement a decision process – in contrast to solving the related problem of &lt;strong&gt;discrimination&lt;/strong&gt; or &lt;strong&gt;classification&lt;/strong&gt;, where we find a direct mapping from each input &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; to a discrete, often binary, output and sacrifice an estimate of uncertainty (see Ch. 1.5 of &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt; for more detail).&lt;/p&gt;
&lt;p&gt;We are used to seeing &lt;em&gt;point estimates&lt;/em&gt; as outputs from predictive models given some input e.g. &lt;span class=&#34;math inline&#34;&gt;\(\pi( Y = 1 | x) = 0.78\)&lt;/span&gt; and sometimes, with a measure of uncertainty on that output (for example, the standard error on predicted values from &lt;a href=&#34;https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.glm.html&#34;&gt;&lt;code&gt;predict.glm()&lt;/code&gt;&lt;/a&gt; in &lt;code&gt;R&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;This point value is a summary of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Probability_distribution&#34;&gt;posterior distribution&lt;/a&gt; of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; (for example, the mean) and represents the output of a decision making process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34; number=&#34;2&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;2&lt;/span&gt; Setup&lt;/h1&gt;
&lt;p&gt;Assume that for some predictive model, we present a single patient &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and we are able to access the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; i.e. we can obtain samples from the posterior distribution for that patient denoted &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;; for example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to ensure reproducible result
set.seed(3141)
# simulate a bimodal posterior distribution $\pi(Y|x)$
samples &amp;lt;- range01( c(rnorm( 1000, mean = 0, sd = 1 ), rnorm( 1000, mean = 5, sd = 2) ) )
df &amp;lt;- data.frame( y = samples )
ggplot( df, aes( y ) ) +
  geom_density( fill = &amp;quot;#fa9fb5&amp;quot;) + 
  ylab(&amp;quot;Density\n&amp;quot;) + 
  xlab(&amp;quot;\nScore (Y)&amp;quot;) + basictheme&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have 2000 samples from &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; stored as &lt;code&gt;samples&lt;/code&gt; (usually, these samples will be from the &lt;a href=&#34;https://en.wikipedia.org/wiki/Posterior_predictive_distribution&#34;&gt;posterior predictive distribution&lt;/a&gt; obtained by e.g. MCMC sampling):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;round( samples[1:10], 3 )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 0.291 0.130 0.271 0.295 0.294 0.373 0.261 0.232 0.177 0.313&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and the following code will give us a &lt;code&gt;data.frame&lt;/code&gt; that represents an approximation to the probability distribution function as a lookup table (basically, a histogram):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdfFromSamples &amp;lt;- function(a, b, delta, samples) {
  H &amp;lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) )
  ret &amp;lt;- data.frame(
    mids  = H$mids,
    freq  = H$counts
  )
  ret$P &amp;lt;- ret$freq / sum(ret$freq)
  return(ret)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, we can examine &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; in the region &lt;span class=&#34;math inline&#34;&gt;\(Y \in [0.5,0.6]\)&lt;/span&gt; with a bin-width of &lt;code&gt;delta = 1/50&lt;/code&gt; as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0, 1, delta = 1/50, samples )
knitr::kable( pdf.Y[ pdf.Y$mids &amp;gt;= 0.5 &amp;amp; pdf.Y$mids &amp;lt;= 0.6, ], 
              format = &amp;quot;html&amp;quot;, 
              align = &amp;#39;c&amp;#39;, 
              full_width = FALSE,
              row.names = FALSE ) %&amp;gt;%
        kable_styling(position = &amp;quot;center&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
mids
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
freq
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
P
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.51
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0285
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.53
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
46
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0230
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.55
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0285
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
62
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0310
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.59
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
48
&lt;/td&gt;
&lt;td style=&#34;text-align:center;&#34;&gt;
0.0240
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With some abuse of notation, we can state that (approximately) &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0.55|x) = 0.0285\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y=0.59|x) = 0.0240\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-loss&#34; class=&#34;section level1&#34; number=&#34;3&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;3&lt;/span&gt; Loss Functions&lt;/h1&gt;
&lt;p&gt;Consider the task of choosing a summary of the information contained in the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; as a single (point) value. We can see that the score &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; ranges from 0 to 1 and we could potentially pick any one of an infinite number of values as our chosen point summary (of course, some will be meaningful and others less so).&lt;/p&gt;
&lt;p&gt;To make this concrete, we’ll cheat and look-ahead to the answer. One meaningful decision to summarise &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt; is to choose the &lt;strong&gt;mode&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y &amp;lt;- pdfFromSamples(0, 1, delta = 1/100, samples )

mode.pdf.Y &amp;lt;- pdf.Y$mids[ order( pdf.Y$freq, decreasing = TRUE) ][1]

pdf.plot &amp;lt;- ggplot( pdf.Y, aes( x = mids, y = P ) ) + 
  geom_col( fill = &amp;quot;#fa9fb5&amp;quot; ) + 
  xlab(&amp;quot;Score (Y)&amp;quot;) +
  ylab(TeX(&amp;#39;$\\pi(Y|x)&amp;#39;)) +
  basictheme

mode.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = mode.pdf.Y, colour = &amp;quot;black&amp;quot;, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Mode = &amp;quot;, mode.pdf.Y ), 
            x = mode.pdf.Y, y = 0.01)

print( mode.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now assume we don’t have this information but instead, let &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; be one such candidate decision for the point summary. The actual point summary is &lt;span class=&#34;math inline&#34;&gt;\(y_{j} \in [0,1]\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We define the cost of choosing &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; – where the true value is &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; – as the &lt;strong&gt;loss function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L(a_{i},y_{j})\)&lt;/span&gt;. Now, construct a loss function that penalizes any candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; by a single unit if &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; is not equal to &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; (incorrect) and zero if &lt;span class=&#34;math inline&#34;&gt;\(a_i\)&lt;/span&gt; is equal to &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;; this is the so-called “zero-one” loss function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_{0}(a_{i},y_{j}) = \begin{cases}
          0 &amp;amp; \text{if}\ a_{i} = y_{j} \\
          1 &amp;amp; \text{otherwise}
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now, we propose an arbitrary candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{1} = 0.555\)&lt;/span&gt; and we want to know the associated loss over the range of possible values of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the range of possible values $y_{j}$
y &amp;lt;- pdf.Y$mids
# our estimate $a_{i}$
a1 &amp;lt;- 0.555

# the L_{0} loss function:
loss0 &amp;lt;- function( y, a ) { ifelse( a == y, 0, 1 ) }

# the loss function evaluated over the range $y_{j}$
l0.ex &amp;lt;- data.frame( mids = pdf.Y$mids, loss.ex1 = loss0( y, a1 ) )

loss.plot &amp;lt;- ggplot( l0.ex, aes( x = l0.ex$mids, y = loss.ex1) ) +
  geom_line( colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$y_{j}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$L_{0}$&amp;#39;)) +
  basictheme

print( loss.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Let’s repeat the same process for two other (arbitrarily chosen) candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{2} = 0.095\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(a_{3} = 0.755\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a2 &amp;lt;- 0.095
a3 &amp;lt;- 0.755

# the loss function evaluated over the range $y_{j}$
l0.ex$loss.ex2 &amp;lt;- loss0( y, a2 )
l0.ex$loss.ex3 &amp;lt;- loss0( y, a3 )

loss.plot &amp;lt;- ggplot( l0.ex ) +
  geom_line( aes( x = mids, y = loss.ex1 ), colour = &amp;quot;#bdbdbd&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{1}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a1, y = 0.25) +
  geom_line( aes( x = mids, y = loss.ex2 ), colour = &amp;quot;#969696&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{2}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a2, y = 0.25) +
  geom_line( aes( x = mids, y = loss.ex3 ), colour = &amp;quot;#636363&amp;quot; ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
                label = TeX(&amp;#39;$a_{3}$&amp;#39;, output=&amp;quot;character&amp;quot;),
                parse = TRUE,
                x = a3, y = 0.25) +
  xlab(TeX(&amp;#39;$y_{j}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$L_{0}$&amp;#39;)) +
  basictheme

print( loss.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly using the loss function we get ‘spikes’ when our candidates coincide with a value in the range of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But, what we care about is loss associated with a candidate with respect to the probability of each possible value &lt;span class=&#34;math inline&#34;&gt;\(Y=y_j\)&lt;/span&gt;; so we weight the loss associated with each “decision” (each candidate &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;) by the posterior probability of &lt;span class=&#34;math inline&#34;&gt;\(Y = y_{j}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_0(a_{i},y_{j})\pi(y_{j}|x)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This leads us to consider the &lt;strong&gt;risk&lt;/strong&gt; associated with each candidate. In essence, we want to know the loss associated with &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; when the true value is &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; weighted by how likely or how frequently we see &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-risk-functions&#34; class=&#34;section level1&#34; number=&#34;4&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;4&lt;/span&gt; Risk Functions&lt;/h1&gt;
&lt;p&gt;We could continue randomly choosing candidates but instead, we’ll be systematic and check all values of &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;. At the same time, we’ll shift representation and instead of plotting the loss function for each candidate against the range of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt;, we’ll compute a &lt;strong&gt;risk function&lt;/strong&gt; for each candidate as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Expected_value&#34;&gt;expected value&lt;/a&gt; of the loss function evaluated for each &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbb{E}[L_0(a_{i},y_{j})] = \sum_{j} \underbrace{L_{0}(a_{i},y_{j})}_\text{loss} \underbrace{\pi(y_{j}|x)}_\text{posterior}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, instead of &lt;span class=&#34;math inline&#34;&gt;\(y_{j}\)&lt;/span&gt; on the horizontal axis and &lt;span class=&#34;math inline&#34;&gt;\(L_{0}\)&lt;/span&gt; on the vertical, we instead show candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; on the horizontal with the risk &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[L]\)&lt;/span&gt; on the vertical:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all candidate values for $a_{i}$ 
a &amp;lt;- pdf.Y$mids
# the range of values for $y_{j}$
y &amp;lt;- pdf.Y$mids

pdf.Y$risk.L0 &amp;lt;- rep(NA, nrow(pdf.Y))

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  # with reference to the above equation
  pdf.Y$risk.L0[i] &amp;lt;- sum(                     # sum over j
                        loss0(y, a[i]) *       # loss
                        pdf.Y$P                # posterior
                      ) 
}

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L0  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{0})$&amp;#39;)) +
  basictheme

print(risk.plot)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sec-optimal-decision&#34; class=&#34;section level1&#34; number=&#34;5&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;5&lt;/span&gt; Optimal Decision&lt;/h1&gt;
&lt;p&gt;Our question is now: what is the best &lt;strong&gt;action&lt;/strong&gt; – or decision – over our candidates &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; to choose as the point summary given the loss function &lt;span class=&#34;math inline&#34;&gt;\(L_{0}\)&lt;/span&gt; and the posterior &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;The answer is, the &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; that &lt;strong&gt;minimises&lt;/strong&gt; the risk (expected loss). Implementing this, we arrive at:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# find the $a_{i}$ that minimises the risk function
min.risk.L0 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L0 ) ]

risk.plot &amp;lt;- risk.plot + 
    geom_vline( xintercept = min.risk.L0, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Minimum = &amp;quot;, min.risk.L0 ), 
            x = mode.pdf.Y, y = 0.99)
print( risk.plot )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
Compare with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;grid.arrange(risk.plot, mode.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The result then, can be summarised as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Under the &lt;strong&gt;zero-one loss function&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(L_0\)&lt;/span&gt;, the action/decision &lt;span class=&#34;math inline&#34;&gt;\(a_{i}\)&lt;/span&gt; (point summary) which minimises the risk function (expected loss) is the &lt;strong&gt;mode&lt;/strong&gt; of the posterior distribution &lt;span class=&#34;math inline&#34;&gt;\(\pi(Y|x)\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;other-loss-functions&#34; class=&#34;section level1&#34; number=&#34;6&#34;&gt;
&lt;h1&gt;&lt;span class=&#34;header-section-number&#34;&gt;6&lt;/span&gt; Other Loss Functions&lt;/h1&gt;
&lt;p&gt;We can repeat the same process as for section &lt;a href=&#34;#sec-loss&#34;&gt;3&lt;/a&gt; through &lt;a href=&#34;#sec-optimal-decision&#34;&gt;5&lt;/a&gt; with different loss functions.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;linear loss&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(L_{1}\)&lt;/span&gt; loss is defined as:
&lt;span class=&#34;math display&#34;&gt;\[
L_{1}(a_{i},y_{j}) = \begin{cases}
          c_{1} |a_{i} - y_{j}| &amp;amp; \text{ if } a_{i} \leq y_{j} \\
          c_{2} |a_{i} - y_{j}| &amp;amp; \text{ if } a_{i} &amp;gt; y_{j} \\
    \end{cases}
\]&lt;/span&gt;
Where &lt;span class=&#34;math inline&#34;&gt;\(c_{1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(c_{2}\)&lt;/span&gt; are constants. If &lt;span class=&#34;math inline&#34;&gt;\(c_{1} = c_{2}\)&lt;/span&gt; we arrive at the &lt;strong&gt;median&lt;/strong&gt; of the posterior, as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the linear loss function
loss1 &amp;lt;- function( y, a, c1, c2 ) {
  ifelse(
    a &amp;lt;= y, c1 * abs( a - y ),
            c2 * abs( a - y )
  )
}

pdf.Y$risk.L1 &amp;lt;- rep(NA, nrow(pdf.Y))

# set constants equal
c1 &amp;lt;- c2 &amp;lt;- 1

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L1[i] &amp;lt;- sum( 
                        loss1(y, a[i], c1, c2) * 
                        pdf.Y$P
                      ) 
}

# find the minimum
min.lossL &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1 ) ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The minimum of the risk function is 0.355:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L1  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{1})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL, colour = &amp;quot;black&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL, 2) ), 
              x = min.lossL, y = 0.4) +
  basictheme

median.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = median( samples ), colour = &amp;quot;black&amp;quot;, size = 2 ) +
      annotate( geom = &amp;quot;label&amp;quot;, 
            label = paste0( &amp;quot;Median = &amp;quot;, round( median(samples), 2 ) ), 
            x = median(samples), y = 0.01)

grid.arrange(risk.plot, median.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;More generally, for positive constants, the &lt;span class=&#34;math inline&#34;&gt;\(c_{1}/(c_{1}+c_{2})\)&lt;/span&gt; quantile of the posterior distribution can be found. For example, we can obtain the 25th and 75th percentiles:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pdf.Y$risk.L1_75 &amp;lt;- rep(NA, nrow(pdf.Y))
pdf.Y$risk.L1_25 &amp;lt;- rep(NA, nrow(pdf.Y))

# set constants
q95 &amp;lt;- 0.75
q05 &amp;lt;- 0.25
c1 &amp;lt;- 1
c2.q95 &amp;lt;- (c1/q95) - c1
c2.q05 &amp;lt;- (c1/q05) - c1

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L1_75[i] &amp;lt;- sum( 
                            loss1(y, a[i], c1, c2.q95) * 
                            pdf.Y$P
                         ) 
  pdf.Y$risk.L1_25[i] &amp;lt;- sum( 
                            loss1(y, a[i], c1, c2.q05) * 
                            pdf.Y$P
                         ) 
}

# find the minima
min.lossL_75 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_75 ) ]
min.lossL_25 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_25 ) ]

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L1_75  ), colour = &amp;quot;#1f78b4&amp;quot; ) +
  geom_line( aes( x = mids, y = risk.L1_25  ), colour = &amp;quot;#33a02c&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{1})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL_75, colour = &amp;quot;#1f78b4&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL_75, 3) ), 
              x = min.lossL_75, y = 1.25) +
  
  geom_vline( xintercept = min.lossL_25, colour = &amp;quot;#33a02c&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL_25, 3) ), 
              x = min.lossL_25, y = 1.25) +
  basictheme

fun.q75 &amp;lt;- as.numeric( round( quantile(samples, probs = c(0.75)), 3 ) )
fun.q25 &amp;lt;- as.numeric( round( quantile(samples, probs = c(0.25)), 3 ) )

quantile.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = fun.q75, colour = &amp;quot;#1f78b4&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;75th = &amp;quot;, fun.q75 ), 
              x = fun.q75, y = 0.01) +
      geom_vline( xintercept = fun.q25, colour = &amp;quot;#33a02c&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;25th = &amp;quot;, fun.q25 ), 
              x = fun.q25, y = 0.01)

grid.arrange(risk.plot, quantile.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the top panel of the figure above, the green and blue lines represents the risk using the linear loss function with constants configured to locate the 25th and 75th percentile respectively. The bottom panel shows the same result obtained directly from the &lt;code&gt;quantile()&lt;/code&gt; function operating directly on the raw &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To complete the loss functions for common measures of central tendency, we have the &lt;span class=&#34;math inline&#34;&gt;\(L_{2}\)&lt;/span&gt; &lt;strong&gt;quadratic&lt;/strong&gt; loss function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
L_{2}(a_{i},y_{j}) = (a_{i} - y_{j})^2
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s the result for quadratic loss:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the quadratic loss function
loss2 &amp;lt;- function( y, a ) {
  return( 
    (a - y)^2 
  )
}

pdf.Y$risk.L2 &amp;lt;- rep(NA, nrow(pdf.Y))

for( i in 1:length( a ) ) {
  # compute risk function at each candidate $a_{i}$
  pdf.Y$risk.L2[i] &amp;lt;- sum( 
                        loss2(y, a[i]) * 
                        pdf.Y$P
                      ) 
}

# find the minimum
min.lossL2 &amp;lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L2 ) ]

risk.plot &amp;lt;- ggplot( pdf.Y ) +
  geom_line( aes( x = mids, y = risk.L2  ), colour = &amp;quot;#636363&amp;quot; ) +
  xlab(TeX(&amp;#39;$a_{i}$&amp;#39;)) +
  ylab(TeX(&amp;#39;$E(L_{2})$&amp;#39;)) +
  geom_vline( xintercept = min.lossL2, colour = &amp;quot;black&amp;quot;, size = 2 ) +
    annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Minimum = &amp;quot;, round( min.lossL2,3) ), 
              x = min.lossL2, y = 0.25) +
  basictheme

mean.plot &amp;lt;- pdf.plot + 
      geom_vline( xintercept = mean( samples ), colour = &amp;quot;black&amp;quot;, size = 2 ) +
        annotate( geom = &amp;quot;label&amp;quot;, 
              label = paste0( &amp;quot;Mean = &amp;quot;, round( mean( samples ), 3 ) ), 
              x = mean(samples), y = 0.005)

grid.arrange(risk.plot, mean.plot, nrow = 2, ncol = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/loss-functions-and-posteriors/index_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;70%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The estimate from &lt;code&gt;quantile()&lt;/code&gt; plotted on the bottom panel differs from the top panel (the mean estimated by minimising the risk function) due to the granularity (&lt;code&gt;delta = 1/100&lt;/code&gt;) of the approximation of the distribution function used in the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-duda2012pattern&#34; class=&#34;csl-entry&#34;&gt;
Duda, Richard O, Peter E Hart, and David G Stork. 2012. &lt;em&gt;Pattern Classification&lt;/em&gt;. John Wiley &amp;amp; Sons.
&lt;/div&gt;
&lt;div id=&#34;ref-green1966signal&#34; class=&#34;csl-entry&#34;&gt;
Green, David Marvin, and John A Swets. 1966. &lt;em&gt;Signal Detection Theory and Psychophysics&lt;/em&gt;. Wiley New York.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
