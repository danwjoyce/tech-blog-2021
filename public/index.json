[{"authors":null,"categories":null,"content":"This site is for hosting technical content that won\u0026rsquo;t work on my home site.\nI am a consultant psychiatrist and Professor of Connected Mental Health at the Institute of Population Health, University of Liverpool, where I work on using data to better inform how treatments and interventions can be targeted to, or made more effective for, individuals. I am particularly interested in methods for delivering \u0026ldquo;actionable data\u0026rdquo; \u0026ndash; for example, modeling clinical state and trajectories \u0026ndash; that directly informs interventions/management and how to use this data build decision support tools. This work draws on principles from computation, applied multivariate statistics and artificial intelligence / machine learning. While I\u0026rsquo;m generally optimistic about the potential of these approaches, I\u0026rsquo;m more cautious about the current AI/ML hype.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/dan-w-joyce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dan-w-joyce/","section":"authors","summary":"This site is for hosting technical content that won\u0026rsquo;t work on my home site.\nI am a consultant psychiatrist and Professor of Connected Mental Health at the Institute of Population Health, University of Liverpool, where I work on using data to better inform how treatments and interventions can be targeted to, or made more effective for, individuals.","tags":null,"title":"Dan W Joyce","type":"authors"},{"authors":null,"categories":null,"content":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"da99cb196019cc5857b9b3e950397ca9","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"Âê≥ÊÅ©ÈÅî is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Âê≥ÊÅ©ÈÅî","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor Dan W Joyce FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"17a31b92253d299002593b7491eedeea","permalink":"/courses/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/python/","section":"courses","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"1b341b3479c8c6b1f807553b77e21b7c","permalink":"/courses/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/visualization/","section":"courses","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"6f4078728d71b1b791d39f218bf2bdb1","permalink":"/courses/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/courses/example/stats/","section":"courses","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":["psychiatry","data science","AI"],"content":" My colleague Lia Ali pointed me to this ‚Äì at this year‚Äôs Royal College of Psychiatrist‚Äôs International Congress, there‚Äôs a debate titled Clinic appointment with Skynet? This house believes that the RCPsych should embrace Artificial Intelligence and Big Data in guiding clinical decision making and service development. Regrettably, I‚Äôm going to miss the Congress so won‚Äôt be able to attend this exciting panel discussion, but here‚Äôs some thoughts.\nFair advance warning, this is a highly opinionated and somewhat personal/anecdotal piece so you know the risks ‚Ä¶\nFirst of all ‚Äì in my view ‚Äì the two topics (Big Data and AI) would benefit from being debated separately, especially as two application domains are mentioned (clinical decision making and service development). And I‚Äôll explain why as we go. I wonder if the two topics (AI and Big Data) are often headlined together because there‚Äôs an assumption that AI techniques ‚Äì almost always, inductive learning algorithms ‚Äì often require vast datasets. But not all inductive methods required huge amounts of data (more on this later). Further, as I‚Äôll elaborate, I think there is a tendency to see AI as some computational alchemy which turns variable quality (but voluminous) data into robust inferences and insights you couldn‚Äôt possibly have obtained using mature, established disciplines like statistics and epidemiology. My view is that intellectually, AI (however you conceive it or divide up the many sub-fields) inherits from the traditions of formal (rather than empirical) sciences and engineering. In this regard, given evidence-based medicine (EBM) inherits more from empirical science my view is that statistics, biostatistics and epidemiology as sanity-checks for the direction of travel and outputs of AI, machine learning and engineering-based treatments of clinical data.\n1 Reputation Risk AI is a broad concept and is used today (in my view) vaguely and without qualification ‚Äì almost as if everyone implicitly knows what the term means. But let‚Äôs be clear ‚Äì the term carries substantial historical baggage and investing blindly in ‚ÄúAI will solve this problem‚Äù is risky because if we fail, the cost in terms of resource, enthusiasm (often, to be read as ‚Äúhype‚Äù), energy and reputation will be substantial. For this reason, I was left wanting by the Topol review ‚Äì look at pp.¬†5 of the Topol review for mental health to see why.\nAn anecdote: when I was writing up my Ph.D.¬†in early 2000, my supervisor (one of the most helpful, tolerant and supportive academics I‚Äôve ever met) told me bluntly ‚ÄúDo not juxtapose the words artificial and intelligence in your thesis title ‚Ä¶ No one will examine it‚Äù.\nAnd this was precisely because at that time, there was a history of at least two ‚Äúwaves‚Äù of AI that had failed; so-called AI winters ‚Äì and as I discuss shortly, skepticism about the status of current machine learning methods.\nDebatably, the first AI winter resulted from the failures of what became know in the 1980s-1990s as connectionism, or, the period of neural networks research dating back to the McCulloch-Pitts model neuron, early learning algorithms such as the Widrow-Hoff rule and Frank Rosenblatt‚Äôs perceptron that culimated in 1969, when Minsky and Papert‚Äôs book ‚ÄúPerceptrons‚Äù showed that certain kinds of neural networks can‚Äôt learn a fundemantal logical operator, the exclusive OR (XOR) function. Like most stories, there‚Äôs subtlety that get‚Äôs ignored but ultimately, in the UK at least, the 1973 Lighthill report certainly kaiboshed the field. Neural networks (at least, the feed-forward, function approximating kind most like modern deep networks) came alive again in the 1980s, primarily because it was discovered that if you add more layers of model McCulloch-Pitts style neurons between the input and output and combine this architecture with an algorithm to adjust the parameters between model neurons (the back-propogation rule) suddenly, you could do much more. Notice something here: a way out of a difficult slump is to a) build a bigger network and b) innovate on learning algorithms that can optimise and estimate the model‚Äôs parameters to make it perform. Sound familiar ?\nMore anecdotal data: toward the end of the 1990s, where I was working, the enthusiasm for neural networks ‚Äì invigorated again in 1987 by the publication of the two-volume book ‚ÄúParallel Distributed Processing: Explorations in the Microstructure of Cognition‚Äù by Rumelhart, McClelland and the PDP Research Group ‚Äì was waning because\n it could be shown that multilayer perceptrons (the pre-cursor to today‚Äôs contemporary deep learning networks) essentially implemented multivariable (and in some cases, multivariate) regression there was a Bayesian formulation of multilayer perceptrons and their training by the back-propogation method (nowadays, autodiff) which offered a solid mathematical framework and dispelled some of the mythology around these techniques (that is to say, the story was ‚Äúthey don‚Äôt do anything special but are another way of implementing regression‚Äù) additionally, where I was working, techniques like support vector machines (SVMs) and decision trees (such as Breiman‚Äôs CART method) could perform as well (if not better) on classification tasks. And many people where excited about fuzzy logic.  Alongside this, ‚Äúclassical‚Äù AI ‚Äì based on symbolic reasoning, rather than the numerical, probabilistic and statistical methods of neural networks ‚Äì had been shown to be brittle and in some cases, computationally intractable; examples include the frame problem and symbolic search methods for planning actions. Expert systems ‚Äì the 1980s version of engineering applications of symbolic AI to problems such as decision support in medicine ‚Äì didn‚Äôt get very far either.\nAnyway ‚Äì in the 1990s and 2000s researchers attempted to position themseleves outside the umbrella of AI because in essence, it was a toxic brand and this is because there had been too many promises that failed to deliver in the previous hype-cycles of the 1960s, 70s, 80s and 1990s.\n 2 AI and the NHS Anecdotally, when speaking with colleagues across industry, academia and the NHS, I‚Äôve heard the following:\n‚ÄúReal AI‚Äù is contemporary neural networks (by implication, anything else is not real AI) ‚Äì this is just machismo and patently non-sense. I‚Äôve heard it said that the ‚ÄúNHS doesn‚Äôt do real AI‚Äù because it doesn‚Äôt deploy large, sophisticated neural networks. Let‚Äôs assume that ‚Äúreal AI‚Äù is equated specifically with the branch of machine learning that studies large neural networks and then let‚Äôs cherry pick one impressive achievement ‚Äì beating a human at Go. By some estimates, DeepMind spent around $35 million training a system to play Go. Take a look at the paper, and you‚Äôll see that they used either 48 CPUs and 8 GPUs, or for the distributed version, 1,202 CPUs and 176 GPUs. And it tooks 3 weeks of compute time to train.  You might reasonably ask ‚ÄúSo what‚Äù. But if I want to buy cloud-based resource to implement and run my neural network-based solution to some problem, I might go to AWS and cost an on-demand p4d.24xlarge instance with 8 GPUs and I‚Äôm going to drive this thing hard for training and then use in production for inference ‚Äì that‚Äôs over 23,000 USD (or ¬£18,800) per month if I want it switched on 24/7 or if I buy a reserved instance, I can get this down to just over 8,000 USD (¬£6500) per month if I purchase for three years. To buy my own single GPU (of a similar specification to those in the AWS instance described) will set me back about ¬£9,000 ignoring the costs of a host server, storage and maintenance costs for running the rig.\nSo, if we‚Äôre going to equate ‚Äúreal AI‚Äù with neural networks, the NHS better be sure it‚Äôs going to be worth it.\nContemporary large, deep neural networks can locate non-linear and long-range interactions (say, between variables) in a way that standard statistical techniques cannot ‚Äì I‚Äôve heard this said often and the architectures of modern neural networks are certainly configurable for this to be the case ‚Ä¶ but I‚Äôve not seen this advantage demonstrated in applications (alluded to, certainly, but not demonstrated).  This is one of those alchemy-like principles which I wonder may be based on an analogy with some actual theory. Back in the day (1989) Cybenko published a universal approximation theorem which showed multilayer perceptrons could approximate any real-valued mathematical function to an arbitrary degree of accuracy. Similar theorems have been developed for modern deep networks. These essentially tell us that neural networks can capture very sophisticated and complex mappings from inputs to outputs but they these theorems don‚Äôt deliver a specification for the actual, realisable network (that is, the inputs, hidden layers, parameterisation and so on). I don‚Äôt know for sure, but perhaps using these ideas (universal approximation) analogically ‚Äì combined with the representational sophistication afforded by very large networks ‚Äì provide us with hope that these long-range interactions, dependencies and correlations (say, in very long time series) might be captured or exploited.\nThere seems to be an implicit belief that if one throws a big enough neural network at a problem, it can magically divine signal from noise and ‚Äúadd value‚Äù in ways that traditional inductive inference (e.g.¬†statistical methods) cannot. It‚Äôs seems (to me at least) that there‚Äôs a tacit belief that low-fidelity data can be transformed by contemporary AI without critically looking at the source data and the desired application.  I think this last point is particularly pernicious. Some neural network architectures can do amazing things, but they tend to be in domains where ‚Äì from the design of these neural networks ‚Äì you‚Äôd predict that they should work. For example, convolutional neural networks in radiology and imaging tasks. But still, they‚Äôve been shown to learn aberrant mappings which are kinds of ‚Äúshort-cuts‚Äù from inputs to desired outputs that bear no resemblance to clinical reasoning; roughly, correlations between image features and classification task that turn out to be unlike any information a skilled radiologist would use.\nI‚Äôll finish this section with this thought: it‚Äôs easy to be a nay-sayer and dump on other‚Äôs work. But my point is simply that we do require more critical thinking that focuses on how data that is available mesh with robust and reliable methods that might be reasonably combined to produce useful results. And not just blindly expecting AI (however conceived) will ‚Äújust solve the problem‚Äù ‚Äì which as described above, didn‚Äôt get us very far.\nAlso, I‚Äôd add that we should be more prosaic with our language ‚Äì in some applications of AI or machine learning (ML) we are really just saying ‚ÄúWe used methods derived from computational sciences and engineering to solve a problem‚Äù. A great example is when we use regularised regression to combine an element of feature selection with learning the probability of some outcome alongside internal validation techniques like cross-validation, resampling etc. If you‚Äôre going to call (essentially) regression ‚ÄúAI‚Äù then where do you stop ? You might as well decide that linear algebra is basically AI.\n 3 2. Decision Support and Service Development Helping clinicians and patients make a decision is very different from service development (again, in my opinion). The latter is (I think) about changing how we deliver care in response to demand, capacity, shifts in medical technology and the demographics of the people that a healthcare system serves. The former is about sitting alongside a patient, their data and helping decide on management, establishing a diagnosis and so on.\nSomewhat over-simplistically, one could say that service development requires understanding e.g.¬†data about the locality/population characteristics, audits of the operational aspects of services and their delivery to targets or clinical guidelines and methods for marrying the two with a view to changing how care is delivered. Certainly, this requires lots of data and perhaps, opportunistic re-use of existing routinely-collected data. And statistical process control (SPC) is described as being widely used in the NHS for understanding how changes in services deliver differences in outcomes.\nBut I doubt this is the same data required when sitting alongside a patient, trying to arrive at a shared decision using decision support tools ‚Äì the QRISK cardiovascular risk score being an example. It could be the case that if a service collects high-fidelity, granular data about patients that this same data could be reused at the population/service level, but I think the two things remain very different.\nAnd this leads us to the next point: what data and methods you need, or use, depends on detailed understanding of the task at hand.\n 4 Big Is Not Necessarily Better When I trained in medicine, we got well-intentioned, but rudimentary, training on statistical methods under the guise of the evidence-based medicine (EBM) curriculum. As part of this, we learn many examples of where the precision of some estimate increases as the sample size, \\(n\\) increases. For example, the standard error of the mean is \\(\\sigma/\\sqrt{n}\\) where \\(\\sigma\\) is the sample standard deviation. Almost everywhere you look, you find \\(n\\) in the denominator and this leads us to internalise the idea that ‚Äúbigger \\(n\\) means more precision‚Äù and we should trust large studies over small studies.\nAnecdotally, I‚Äôve been involved in a few studies where ‚Äì in spectacular examples of terrible research practices ‚Äì the team has decided to collect ‚Äújust a few more samples‚Äù, to increase \\(n\\) and shrink the standard error of some inferential estimates and even worse, to shrink uncertainty estimates to the point where it ‚Äúreaches statistical significance‚Äù. That‚Äôs a whole other debate.\nBut of course, it‚Äôs never that easy. Achieving more precision requires better measurement not just larger \\(n\\) and psychiatry (alongside biological sciences and medicine more generally) has noisy and imprecise measurements.\n4.1 Noise and Measurement Error An example, lifted from Loken and Gelman‚Äôs 2017 paper ‚ÄúMeasurement error and the replication crisis‚Äù: If I tell you I can run a mile in 5 minutes, that‚Äôs impressive. If I inform you that I did this with a rucksack loaded with bricks, then you‚Äôd likely conclude I would have been even faster without the weight slowing me down. By analogy: I perform a study involving \\(n=100\\) people, collecting the kind of data that allows me to estimate the odds ratio of having a disorder given some factors I want to control for and including a categorical variable of interest, representing membership of some tentative risk group (e.g.¬†natal sex, membership of a certain ethnic group and so on). I estimate the odds ratio for the categorical variable of interest at 1.2, but my uncertainty is wide and the odds ratio might be between 0.7 and 5.7. I write this up, stating we can‚Äôt be conclusive about the contribution of the categorical risk variable‚Äôs effect because the uncertainty interval [0.7, 5.7] includes 1.0. In my ‚Äúfuture work, discussion and limitations‚Äù section, I write that ‚Äúfuture work should include larger sample sizes‚Äù because (as above) I expect that my effect would be larger or more visible if it weren‚Äôt for the noise, and further, I can overcome the noise ‚Äì increasing the precision of my estimate ‚Äì by increasing \\(n\\).\nLoken and Gelman show that: if you take two variables, \\(x\\) and \\(y\\) with an actual ‚Äúground truth‚Äù small effect of \\(x\\) on \\(y\\) and then consider two scenarios where the observed measurements have either low or high measurement error (i.e.¬†we add either a small or a large amount of noise) then:\n with large \\(n\\) and a truly modest effect size (i.e.¬†the correlation between \\(x\\) and \\(y\\) is in reality small) ‚Äì adding measurement error almost certainly reduces the observed correlation, making the effect less visible. Analogously, running a mile in 5 minutes but with a heavy rucksack. with small \\(n\\) and a truly modest effect size ‚Äì adding measurement error can result in the observed correlation being larger, making the effect artificially more ‚Äúvisible‚Äù to us. This represents the analogy and assumption we would have run faster (effect size/correlation would be larger) if we weren‚Äôt burdened by the rucksack (measurement error)  This tells us that when we draw a conclusion from small samples, we should not rely on the idea that the impact of measurement error is obscuring what would have been a more impressive effect size.\n 4.2 Big Data by Analogy with Big Surveys Now, I hear you: isn‚Äôt this why we need Big Data? But let‚Äôs consider the Big Data Paradox ‚Äì introduced in Xiao-Li Meng‚Äôs 2018 paper ‚ÄúStatistical Paradises and Paradoxes in Big Data‚Äù in the context of population surveys. Meng‚Äôs work shows that while increasing the sample size may reduce estimates of uncertainty, e.g.¬†shrink confidence intervals for some statistic, it can also magnify bias in the statistic; consequently, we end up being very confident in summary statistics which are completely off-the-mark.\nMeng considers a finite sample \\(n\\) of some population \\(N\\) and focuses on the difference between some sample statistic (say, the average) \\(\\bar{G}_n\\) and the ‚Äútrue‚Äù population value of the same, \\(\\bar{G}_N\\).\n for some sample \\(n\\), the difference (error) between the sample and population average is \\(\\bar{G}_n - \\bar{G}_N\\) ‚Äì how much we can trust the sample average \\(\\bar{G}_n\\) depends on this difference/error The error \\(\\bar{G}_n - \\bar{G}_N\\) can be ‚Äúdecomposed‚Äù into a formula consisting of three terms  The three terms are:\na data quality measure ‚Äì or the data defect correlation measure ‚Äì which captures total bias as the correlation between the event that an individual‚Äôs data was recorded in the sample (a function of the sampling method) and the actual value recorded; in a high-quality sample, the values recorded should bear no relationship to whether or not the individual was included in the sample. Hence, the value of the data defect correlation will be low and ideally, close to zero.  A concrete example: In one electronic health record (EHR) study I was involved in, patient‚Äôs with long histories (and therefore, having larger volumes of clinical data in the EHR) were absent from the sample for purely technical reasons that we hadn‚Äôt spotted at the time (FYI, there was a bug in the software that just ‚Äúdumped‚Äù patients from the sample if they had lots of historical notes). Assume we‚Äôre studying recurrent depression and using EHR samples to estimate the mean number of episodes as a function of age. In this case, the data defect correlation will be high because the event that the individual data was ‚Äòcaptured‚Äô in any sample would strongly correlate with both age and how many episodes they‚Äôd experienced (older people are more likely to have experienced more episodes and therefore have longer records).\na data quantity measure ‚Äì expressed as a function of \\(\\sqrt{(N-n)/n}\\) so that if the sample is the entire population (complete data) \\(n = N\\) and this expression becomes zero (i.e.¬†contributes nothing to the error).  Note that the data quantity is defined such that the size of the sample \\(n\\) alone is not so important, rather, it‚Äôs the sample size as a proportion of the population size \\(N\\) that contributes to the overall error \\(\\bar{G}_n - \\bar{G}_N\\)\na problem difficulty measure ‚Äì expressed as a function of the standard deviation of the values measured in the population, which is to say, the more heterogenous the population values are, the harder it is to estimate a robust average \\(\\bar{G}_n\\)  Narratively, Meng is asking (see pp.¬†687) ‚ÄúWhich one should we trust more, a 5% survey sample or an 80% administrative dataset?‚Äù or alternatively, ‚ÄúShould we invest in a small but well-designed, robust sample or large, routinely collected data from electronic health records, population surveillance system or a big online social-media opportunistic sample?‚Äù. The headline lesson of Meng‚Äôs paper is the Big Data Paradox: ‚ÄúThe more the data, the surer we fool ourselves‚Äù.\nA great example of the Big Data Paradox using Meng‚Äôs decomposition is Bradley et al‚Äôs (2021) paper ‚ÄúUnrepresentative big surveys significantly overestimated US vaccine uptake‚Äù. In this paper, they estimated the components above for three different survey designs of Covid vaccine uptake in the US. They had access to the actual number of vaccines delivered (i.e.¬†the ‚Äútrue‚Äù population values) and could look at how biased the estimates were for the three surveys (samples). One of the surveys was a robustly designed, probabilistic sample of the population, but was relatively small at 995 people. Another was an opportunistically sampled survey from active social media users (with 181,949 people) and another was based on census addresses for which there was a mobile phone or email contact available consisting of 76,068 people.\nThe results are interesting. Bradley et al showed how the two very large surveys (relying more on opportunistic sampling) had high data defect correlations and the estimated average vaccine uptake differed systematically from the actual vaccinations administered. The small \\(n=995\\), higher quality-sampled survey was, however, much closer to the population average.\nAnd the headline in Bradley et al‚Äôs paper is that a survey of 250,000 people (with properties similar to the two large surveys) produces estimates of the population‚Äôs mean that are no more accurate than a truly random sample of just 10 (yes, ten) people.\n  5 Conclusion The points I‚Äôm trying to articulate here are:\nWe need more precision in how we describe our methods ‚Äì ‚ÄúAI‚Äù obscures what we mean, especially in the age of hype and over-promise. Sometimes, a more prosaic description of our methods might be less attention-grabbing but perhaps, more honest ‚Äì so let‚Äôs see more papers saying ‚ÄúWe used resampling methods for internal validation, on a modest data set, estimating a linear model‚Äôs parameters using maximum-likelihood‚Äù rather than assuming we need to call this AI to persuade readers of the value of what was undertaken. And let‚Äôs not assume contemporary AI has to be equated with the current trend for deep learning / massive neural networks Large neural network-models with millions/billions of parameters and sophisticated architectures are expensive to train and deploy ‚Äì so we better be sure that they‚Äôre worth the investment Knowing the history of previous cycles of AI (and the subsequent ‚ÄúAI winters‚Äù) and understanding where and why they failed (which domains, applications and so on) is important; spectacular results enabled by increases in ‚Äúraw‚Äù computational power and resources shouldn‚Äôt persuade us that everything is a breakthrough or a game-changer.\n Big Data is not always better data ‚Äì sure, size matters, but we should understand and attend to the intended use (or re-use) of any large data set; some opportunistically acquired data sets might be useful for some tasks, but give unreliable results for other tasks. For example, how we use routinely collected data for service development might radically differ from using the same data for individual-level clinical decision support  I‚Äôm aware that this blog covered none of the profound ethical or sociotechnical aspects, but that‚Äôs for another time.\n ","date":1655510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655503206,"objectID":"b0bce374ddd0abc27720d5b7291bd4b0","permalink":"/post/big-data-ai/","publishdate":"2022-06-18T00:00:00Z","relpermalink":"/post/big-data-ai/","section":"post","summary":"My colleague Lia Ali pointed me to this ‚Äì at this year‚Äôs Royal College of Psychiatrist‚Äôs International Congress, there‚Äôs a debate titled Clinic appointment with Skynet? This house believes that the RCPsych should embrace Artificial Intelligence and Big Data in guiding clinical decision making and service development.","tags":["computation","data"],"title":"Big Data, AI and Psychiatry","type":"post"},{"authors":["Dan W Joyce","Andrey Kormilitzin","Julia Hamer-Hunt","Anthony James","Alejo Nevado-Holgado","Andrea Cipriani"],"categories":null,"content":"","date":1638403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638403200,"objectID":"df0a22d89bf0b8cd74be8f313a69d3c1","permalink":"/publication/joyce-2021-chronosig/","publishdate":"2021-12-02T00:00:00Z","relpermalink":"/publication/joyce-2021-chronosig/","section":"publication","summary":"Accessing specialist secondary mental health care in the NHS in England requires a referral, usually from primary or acute care. Community mental health teams triage these referrals deciding on the most appropriate team to meet patients‚Äô needs. Referrals require resource-intensive review by clinicians and often, collation and review of the patient‚Äôs history with services captured in their electronic health records (EHR). Triage processes are, however, opaque and often result in patients not receiving appropriate and timely access to care that is a particular concern for some minority and under-represented groups. Our project, funded by the National Institute of Health Research (NIHR) will develop a clinical decision support tool (CDST) to deliver accurate, explainable and justified triage recommendations to assist clinicians and expedite access to secondary mental health care","tags":null,"title":"CHRONOSIG: Digital Triage for Secondary Mental Healthcare using Natural Language Processing - rationale and protocol","type":"publication"},{"authors":["dwj"],"categories":["computation","maths"],"content":"  1 Purpose 2 Posterior Predictive Distributions  2.1 Conditional Probability 2.2 Law of Total Probability 2.3 Conditional Independence 2.4 Bayes Theorem 2.5 Bayes with Extra Conditioning 2.6 Deriving the PPD  3 Sampling and Integration 4 Logits, Probabilities and Odds  4.1 Odds 4.2 Odds Ratios 4.3 Logistic Regression  References   1 Purpose This is a living, annotated bibliography of stuff I need to use on a semi-regular basis, always have to look up but in my chaotic file system, can never find. It also documents some embarrassing truths ‚Äì stuff like ‚ÄúIs variance the square root of standard deviation, or the other way round?‚Äù ‚Ä¶ Of course, it‚Äôs the other way round.\nSo, point number one: If \\(\\sigma_X\\) is the standard deviation of \\(X\\) then: \\[ \\mathrm{Var}(X) = \\sigma^2_X \\]\n 2 Posterior Predictive Distributions I spent a week trying to find a derivation of equation 1.4 on pp.7 of (Gelman et al. 2014), the posterior predictive distribution (Rubin 1984) of new data \\(\\tilde{y}\\) given previously observed data \\(y\\) and a model with parameters \\(\\theta\\) \\[\\begin{equation} p( \\tilde{y} | y ) = \\int p\\left( \\tilde{y} | \\theta \\right) p\\left( \\theta | y \\right) d\\theta \\tag{2.1} \\label{eqn:finalPPD} \\end{equation}\\]\nThere are explanations floating around on the internet, but none I could follow because they skipped steps and left me confused.\nWe need a few basic laws and definitions from probability theory as follows (Blitzstein and Hwang 2019):\n2.1 Conditional Probability For two variables \\(a\\) and \\(b\\): \\[\\begin{equation} p(a | b) = \\frac{p(a,b)}{p(b)} \\tag{2.2} \\end{equation}\\]\nOr re-arranged: \\[\\begin{equation} p(a,b) = p(a | b) p(b) \\tag{2.3} \\end{equation}\\]\nAnd for two variables \\(a\\) and \\(b\\), conditioned on \\(c\\): \\[\\begin{equation} p(a,b | c) = \\frac{p(a,b,c)}{p(c)} \\tag{2.4} \\end{equation}\\]\nand the re-arrangement: \\[\\begin{equation} p(a,b,c) = p(a,b | c) p(c) \\end{equation}\\]\nNote that \\(p(a,b,c)\\) can also be factorised as any of the following (depending on what we want to achieve): \\[\\begin{align} p(a,b,c) \u0026amp;= p(b,c|a) p(a) \\tag{2.5} \\\\ \u0026amp;= p(a,c|b) p(b) \\tag{2.6} \\\\ \u0026amp;= p(a,b|c) p(c) \\tag{2.7} \\end{align}\\]\n 2.2 Law of Total Probability From the joint probability \\(p(a,b)\\), the marginal \\(p(a)\\) is: \\[\\begin{equation} p(a) = \\int p \\left( a, b \\right) db \\end{equation}\\]\nAnd the continuous law of total probability is (Blitzstein and Hwang 2019) pp.¬†289: \\[\\begin{align} p(a) \u0026amp;= \\int p \\left( a,b \\right) db \\\\ \u0026amp;= \\int p \\left( a|b \\right) p( b )db \\tag{2.8} \\end{align}\\]\nwhere we‚Äôve used equation (2.3) to re-write \\(p \\left( a,b \\right)\\) as \\(p \\left( a|b \\right) p( b )\\)\nAdding conditioning on \\(c\\) we obtain (Blitzstein and Hwang 2019) pp.¬†54: \\[\\begin{align} \\label{eqn:lotp_cont} p \\left( a|c \\right) \u0026amp;= \\int p \\left( a,b | c \\right) db \\\\ \u0026amp;= \\int p \\left( a|b,c \\right) p( b | c ) db \\tag{2.9} \\end{align}\\]\n 2.3 Conditional Independence Two variables \\(a\\) and \\(c\\) are conditionally independent given a third variable \\(b\\) (Blitzstein and Hwang 2019) pp.¬†58: \\[\\begin{equation} ( a \\perp\\!\\!\\!\\perp c ) | b \\iff p(a,c | b) = p( a | b ) p( c | b) \\tag{2.10} \\end{equation}\\]\n 2.4 Bayes Theorem For two variables \\(a\\) and \\(b\\): \\[\\begin{equation} p(a|b) = \\frac{p(b|a)p(a)}{p(b)} \\tag{2.11} \\end{equation}\\]\n 2.5 Bayes with Extra Conditioning I frequently have to remind myself how to rewrite this form: \\(p(a | b, c)\\) ‚Äì this is covered in (Blitzstein and Hwang 2019) Theorem 2.4.2 on pp.¬†54-56.\nThere are a few useful ways to re-write.\nUsing Conditional Probability\nAssume it makes sense (in the problem we‚Äôre trying to solve) to view \\(b\\) and \\(c\\) as ‚Äúone thing together‚Äù then using the formula for conditional probability we get: \\[\\begin{equation} p( a | b, c ) = \\frac{p(a,b,c)}{p(b,c)} \\tag{2.12} \\end{equation}\\]\nApplying conditional probability again ‚Äì equation (2.4) and the different factorisations in (2.5) through (2.7) ‚Äì to rewrite \\(p(a,b,c)\\): \\[\\begin{align} p(a,b,c) \u0026amp;= p(b,c|a) p(a) \\\\ \u0026amp;= p(a,c|b) p(b) \\\\ \u0026amp;= p(a,b|c) p(c) \\end{align}\\]\nChoosing the RHS as suited to the problem - here, we take \\(p(b,c|a) p(a)\\) as we are treating \\(b\\) and \\(c\\) as ‚Äúone event‚Äù (we want to keep them together) and substitute in (2.12): \\[\\begin{equation} p( a | b, c ) = \\frac{ p(b,c|a) p(a) }{ p( b,c ) } \\tag{2.13} \\end{equation}\\]\nNow, we have to find an expression for the denominator \\(p( b,c )\\) and we have options including another application of conditional probability so \\(p( b, c ) = p(b|c) p(c)\\) resulting in: \\[\\begin{equation} p( a | b, c ) = \\frac{ p(b,c|a) p(a) }{ p(b|c) p(c) } \\tag{2.13} \\end{equation}\\]\nUsing the Chain Rule of Probability\nStart, as before, with: \\[\\begin{equation} p( a | b, c ) = \\frac{p(a,b,c)}{p(b,c)} \\tag{2.14} \\end{equation}\\]\nThis time, decompose \\(p(a,b,c)\\) differently, using the chain rule: \\[\\begin{align} p(a,b,c) \u0026amp;= p(a|b,c)p(b,c) \\\\ \u0026amp;= p(b|a,c)p(a,c) \\\\ \u0026amp;= p(c|a,b)p(a,b) \\end{align}\\] Obviously, the first re-write gets us nowhere ‚Äì it merely restates \\(p(a|b,c)\\). Lets say the second factorisation is helpful for our problem \\(p(b|a,c)p(a,c)\\), then we substitute in (2.14):\n\\[\\begin{equation} p( a | b, c ) = \\frac{p(b|a,c)p(a,c)}{p(b,c)} \\tag{2.15} \\end{equation}\\]\nWe now have to deal with \\(p(a,c)\\) in the numerator and \\(p(b,c)\\) in the denominator. Starting with the numerator, apply conditional probability again: \\[\\begin{equation} p( a, c ) = p(a|c)p(c) \\end{equation}\\]\nFor the denominator: \\[\\begin{equation} p( b, c ) = p(b|c)p(c) \\end{equation}\\]\nWhat we end up with is Bayes theorem with extra conditioning on \\(c\\) ‚Ä¶\nBayes Theorem with Extra Conditioning on \\(c\\)\nSubstitute both into (2.15): \\[\\begin{align} p( a | b, c ) \u0026amp;= \\frac{p(b|a,c) p(a|c)p(c)} {p(b|c)p(c)} \\\\ \u0026amp;= \\frac{p(b|a,c) p(a|c)} {p(b|c)} \\end{align}\\]\nBayes Theorem with Extra Conditioning on \\(b\\)\nHad we chosen, instead, to use \\(p(c|a,b)p(a,b)\\), we end up with: \\[\\begin{align} p( a | b, c ) \u0026amp;= \\frac{p(a,b,c)}{p(b,c)} \\\\ \u0026amp;= \\frac{p(c|a,b)p(a,b)}{p(b,c)} \\end{align}\\]\nwith \\(p(a,b) = p(a|b)p(b)\\): \\[\\begin{equation} p( a | b, c ) = \\frac{p(c|a,b)p(a|b)p(b)}{p(b,c)} \\end{equation}\\]\nOf course, \\(p(b,c) = p(c,b) = p(c|b)p(b)\\) in the denominator: \\[\\begin{align} p( a | b, c ) \u0026amp;= \\frac{p(c|a,b)p(a|b)p(b)}{p(c|b)p(b)} \\\\ \u0026amp;= \\frac{p(c|a,b)p(a|b)}{p(c|b)} \\end{align}\\]\nWe chose between conditioning on \\(c\\) or \\(b\\) depending on the problem we are trying to solve (i.e.¬†does it make sense to consider everything being conditioned on \\(c\\) or \\(b\\) ?)\n 2.6 Deriving the PPD Starting with the fundamental Bayesian modelling framework:\nbefore observing the data, \\(y\\), the prior distribution of the parameters is \\(p(\\theta)\\) we have a sampling distribution of the data given parameters \\(p(y|\\theta)\\) the joint distribution of \\(\\theta\\) and \\(y\\) is \\(p(\\theta, y) = p(\\theta)p(y|\\theta)\\) we then obtain the posterior distribution of the parameters of the model given the observed data:  \\[\\begin{equation} p(\\theta|y) = \\frac{p(\\theta,y)}{p(y)} = \\frac{p(\\theta)p(y|\\theta)}{p(y)} \\end{equation}\\]\nSo, we can think of the posterior distribution \\(p(\\theta|y)\\) as the `output‚Äô of Bayesian model estimation.\nStep 1\nWe want to obtain a distribution for future values \\(\\tilde{y}\\) given the observed (and modelled) data \\(y\\) which is \\(p(\\tilde{y}|y)\\) using what we know about the posterior distribution arising from parameter estimation \\(p(\\theta | y)\\).\nThe first step is to write \\(p(\\tilde{y}|y)\\) using the law of total probability (with conditioning): equation (2.9): \\[\\begin{equation} p( \\tilde{y} | y ) = \\int p\\left(\\tilde{y},\\theta | y \\right) d\\theta \\tag{2.16} \\end{equation}\\]\nStep 2\nRe-write the integrand \\(p\\left(\\tilde{y},\\theta | y \\right)\\) using equation (2.4):\n\\[\\begin{equation} p( \\tilde{y} | y ) = \\int \\frac{p(\\tilde{y},\\theta,y)}{p(y)} d\\theta \\tag{2.17} \\end{equation}\\]\nStep 3\nWe assert that \\(\\tilde{y}\\) is conditionally independent of \\(y\\) given the model parameters \\(\\theta\\): \\[\\begin{equation} ( \\tilde{y} \\perp\\!\\!\\!\\perp y ) | \\theta \\iff p( \\tilde{y}, y | \\theta) = p( \\tilde{y} | \\theta ) p( y | \\theta) \\tag{2.18} \\end{equation}\\]\nTo make use of the conditional independence \\(p( \\tilde{y}, y | \\theta)\\), we have to factorise \\(p(\\tilde{y},\\theta,y)\\) in equation (2.17).\nLet \\(a = \\tilde{y}\\), \\(b = \\theta\\) and \\(c = y\\); we are seeking a factorisation of \\(p(a,b,c)\\) as \\(p(a,c|b)\\) and inspecting the factorisations (2.5) through (2.7) we find that (2.6) matches:\n\\[\\begin{align} p(a,b,c) \u0026amp;= p(a,c|b) p(b) \\\\ p( \\tilde{y}, \\theta, y) \u0026amp;= p( \\tilde{y}, y | \\theta) p(\\theta) \\tag{2.19} \\end{align}\\]\nSubstitution (2.19) into (2.17) we have:\n\\[\\begin{equation} p(\\tilde{y} | y ) = \\int \\frac{p(\\tilde{y},y | \\theta) p(\\theta)}{p(y)} d \\theta \\tag{2.20} \\end{equation}\\]\nWe make use of the equality from equation (2.18) i.e.¬†that \\(p( \\tilde{y}, y | \\theta) = p( \\tilde{y} | \\theta ) p( y | \\theta)\\) and substitute into (2.20):\n\\[\\begin{equation} p(\\tilde{y} | y ) = \\int \\frac{ p( \\tilde{y} | \\theta ) p( y | \\theta) p(\\theta)}{p(y)} d \\theta \\tag{2.21} \\end{equation}\\]\nStep 4\nRecall that we want to make use of the ‚Äòoutput‚Äô of parameter estimation, the posterior distribution of the model parameters given the observed data \\(p(\\theta|y)\\), and in equation (2.21) we see the term \\(p(y|\\theta)\\). All we need to do is re-write \\(p(y|\\theta)\\) using Bayes rule, equation (2.11):\n\\[\\begin{equation} p(y|\\theta) = \\frac{p(\\theta|y) p(y)}{p(\\theta)} \\end{equation}\\]\nAnd substitute into equation (2.21): \\[\\begin{align} p(\\tilde{y} | y ) \u0026amp;= \\int \\frac{ p( \\tilde{y} | \\theta ) p(\\theta|y) p(y) p(\\theta)}{p(y)p(\\theta)} d \\theta \\\\ \u0026amp;= \\int p( \\tilde{y} | \\theta ) p(\\theta|y) d \\theta \\tag{2.22} \\\\ \\end{align}\\]\n‚Ä¶ and there we have it, the expression for the PPD, equation (2.1)\n  3 Sampling and Integration This one came from Chad Scherrer who posted a tweet about the relationship between integration and sampling and I thought this was a really helpful heuristic.\nWhen I work through examples of Bayesian problems, my first thought is ‚Äúhow will I code this?‚Äù and Chad‚Äôs tweet comes to mind, and helpfully, it follows nicely from the posterior predictive distribution example.\nTo paraphrase Chad‚Äôs tweet:\n To sample from \\(p(y|x)p(x)\\) ‚Ä¶ Sample \\(x\\) Use the sample of \\(x\\) to sample \\(y\\) Return \\((x,y)\\)  To sample from \\(\\int p(y|x)p(x) dx\\) ‚Ä¶ Sample \\(x\\) Use the sample of \\(x\\) to sample \\(y\\) Discard \\(x\\) Return \\(y\\)   In practice then: we want to obtain samples from the posterior predictive distribution \\(p(\\tilde{y}|y)\\) i.e.¬†for some new or unseen \\(\\tilde{x}\\).\n\\[\\begin{equation} p(\\tilde{y} | y ) = \\int p( \\tilde{y} | \\theta ) p(\\theta|y) d \\theta \\end{equation}\\]\nHere‚Äôs how to proceed. We‚Äôve used some Bayesian parameter estimation method (e.g.¬†MCMC or similar) to obtain samples from \\(p(\\theta|y)\\) and stored them as \\(\\Theta_S\\)\nWe have a function that returns a value \\(\\tilde{y}\\) given an input \\(\\tilde{x}\\) and parameters \\(\\theta\\) such that \\(\\tilde{y} = f(\\tilde{x}; \\theta)\\)\nFor each sample \\(\\theta^{s} \\in \\Theta_S\\) ‚Äì one sample from \\(p(\\theta|y)\\)\nCompute \\(\\tilde{y}^s = f(\\tilde{x}; \\theta^{s})\\) ‚Äì a sample from \\(p(\\tilde{y}|\\theta)\\)\n Store \\(\\tilde{y}^s\\) and throw away \\(\\theta^{s}\\)\n  Our resulting collection of \\(\\tilde{y}^s\\) are samples from the PPD from which we can then take expected values, quantiles etc.\n 4 Logits, Probabilities and Odds Can never remember these relations, so wrote them down explicitly for future reference.\n4.1 Odds If \\(p_A\\) is the probability of event \\(A\\) then:\n\\(\\textrm{odds}_A = \\frac{1}{(1-p_A)}\\) given the odds, we recover the probability as \\(p_A = \\frac{\\textrm{odds}_A}{1+\\textrm{odds}_A}\\) the log odds are given by \\(\\ln(\\textrm{odds}_A) = \\ln \\left( \\frac{1}{1-p_A} \\right) = \\textrm{logit}(p_A) = \\ln(p_A) - \\ln(1-p_A)\\)  So, odds of 1.0 equals a probability \\(p_A = 0.5\\) ‚Äì the probability of the event occurring is at chance level.\n 4.2 Odds Ratios For two events, \\(A\\) and \\(B\\) with probabilities \\(p_A\\) and \\(p_B\\):\n\\(\\textrm{odds}_A = \\frac{1}{(1-p_A)}\\) and \\(\\textrm{odds}_B = \\frac{1}{(1-p_B)}\\) the odds ratio \\(\\textrm{OR}_{AB} = \\frac{\\textrm{odds}_A}{\\textrm{odds}_B} = \\frac{1-p_B}{1-p_A}\\)   4.3 Logistic Regression In logistic regression applied to clinical situations, we are usually interested in a single ‚Äúthing‚Äù associated with two discrete and mutually-exclusive events (e.g.¬†\\(A\\) = ‚Äúdying‚Äù or \\(B\\) = ‚Äúnot dying‚Äù) ‚Äì the thing either occurs or it does not occur. In these circumstances \\(p_B = 1 - p_A\\).\nTo convert to the language of regression, denote an outcome \\(y\\) (for example, death, experiencing a side effect, obtaining a positive response to a treatment) then:\nthe probability that \\(y\\) occurred is \\(p_y\\) (equivalent to \\(p_A\\) in the example above) the corresponding probability \\(y\\) does not occur (\\(¬¨y\\)) is \\(1-p_y\\) (equivalent to \\(p_B\\) in the example above) the odds of \\(y\\) occurring are \\(\\textrm{odds}_{y} = \\frac{1}{(1-p_y)}\\) and the odds of \\(¬¨y\\) are \\(\\textrm{odds}_{¬¨y} = \\frac{1}{1-(1-p_y)} = \\frac{1}{p_y}\\) the odds ratio of \\(y\\) and \\(¬¨y\\) is then \\(\\textrm{OR}_{(y,¬¨y)} = \\frac{p_y}{1-p_y}\\)  Further reading: Chapter 13 of (Gelman, Hill, and Vehtari 2020) walks through all this with examples and code in R. On the web, Jay Rotella has a nice PDF walk through of the same material with elaboration and graphical examples in R.\n  References Blitzstein, Joseph K, and Jessica Hwang. 2019. Introduction to Probability. 2nd ed. Chapman; Hall/CRC.  Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. Regression and Other Stories. Cambridge University Press.  Gelman, Andrew, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. Bayesian Data Analysis. 3rd ed. Chapman; Hall/CRC.  Rubin, Donald B. 1984. ‚ÄúBayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician.‚Äù The Annals of Statistics 12 (4): 1151‚Äì72.    ","date":1636070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636149906,"objectID":"34f4781f02fd8887872eec7fa652af2d","permalink":"/post/my-list-of-important-things/","publishdate":"2021-11-05T00:00:00Z","relpermalink":"/post/my-list-of-important-things/","section":"post","summary":"1 Purpose 2 Posterior Predictive Distributions  2.1 Conditional Probability 2.2 Law of Total Probability 2.3 Conditional Independence 2.4 Bayes Theorem 2.5 Bayes with Extra Conditioning 2.6 Deriving the PPD  3 Sampling and Integration 4 Logits, Probabilities and Odds  4.","tags":["probability","regression"],"title":"Stuff I Always Look Up ...","type":"post"},{"authors":["Dan W. Joyce","Nicholas Meyer"],"categories":null,"content":"","date":1634515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634515200,"objectID":"7d39943e729fd9bbc2be8633a6b15f37","permalink":"/publication/joyce-2021-psychometrics/","publishdate":"2021-10-18T00:00:00Z","relpermalink":"/publication/joyce-2021-psychometrics/","section":"publication","summary":"Validated instruments such as questionnaires, patient-reported outcome measures and clinician-rated psychopathology scales, are indispensable for measuring symptom burden and mental state, and for defining outcomes in both psychiatric practice and clinical trials. Most often, the values on the instrument‚Äôs multiple items (dimensions) are added to derive a single, univariate (scalar) sum-score. Although this approach simplifies interpretation, there are always many possible combinations of individual items that can yield the same sum-score. Two patients can therefore obtain identical scores on a given instrument, despite having very different combinations of underlying item scores corresponding to different patterns of clinical symptoms. The same is also true when a single patient is measured at two different time points, where the resulting sum-scores can obscure changes that may be clinically meaningful.\nWe present an alternative analytic framework, which leverages geometric concepts to represent measurements as points in a vector space. Using this framework, we show why sum-scores obscure information present in measurements of clinical state, and also provide a straightforward algorithm to mitigate against this problem. Clinically-relevant outcomes, such as remission or patient-centered treatment goals, can be represented intuitively, as reference points or ‚Äòanchors‚Äô within this space. Using real-world data, we then demonstrate how measuring the relative distance between points and anchors preserves more information, allowing outcomes such as proximity to remission, to be defined and measured.","tags":null,"title":"Refining the Analysis of Multidimensional Psychometric Data: A Geometric Distance Approach","type":"publication"},{"authors":["Ioannis Bakolis","Robert Stewart","David Baldwin","Jane Beenstock","Paul Bibby","Matthew Broadbent","Rudolf Cardinal","Shanquan Chen","Karthik Chinnasamy","Andrea Cipriani","Simon Douglas","Philip Horner","Caroline A Jackson","Ann John","Dan W Joyce","Sze Chim Lee","Jonathan Lewis","Andrew McIntosh","Neil Nixon","David Osborn","Peter Phiri","Shanaya Rathod","Tanya Smith","Rachel Sokal","Rob Waller","Sabine Landau"],"categories":null,"content":"","date":1620950400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620950400,"objectID":"b366f3aaec176f53165489ff4763c5ee","permalink":"/publication/bakolis2021/","publishdate":"2021-05-14T00:00:00Z","relpermalink":"/publication/bakolis2021/","section":"publication","summary":"To investigate changes in daily mental health (MH) service use and mortality in response to the introduction and the lifting of the COVID-19 lockdown policy in Spring 2020.","tags":null,"title":"Changes in daily mental health service use and mortality at the commencement and lifting of COVID-19 ‚Äòlockdown‚Äô policy in 10 UK sites: a regression discontinuity in time design","type":"publication"},{"authors":["Megan Thomas","Timea Szentgyorgyi","Lucy D. Vanes","Elias Mouchlianitis","Erica F. Barry","Krisna Patel","Katie Wong","Dan W Joyce","Sukhwinder Shergill"],"categories":null,"content":"","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609545600,"objectID":"a0d39cc626f145ec3717c44a9310da20","permalink":"/publication/thomas-2021/","publishdate":"2021-01-02T17:39:34.007652Z","relpermalink":"/publication/thomas-2021/","section":"publication","summary":"Approximately one third of psychosis patients fail to respond to conventional antipsychotic medication, which exerts its effect via striatal dopamine receptor antagonism. The present study aimed to investigate impaired cognitive control as a potential contributor to persistent positive symptoms in treatment resistant (TR) patients. 52 medicated First Episode Psychosis (FEP) patients (17 TR and 35 non-TR (NTR)) took part in a longitudinal study in which they performed a series of cognitive tasks and a clinical assessment at two timepoints, 12 months apart. Cognitive performance at baseline was compared to that of 39 healthy controls (HC). Across both timepoints, TR patients were significantly more impaired than NTR patients in a task of cognitive control, while performance on tasks of phonological and semantic fluency, working memory and general intelligence did not differ between patient groups. No significant associations were found between cognitive performance and psychotic symptomatology, and no significant performance changes were observed from the first to second timepoint in any of the cognitive tasks within patient groups. The results suggest that compared with NTR patients, TR patients have an exacerbated deficit specific to cognitive control, which is established early in psychotic illness and stabilises in the years following a first episode","tags":null,"title":"Cognitive performance in early, treatment-resistant psychosis patients: Could cognitive control play a role in persistent symptoms?","type":"publication"},{"authors":["Meyer, Nicholas","Joyce, Dan W","Karr, Chris","de Vos, Maarten","Dijk, Derk-Jan","Jacobson, Nicholas C","MacCabe, James H"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"3e44600d75ec8b77fc9b8c8808ea2869","permalink":"/publication/meyer-2021/","publishdate":"2021-01-01T17:39:34.007652Z","relpermalink":"/publication/meyer-2021/","section":"publication","summary":"Sleep disruption is a common precursor to deterioration and relapse in people living with psychotic disorders. Understanding the temporal relationship between sleep and psychopathology is important for identifying and developing interventions which target key variables that contribute to relapse.","tags":null,"title":"The temporal dynamics of sleep disturbance and psychopathology in psychosis: a digital sampling study","type":"publication"},{"authors":null,"categories":["R","graphs"],"content":"  Here‚Äôs the problem set-up: we have a group of diseases/disorders/conditions (A,B,C and D) and associated known treatments (RxA, RxB, RxC and RxD). We want to build graphs that represent associations between the conditions and treatments for a group of patients. We are given time-ordered data (from their medical records) that records a sequence of recorded episodes (but not the precise dates of each) of both conditions and treatments. Ultimately, we want to discover patterns of common treatments and conditions to derive some understanding of multimorbidity.\nAn example:\nThe circles represent mentions of disorders and the squares treatment episodes. Here, we can see someone with a life-long condition (A) for which they had continuous treatment (or at least, at the resolution of recording, whenever A was mentioned, RxA was also). At some point, they develop an episode of condition B and on the second occurrence, they‚Äôre prescribed treatment RxB. And so on. We can see that we could represent each time-ordered series as a binary indicator vector, for instance condition B = (0,0,1,1,0,0,1,1,1) and treatment RxD = (0,0,0,0,0,1,1,0,0).\nWe can construct a graph where nodes/vertices are the conditions and treatments, and the edges between represent relationships. In this example, A and RxA always co-occur together so if an edge represents co-occurence, this would be a very strong association. Another method of constructing the graph is to systematically time-lag each condition/treatment vector and see if there‚Äôs a consistent pattern of e.g.¬†condition B always preceding RxB. To do this, we use pairwise transfer entropy (which generalises the notion of Granger temporality) for each pair of conditions and treatments.\nIf we do this for the above time-ordered data, we get the following directed graph (note, squares are treatments and circles conditions):\nThis graph can be parsimoniously represented as an adjacency matrix: rows and columns represent conditions/treatments and the \\((i,j)\\)th element is the strength of association (i.e.¬†that condition/treatment \\(i\\) precedes condition/treatment \\(j\\)). For the graph above, the adjacency matrix looks like:\nImportantly, it‚Äôs an asymmetric directed graph, so the direction (as well as the strength) of associations matter. Ultimately, for this blogpost, it doesn‚Äôt really matter how the graph is constructed, just that we have a bunch of graphs represented as adjacency matrices.\nFor this simulation, we built 4 example time-ordered sequences of conditions/treatments and then derived the adjacency matrices as before, resulting in the following 4 exemplar graphs:\nThen, we derive 25 noisy variations of each examplar, giving us 100 samples as follows: Now assume that we are given the 100 noisy samples and we want to discover underlying graphs (which of course, in this simulated example, we know is 4 and we know what they look like). We can see glimpses of the original 4 exemplars in the table of samples, but if we take the ‚Äúaverage‚Äù graph (adjacency matrix) we‚Äôd get this: Graph partitioning methods try and tease out collections (or communities) of nodes/edges that form discrete graphs by selecting and cutting edges. One approach to this is spectral partitioning where first, we derive the Laplacian matrix (from the adjacency matrix) and perform eigen- or singular value decomposition (SVD) on it ‚Äì to me, this seems like magic: you can carve up a graph (represented as an adjacency matrix) using linear algebra. I recently learned about non-negative matrix factorisation from a paper by Hassaine et al. (2020) on multimorbidity in clinical data which prompted me to read the original paper (Lee and Seung 1999) and a bit like SVD, seemed like more magic to me. Further, Hassaine et al. (2020) described using tensors ‚Äì collections of matrices ‚Äì for a similar task.\nSo here we have our problem: given a bunch of 100 noisy adjacency matrices above, extract the underlying ‚Äòexemplar‚Äô or prototype graphs. The case presented here is not dissimilar to ideas in image processing ‚Äì each matrix is a noisy image and we want to find some underlying latent structure. If each of the 100 noisy adjacency matrices is a sample, it seems logical to ‚Äòstack‚Äô them up and see what can be learned from this ‚Äòstack.‚Äô A series of matrices can be stacked up into a multidimensional array structure (a tensor) and there‚Äôs a literature on how to perform decomposition (akin to SVD) on tensors. So you can see how the logic proceeds: if matrix decompositions (like SVD) can can be used to partition a single graph (connectivity matrix), then maybe tensors can help tease out common structure in a stack of adjacency matrices.\n1 Tensors There are plenty of great online resources on tensors and multilinear algerbra. One source that I found spectacularly helpful was Alexej Gossman‚Äôs tutorial on Tucker and Rank-1 decompositions, not least because it serves as an introduction to programming with tensors in R. A frequently-cited and comprehensive tutorial paper is (Kolda and Bader 2009) which contains the formal details alluded to here and from which we borrow notation. Much of what follows are notes to help me remember the core concepts.\nA tensor is essentially a multi-dimensional array ‚Äì much the same as the notion of a multi-dimensional array of numbers in programmning languages ‚Äì and generalises familiar objects like vectors and matrices.\nA tensor has an order which equates the number of dimensions of the array.\nAn order one tensor is a vector \\(\\mathbf{v} = \\left[ v_1, v_2, \\ldots, v_{I_1} \\right]\\) with \\(I_1\\) elements (i.e.¬†is of length \\(I_1\\)). A vector of real numbers is denoted \\(\\mathbf{v} \\in \\mathbb{R}^{I_1}\\) (and in what follows, all our tensors will contain real elements).\nAn order two tensor is a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{I_1 \\times I_2}\\) with \\(I_1\\) rows and \\(I_2\\) columns.\nBefore getting carried away, a simple motivating example. Take the matrix \\(\\mathbf{M} \\in \\mathbb{R}^{2 \\times 3}\\)\nM \u0026lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE ) M ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 If we want the first row of the matrix \\(\\mathbf{M}\\) we‚Äôll write this as \\(\\mathbf{M}_{1:}\\) which we read as ‚Äúset the row index \\(i_1 = 1\\) and retrieve all columns‚Äù yielding (1, 2, 3). Similarly, if we want the third column of \\(\\mathbf{M}\\) we‚Äôll write this as \\(\\mathbf{M}_{:3}\\), to be read as ‚Äúset the column index \\(i_2 = 3\\) and retrieve all rows‚Äù yielding (3, 6).\n1.1 Modes and Fibres Translating to the language of tensors; instead of discussing rows or columns, we generalise to fibres and modes:\n a matrix is a 2nd order tensor with indices \\(i_1\\) and \\(i_2\\) index \\(i_1\\) refers to the first mode index \\(i_2\\) refers to the second mode  ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 Then, we have:\n the mode-1 fibres of the matrix above are the columns denoted by \\(\\mathbf{M}_{:j}\\) which are (1, 4), (2, 5) and (3, 6), the mode-2 fibres are the rows of the matrix above, denoted by \\(\\mathbf{M}_{i:}\\) which are (1, 2, 3) and (4, 5, 6)  Fibres of a tensor are obtained by ‚Äòfixing‚Äô all but one of the mode indices.\n 1.2 Third Order Tensors and Slices Now we come to third order tensors \\(\\mathbf{\\mathcal{X}} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}\\) representing a ‚Äúcuboid‚Äù of elements with three modes and indices ranging from \\(i_1 = 1 \\ldots I_1\\), \\(i_2 = 1 \\ldots I_2\\) and \\(i_3 = 1 \\ldots I_3\\).\nLet‚Äôs illustrate this by first of all defining two matrices of size \\(2 \\times 3\\):\nX1 \u0026lt;- matrix( c(1,2,3,4,5,6), nrow = 2, ncol = 3, byrow = TRUE ) X2 \u0026lt;- matrix( c(7,8,9,10,11,12), nrow = 2, ncol = 3, byrow = TRUE ) print(X1) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 print(X2) ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 And now, glue them together in the third mode with X1 at the front and X2 behind:\nX \u0026lt;- as.tensor( abind( X1, X2, along = 3 ) ) print(X@data) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 By ‚Äústacking‚Äù X1 and X2 we‚Äôve built the tensor \\(\\mathbf{\\mathcal{X}} \\in \\mathbb{R}^{2 \\times 3 \\times 2}\\).\nThis illustrates slices ‚Äì by fixing all the indices except two, we obtain a matrix that represents a ‚Äòcut‚Äô through the tensor in two of the modes. The frontal slices are \\(\\mathbf{X}_{::k}\\) and are simply the two matrices X1 and X2 in the code above. Note that we use the R package rTensor which provides a class for tensor representations.\nWe can slice the tensor on different modes, obtaining horizontal slices \\(\\mathbf{X}_{i::}\\)\nX[1,,]@data ## [,1] [,2] ## [1,] 1 7 ## [2,] 2 8 ## [3,] 3 9 X[2,,]@data ## [,1] [,2] ## [1,] 4 10 ## [2,] 5 11 ## [3,] 6 12 And lateral slices \\(\\mathbf{X}_{:j:}\\)\nX[,1,]@data ## [,1] [,2] ## [1,] 1 7 ## [2,] 4 10 X[,2,]@data ## [,1] [,2] ## [1,] 2 8 ## [2,] 5 11 X[,3,]@data ## [,1] [,2] ## [1,] 3 9 ## [2,] 6 12 We‚Äôre not going to need more than 3rd order tensors for what follows, so we won‚Äôt generalise further.\n 1.3 Matricization and Vectorization of Tensors Order 3 tensors are cuboids and this is intutive. But if the order is higher, it becomes very hard to visualise, manipulate and define operations on tensors.\nOne simple transformation is to vectorize the tensor, which simply means ‚Äòflattening‚Äô the tensor into a vector following some convention for how elements are ‚Äúread out‚Äù by systematically varying the indices with respect to each other (e.g.¬†\\(i_1\\) varies slower than \\(i_2\\)). We denote this operation \\(\\text{vec}(\\mathcal{X})\\) which gives:\nrTensor::vec(X) ## [1] 1 4 2 5 3 6 7 10 8 11 9 12 Notice that this look like a vector of the column fibres of each frontal slice ‚Äì i.e.¬†(1, 4)\\(^T\\), (2, 5)\\(^T\\), (3, 6)\\(^T\\) ‚Ä¶ concatenated together.\nMatricization is the transforming of a tensor into a matrix represention by unfolding the tensor along a mode. This helps us visualise as well as understand operations on tensors. The sources to understand this in detail are Kolda and Bader (2009) and Bader and Kolda (2006).\nAn n-mode matricization of a tensor yields a matrix with columns being the mode-n fibres of the tensor.\nTake our \\(2 \\times 3 \\times 2\\) tensor \\(\\mathcal{X}\\):\nX@data ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 If we unfold the tensor along the first mode we get the two frontal slices (matrices) \\(\\mathbf{X}_{::1}\\) and \\(\\mathbf{X}_{::2}\\) concatenated side-by-side as a matrix \\(\\left[ \\mathbf{X}_{::1} \\mid \\mathbf{X}_{::2} \\right]\\) and we denote this with a bracketed subscript indicating the mode \\(\\mathbf{X}_{(1)}\\)\nk_unfold(X, m = 1) ## Numeric Tensor of 2 Modes ## Modes: 2 6 ## Data: ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 7 8 9 ## [2,] 4 5 6 10 11 12 And if we unfold on the second mode \\(\\mathbf{X}_{(2)}\\) we get the transposed frontal slices arranged side-by-side \\(\\left[ \\mathbf{X}^{\\intercal}_{::1} \\mid \\mathbf{X}^{\\intercal}_{::2} \\right]\\)\nk_unfold(X, m = 2) ## Numeric Tensor of 2 Modes ## Modes: 3 4 ## Data: ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 Finally, unfolding in the third mode \\(\\mathcal{X}_{(3)}\\) we get:\nk_unfold(X, m = 3) ## Numeric Tensor of 2 Modes ## Modes: 2 6 ## Data: ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 4 2 5 3 6 ## [2,] 7 10 8 11 9 12 Which is the matrix : \\[ \\begin{bmatrix} \\text{vec}(\\mathcal{X}_{::1})^T \\\\ \\text{vec}(\\mathcal{X}_{::2})^T \\end{bmatrix} \\] That is, the first and second rows are the vectorization of the first and second frontal slices respectively:\nrbind( vec(X[,,1]), vec(X[,,2] ) ) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 4 2 5 3 6 ## [2,] 7 10 8 11 9 12  1.4 Tensor-Matrix Products An important operation is the product of a tensor and a matrix along the \\(n\\)th mode, or the n-mode product. Restricting out attention to order 3 tensors (‚Äúcubes‚Äù) ‚Äì we multiply the tensor \\(\\mathbf{\\mathcal{X}}^{I_1 \\times I_2 \\times I_3}\\) with a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{J \\times I_n}\\) where \\(n\\) is mode 1, 2 or 3.\nTo my mind, the definition of the n-mode product is most easily understood in terms of matricization (unfolding), matrix products and the folding the result back into a tensor. The expression for the element-wise calculation is given in (Kolda and Bader 2009) pp.¬†460.\nFor example, take the matrix \\(\\mathbf{Q} \\in \\mathbb{R}^{3 \\times 2}\\):\nQ \u0026lt;- matrix( c(1,1,2,2,3,3), nrow = 3, ncol = 2, byrow = TRUE ) print(Q) ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 To multiply \\(\\mathbf{Q}\\) by mode \\(n\\) of the tensor \\(\\mathbf{\\mathcal{X}}\\) we perform this sequence of operations:\nLet \\(\\mathbf{X}_{(n)}\\) be the \\(n\\) mode unfolding of \\(\\mathbf{\\mathcal{X}}\\) Compute the matrix product \\(\\mathbf{X\u0026#39;}_{(n)} = \\mathbf{Q} \\mathbf{X}_{(n)}\\) Fold \\(\\mathbf{X\u0026#39;}_{(n)}\\) along the \\(n\\)-th mode  Note, this only makes sense if the dimensions of the matrix \\(\\mathbf{Q}\\) are conformable with the unfolding of the tensor i.e.¬†that the number of columns in \\(\\mathbf{Q}\\) equals \\(I_n\\), the size of mode \\(n\\) in the tensor\nLet‚Äôs do this for the first mode of our example. Here‚Äôs our tensor \\(\\mathbf{\\mathcal{X}}\\) again:\nprint(X@data) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 8 9 ## [2,] 10 11 12 Mode 1 of \\(\\mathbf{\\mathcal{X}}\\) is of size \\(I_1 = 2\\) and our matrix \\(\\mathbf{Q}\\) has two columns, so we‚Äôre good.\nUnfold \\(\\mathbf{\\mathcal{X}}\\) along the first mode \\(\\mathbf{X}_{(1)}\\)  # mode to take product with n \u0026lt;- 1 X_1 \u0026lt;- k_unfold( X, n )@data print(X_1) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 7 8 9 ## [2,] 4 5 6 10 11 12 Compute the matrix product \\(\\mathbf{X\u0026#39;}_{(1)} = \\mathbf{Q} \\mathbf{X}_{(1)}\\)  X_1_prime \u0026lt;- Q %*% X_1 print( X_1_prime ) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 5 7 9 17 19 21 ## [2,] 10 14 18 34 38 42 ## [3,] 15 21 27 51 57 63 Fold \\(\\mathbf{X\u0026#39;}_{(1)}\\) along the \\(n\\)-th mode  # compute the required modes for the resulting folded tensor Q.dims \u0026lt;- dim(Q) modes.X_prime \u0026lt;- X@modes modes.X_prime[n] \u0026lt;- Q.dims[1] # fold Q_by_X.1 \u0026lt;- k_fold( X_1_prime, n, modes.X_prime ) print( Q_by_X.1@data ) ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 5 7 9 ## [2,] 10 14 18 ## [3,] 15 21 27 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 17 19 21 ## [2,] 34 38 42 ## [3,] 51 57 63 Naturally, there‚Äôs a function to do this for us in one simple step:\nrTensor::ttm( X, Q, m = 1)@data ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 5 7 9 ## [2,] 10 14 18 ## [3,] 15 21 27 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 17 19 21 ## [2,] 34 38 42 ## [3,] 51 57 63   2 Matrix Decompositions We can now venture into the decompositions, or factorizations, of matrices and tensors.\n2.1 Singular Value Decomposition set.seed(314159) A \u0026lt;- matrix( c(1,0,0,1, 1,0,0,1, 1,1,1,1, 1,0,0,1, 1,0,0,1), ncol = 4, nrow = 5, byrow = TRUE) A \u0026lt;- A + runif( 4*5, -0.1, 0.3) UDV \u0026lt;- svd(A) U \u0026lt;- UDV$u D \u0026lt;- diag(UDV$d) V \u0026lt;- UDV$v recon.4 \u0026lt;- U[,1:4] %*% D[1:4,1:4] %*% t(V[,1:4]) recon.2 \u0026lt;- U[,1:2] %*% D[1:2,1:2] %*% t(V[,1:2]) par(mfrow=c(1,3), mar = c(0.5,0.5,2,0.5), pty = \u0026quot;s\u0026quot;) image( t(A), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;Original\u0026quot; ) image( t(recon.4), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;k=4\u0026quot; ) image( t(recon.2), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;k=2\u0026quot; ) This diagram shows on the left, an original image (a sort of noisy ‚ÄúH‚Äù shape) represented by a matrix \\(\\mathbf{M} \\in \\mathbb{R}^{5 \\times 4}\\).\nWe subject this to a singular valued decomposition: \\[ \\mathbf{M} = \\mathbf{UDV}^{\\intercal} \\] where the factor matrices are:\n \\(D\\) is the diagonal matrix of singular values, with largest value in the top left, descending to the smallest along the diagonal:  print(D) ## [,1] [,2] [,3] [,4] ## [1,] 3.504158 0.000000 0.0000000 0.0000000 ## [2,] 0.000000 1.323071 0.0000000 0.0000000 ## [3,] 0.000000 0.000000 0.2577888 0.0000000 ## [4,] 0.000000 0.000000 0.0000000 0.1509413  \\(U\\) is a matrix of \\(5 \\times 4\\) left singular vectors of \\(\\mathbf{M}\\)  print(U) ## [,1] [,2] [,3] [,4] ## [1,] -0.4365701 -0.2339371 0.813531383 -0.01200304 ## [2,] -0.4120556 -0.2359644 -0.512436719 0.37115595 ## [3,] -0.5058560 0.8584828 -0.003890147 0.06573130 ## [4,] -0.3961206 -0.1703899 -0.263178631 -0.86250248 ## [5,] -0.4762509 -0.3515234 -0.079354732 0.33744341  \\(V\\) is a matrix of \\(4 \\times 4\\) right singular vectors of \\(\\mathbf{M}\\)  print(V) ## [,1] [,2] [,3] [,4] ## [1,] -0.6951074 -0.2107143 -0.68594850 -0.04358657 ## [2,] -0.1943603 0.5802217 -0.03149829 0.79030037 ## [3,] -0.2231139 0.7589987 0.03175303 -0.61084605 ## [4,] -0.6551869 -0.2070343 0.72627423 0.01981508 We can now ‚Äòreconstruct‚Äô an approximation to the original image \\(\\widetilde{\\mathbf{M}} = \\mathbf{UDV^{\\intercal}}\\) and this is shown in the middle picture with \\(k = 4\\) denoting that we use all 4 of the singular vectors.\nThe ‚Äúmagic‚Äù alluded to earlier is when we use a truncated version of the SVD ‚Äì take the first \\(1 \\ldots k\\) singular vectors and use these to reconstruct the image: \\[ \\widetilde{\\mathbf{M}} = \\mathbf{U}_{:k} \\mathbf{D}_{kk} \\mathbf{V}^{\\intercal}_{:k} \\] With \\(k=2\\), we get the third image on the right; a reconstructed image using half the vectors (i.e.¬†compressed) which additionally helps de-noise the original image.\n 2.2 Non-Negative Matrix Factorisation Another decomposition (factorisation) is the non-negative matrix factorisation or NMF (see Lee and Seung (1999) for details). Here, we factor: \\[ \\mathbf{M} = \\mathbf{WH}^{\\intercal} \\] subject to the constraint that the factor matrices \\(\\mathbf{W}\\) and \\(\\mathbf{H}\\) are non-negative (cf.¬†the matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) in SVD as shown above). This results in an algorithm for estimating the factor matrices which is iterative for each element of \\(\\mathbf{W}\\) and \\(\\mathbf{H}\\). The remarkable feature of NMF is that the resulting factor matrices have a component-parts interpretation as follows; the columns of \\(\\mathbf{H}\\) are encodings and columns of \\(\\mathbf{W}\\) are weights that map encodings to weighted-sums in \\(\\mathbf{M}\\).\nThis means, we can use a truncated NMF decomposition to partition ‚Äúcomponent parts‚Äù of \\(\\mathbf{M}\\).\nAs an example, with the same noisy ‚ÄúH‚Äù image, we apply NMF truncating to \\(k=2\\) and reconstruct:\nnmf.1 \u0026lt;- NMF(A, J=2) W \u0026lt;- nmf.1$U H \u0026lt;- nmf.1$V recon.nmf \u0026lt;- W %*% t(H) par(mfrow=c(1,2), mar = c(0.5,0.5,2,0.5), pty = \u0026quot;s\u0026quot;) image( t(A), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;Original\u0026quot; ) image( t(recon.nmf), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;NMF (k = 2)\u0026quot; )  The result is very similar to SVD. The ‚Äúmagic‚Äù here is in the following trick ‚Äì take each column of \\(\\mathbf{W}_{:i}\\) and multiply it by the corresponding column of \\(\\mathbf{H}^{\\intercal}_{:i}\\) and we get:\nrecon.nmf.1 \u0026lt;- W[,1] %*% t(H[,1]) recon.nmf.2 \u0026lt;- W[,2] %*% t(H[,2]) recon.nmf.sum \u0026lt;- recon.nmf.1 + recon.nmf.2 par(mfrow=c(1,3), mar = c(0.5,0.5,2,0.5), pty = \u0026quot;s\u0026quot;) image( t(recon.nmf.1), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;W x H (1st column)\u0026quot; ) image( t(recon.nmf.2), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;W x H (2nd column)\u0026quot; ) image( t(recon.nmf.sum), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;Sum\u0026quot; ) So we see that the columns of the factor matrices capture component parts of the source matrix and reconstruction is the sum of those component parts. This is a direct consequence of imposing the constraint that the elements of \\(\\mathbf{M} = \\mathbf{WH}^{\\intercal}\\) must be non-negative.\nWe can repeat the experiment with \\(k=3\\):\nset.seed(314150) nmf.2 \u0026lt;- NMF(A, J=3, algorithm = \u0026quot;Frobenius\u0026quot;) W \u0026lt;- nmf.2$U H \u0026lt;- nmf.2$V recon.nmf \u0026lt;- W %*% t(H) recon.nmf.1 \u0026lt;- W[,1] %*% t(H[,1]) recon.nmf.2 \u0026lt;- W[,2] %*% t(H[,2]) recon.nmf.3 \u0026lt;- W[,3] %*% t(H[,3]) recon.nmf.sum \u0026lt;- recon.nmf.1 + recon.nmf.2 + recon.nmf.3 par(mfrow=c(1,4), mar = c(0.5,0.5,2,0.5), pty = \u0026quot;s\u0026quot;) image( t(recon.nmf.1), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;W x H (1st column)\u0026quot; ) image( t(recon.nmf.2), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;W x H (2nd column)\u0026quot; ) image( t(recon.nmf.3), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;W x H (3rd column)\u0026quot; ) image( t(recon.nmf.sum), xaxt = \u0026quot;n\u0026quot;, yaxt = \u0026quot;n\u0026quot;, col = viridis(256), main = \u0026quot;Sum\u0026quot; ) Notice now that the three components are the right and left vertical ‚Äòbars‚Äô and the one horizontal bar. The problem is, with \\(k=2\\) the solution found by NMF is stable (i.e.¬†it always extracts a component with the two vertical bars, and one hortizontal) but with \\(k=3\\) the solution varied significantly between runs of the iterative NMF algorithm; sometimes local optima are found which make sense, other times we get less meaningful decompositions. These variable results are not shown here because we fix the pseudo-random number generator seed for reproducibility.\nOne of the problems with applying NMF is finding the right \\(k\\) that reflects the desired component-parts decomposition in the problem domain.\n  3 Tensor Decomposition For a survey of tensor decomposition techniques, Kolda and Bader (2009) is the go-to source. Here, we‚Äôll focus on one decomposition that mirrors NMF for tensors, the non-negative Tucker decomposition (NTD) for mode-3 tensors. Instead of having a target matrix, decomposed by multiplying factor matrices together, we have a more complex problem of factorising a cuboid. Recall our example: 100 noisy samples derived from 4 ‚Äòprototypical‚Äô graph adjacency matrices.\nWith tensors, the intuitions from matrix decomposition apply, but instead of dealing with products of matrices, we now have a more complex situation shown below:\nThe Tucker decomposition is then: \\[ \\mathcal{X} = \\mathcal{G} \\times_1 \\mathbf{A} \\times_2 \\mathbf{B} \\times_3 \\mathbf{C} \\]\nHere:\n \\(\\mathcal{X}\\) is the tensor of dimensions \\(I_1 \\times I_2 \\times I_3\\) which for our application represents the \\(8 \\times 8\\) adjacency matrices stacked in the third mode to build a tensor of size \\(8 \\times 8 \\times 100\\) \\(\\mathcal{G}\\) is the core tensor which has dimensions \\(R_1 \\times R_2 \\times R_3\\) and is smaller than the dimensions of \\(\\mathcal{X}\\) where each \\(R_j \\leq I_j\\) ‚Äì therefore, the core tensor ‚Äúcompresses‚Äù the information in \\(\\mathcal{X}\\) \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are the factor matrices which have dimensions \\(I_1 \\times R_1\\), \\(I_2 \\times R_2\\) and \\(I_3 \\times R_3\\) respectively.  By analogy with principal components analysis, the factor matrices \\(\\mathbf{A}\\), \\(\\mathbf{B}\\) and \\(\\mathbf{C}\\) are the principal components in each of the modes and the core tensor represents the interactions between the components (Kolda and Bader 2009).\nThe iterative algorithm for computing the factor matrices and core tensor subject to non-negativity constraints can be found in in Chapter 7 of (Cichocki et al. 2009) and connections to latent structure and statistics can be found in (Shashua and Hazan 2005). Here, we use the nnTensor package implementation for NTD.\n 4 Application Let‚Äôs return to the example we started with, recovering adjacency matrices from 100 noisy samples.\nThe code for what follows is computationally expensive (so I‚Äôve not shown the code here)\n4.1 Finding an Appropriate Rank for the Core Tensor The example of NMF on the ‚ÄúH‚Äù image above demonstrates that knowing in advance what the expected components are helps decide on the truncation (or reduced rank) to use. In the tensor example of 100 adjacency matrices, assume we don‚Äôt know that there are four prototypical adjacency matrices in advance. We need to decide on the size of the core tensor (and the corresponding factor matrices) without this information.\nRecall also that the core tensor is a compressed representation of the data in each of the three modes.\nHere‚Äôs the approach; we use NTD to obtain a compressed representation and treat the factor matrix \\(\\mathbf{C}\\) (corresponding to the third mode of the tensor \\(\\mathcal{X}\\)) as a projection (i.e.¬†dimensionality reduction) to locate a candidate number of clusters for all 100 samples.\nCompute a low-rank NTD with \\(R_1 = R_2 = 8\\) and \\(R_3 = 3\\) ‚Äì so, compressing over the third mode (the ‚Äòstacking‚Äô of the adjacency matrices) but leaving the modes corresponding to the adjacency matrix rows/columns uncompressed. Reducing on \\(R_3 = 3\\) is somewhat arbitrary, but allows us to visualise the resulting factor matrix \\(\\mathbf{C}\\) in Step 2 Take the factor matrix \\(\\mathbf{C} \\in \\mathbb{R}^{100 \\times 3}\\) which represents each of the 100 adjacency matrices projected in a space \\(\\mathbb{R}^3\\) Perform k-means clustering in \\(\\mathbb{R}^3\\) with multiple re-starts over a range of candidate cluster numbers (e.g.¬†2 through to 10 clusters); for each number of clusters, use the Gap statistic to ascertain the optimal number of clusters (Tibshirani, Walther, and Hastie 2001). Repeat steps 1‚Äì3 a number of times (say, 20) recording the optimum number of clusters located in Step 3 Take the mode number of clusters \\(\\#c\\)  We repeat steps 1‚Äì3 because (as for NMF) NTD uses an iterative algorithm not guaranteed to find a global optimum and often, NTD locates quite different factorisations for different initialisations.\nOn our example data, here‚Äôs what we obtain:\nEach plot is one of three projections of \\(\\mathbf{C}\\) (it looks a lot cooler as a 3D plot but it‚Äôs harder to see the clusters clearly). So we can locate 4 distinct clusters, each corresponding to one of the ‚Äòprototype‚Äô network graphs from earlier:\nBader, Brett W, and Tamara G Kolda. 2006. ‚ÄúAlgorithm 862: MATLAB Tensor Classes for Fast Algorithm Prototyping.‚Äù ACM Transactions on Mathematical Software (TOMS) 32 (4): 635‚Äì53.  Cichocki, Andrzej, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. 2009. Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-Way Data Analysis and Blind Source Separation. John Wiley \u0026amp; Sons.  Hassaine, Abdelaali, Gholamreza Salimi-Khorshidi, Dexter Canoy, and Kazem Rahimi. 2020. ‚ÄúUntangling the Complexity of Multimorbidity with Machine Learning.‚Äù Mechanisms of Ageing and Development 190: 111325. https://doi.org/10.1016/j.mad.2020.111325.  Kolda, Tamara G, and Brett W Bader. 2009. ‚ÄúTensor Decompositions and Applications.‚Äù SIAM Review 51 (3): 455‚Äì500.  Lee, Daniel D, and H Sebastian Seung. 1999. ‚ÄúLearning the Parts of Objects by Non-Negative Matrix Factorization.‚Äù Nature 401 (6755): 788‚Äì91.  Shashua, Amnon, and Tamir Hazan. 2005. ‚ÄúNon-Negative Tensor Factorization with Applications to Statistics and Computer Vision.‚Äù In Proceedings of the 22nd International Conference on Machine Learning, 792‚Äì99.  Tibshirani, Robert, Guenther Walther, and Trevor Hastie. 2001. ‚ÄúEstimating the Number of Clusters in a Data Set via the Gap Statistic.‚Äù Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63 (2): 411‚Äì23.     ","date":1600300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600300800,"objectID":"7a5f5d4ba26f67e0f428559648c7118b","permalink":"/post/tensor-connectivity-matrices/","publishdate":"2020-09-17T00:00:00Z","relpermalink":"/post/tensor-connectivity-matrices/","section":"post","summary":"Here‚Äôs the problem set-up: we have a group of diseases/disorders/conditions (A,B,C and D) and associated known treatments (RxA, RxB, RxC and RxD). We want to build graphs that represent associations between the conditions and treatments for a group of patients.","tags":["R","clinical data","tensors","multimorbidity"],"title":"Tensor Decompositions and Noisy Graphs","type":"post"},{"authors":null,"categories":["clinical decision support","decision theory"],"content":"   In clinical decision making for serious but rare events, there has been discussion about how to use predictive models as tools in decision making.\nOne example is in decision making for assessment and treatment in people at risk of suicide. A systematic review Kessler et al. (2019) of ‚Äúsuicide prediction models‚Äù cites the very real concern that the low positive predictive value (PPV) of current state-of-the-art models renders them at least clinically useless and at worst obviously dangerous. Most published models, however, attempt to predict the absolute risk of suicide based on some feature data (i.e.¬†covariates, independent variables or predictors) for individuals ‚Äì that is, these models attempt to identify people at risk of suicide. A central tennet of Kessler et al‚Äôs argument is that net benefit ‚Äì rather than positive predictive value ‚Äì is the appropriate decision-theoretic framework and in effect, predictive models might be better used as tools for screening out cases (of course, their argument and analysis is far more detailed but this is what I‚Äôm focusing on here). Kessler et al describe how to improve the clinical utility of suicide prediction models by embedding them in a clinical triaging system and using thresholds for intervening (or not) derived from decision curve analysis Vickers, Van Calster, and Steyerberg (2016).\nKessler et al‚Äôs proposal is that if there is a high prevalence of negative cases in routine clinical practice, then such a staged triaging system would enable scarce (and often, intrusive) clinical resources to be directed towards cases which are uncertain. In this post, we consider positive and negative predictive value, net benefit as well as examining a sequential triage model of clinical decision support.\n1 Predictive Values With an assumed representative sample of a population, let \\(Y\\) be the output of the decision rule/system, and \\(E\\) be whether or not the event occurred:\n \\(Y = 1\\) represents the decision that a case is positive, and \\(Y = 0\\) represents a negative decision \\(E = 1\\) represents the serious event occuring, \\(E = 0\\) that it did not  Consider the following hypothetical confusion matrix for a decision system on a representative validation sample of 1000 people:    Event (E)      0  1     Decision (Y)    0  900  10    1  80  10     There are 20 serious events in 1000 cases. The model gives a correct decision for 900/980 negative events (true negatives, TN) and decides that 80 negative cases are in fact positve (false positives; FP). The model performs poorly on decisions with positive cases; it decides 10/20 positive events are positive (true positives, TP) and makes the potentially catastrophic decision that 10/20 positives are in fact negatives (false negatives, FN).\nSo, we find that :\n Sensitivity = \\(\\Pr(Y = 1 \\mid E = 1)\\) = \\(TP/(TP+FN)\\) = 0.5; the probability that the decision was positive, given the event was positive Specificity = \\(\\Pr(Y = 0 \\mid E = 0)\\) = \\(TN/(TN+FP)\\) = 0.918; the probability that the decision was negative, given the event was negative The prevalence of the serious event is 0.02  As noted in the Altman and Bland classic (Altman and Bland 1994), ‚Äúthe whole point of a diagnostic test is to use it to make a diagnosis, so we need to know the probability that the test will give the correct diagnosis‚Äù and sensitivity and specificity of the test (here, the decision rule) aren‚Äôt sufficient.\nThe important point is, in a clinical situation, we are interested in the conditional probabilities \\(\\Pr(E \\mid Y)\\) (the probability of the event given the decision rule output).\nHowever, we only have the conditionals \\(\\Pr(Y \\mid E )\\) and conditional probabilities do not commute so \\(\\Pr( Y \\mid E) \\neq \\Pr( E \\mid Y)\\). Failure to recognise this difference is the prosecutor‚Äôs fallacy or the fallacy of the transposed conditional (Blitzstein and Hwang 2019) Chapter 2.8.\nWe will need to enumerate the probabilities of other conditions (i.e.¬†states of \\(E\\) and \\(Y\\)), so:\n \\(\\Pr(Y = 1 \\mid E = 0)\\) is the false positive rate, or 1-specificity = \\(1-\\Pr(Y=0 \\mid E=0)\\) = \\(FP/(FP+TN)\\) = 0.082 \\(\\Pr(Y = 0 \\mid E = 1)\\) is the false negative rate, or 1-sensitivity = \\(1-\\Pr(Y=1 \\mid E=1)\\) = \\(FN/(FN+TP)\\) = 0.5  1.1 Deriving PPV and NPV The definition of conditional probability means that, for the conditions we want, we can state: \\[ \\Pr(E \\mid Y) = \\frac{\\Pr(E,Y)}{\\Pr(Y)} \\] Or, by rearranging: \\[ \\Pr(E,Y) = \\Pr(E \\mid Y) \\Pr(Y) \\] Applying the same argument for the conditionals we have \\(\\Pr(Y \\mid E)\\) (sensitivity and specificity):\n\\[ \\Pr(Y,E) = \\Pr(Y \\mid E) \\Pr(E) \\]\nThe joint probability of two events are commutative (unlike conditionals) therefore \\(\\Pr(E,Y) = \\Pr(Y,E)\\) and we can equate: \\[ \\Pr(E \\mid Y) \\Pr(Y) = \\Pr(Y \\mid E) \\Pr(E) \\] Noting again that we are interested in \\(\\Pr(E \\mid Y)\\) we can solve: \\[ \\Pr(E \\mid Y) = \\frac{\\Pr(Y \\mid E) \\Pr(E)}{\\Pr(Y)} \\] This is Bayes formula.\n 1.2 Example Calculation Using our example above, here‚Äôs want we want, and what we have available: \\[ \\Pr(E=1 \\mid Y=1) = \\frac{ \\overbrace{\\Pr( Y=1 \\mid E=1)}^\\text{sensitivity} \\overbrace{\\Pr(E=1)}^\\text{prevalence} } { \\underbrace{\\Pr(Y=1)}_\\text{prob. of +ve decision} } \\] We can calculate the denominator \\(\\Pr(Y=1)\\), the unconditional probability of a positive decision, using the law of total probability: \\[ \\begin{aligned} \\Pr(Y=1) =\u0026amp; \\overbrace{\\Pr( Y=1 \\mid E=1)}^\\text{sensitivity} \\overbrace{\\Pr(E=1)}^\\text{prevalence} + \\\\ \u0026amp;\\underbrace{\\Pr( Y=1 \\mid E=0)}_\\text{1-specificity} \\underbrace{\\Pr(E=0)}_\\text{1-prevalence} \\end{aligned} \\]\nThe calculation step-by-step is:\nCompute the denominator \\(\\Pr(Y=1) = 0.5 \\times 0.02 + (1-0.918) \\times (1-0.02) = 0.09\\) Substitute sensitivity and prevalence in the numerator:  \\[ \\Pr(E=1 \\mid Y=1) = \\frac{ \\overbrace{0.5}^\\text{sensitivity} \\times \\overbrace{0.02}^\\text{prevalence} } { \\underbrace{0.09}_\\text{prob. of +ve decision} } = 0.11 \\] Which delivers the positive predictive value.\nWe can similarly derive the negative predictive value:\n\\[ \\Pr(E=0 \\mid Y=0) = \\frac{ \\overbrace{\\Pr( Y=0 \\mid E=0)}^\\text{specificity} \\overbrace{\\Pr(E=0)}^\\text{1-prevalence} } { \\underbrace{\\Pr(Y=0)}_\\text{prob. of -ve decision} } \\] And our denominator in this case: \\[ \\begin{aligned} \\Pr(Y=0) =\u0026amp; \\overbrace{\\Pr( Y=0 \\mid E=1)}^\\text{1-sensitivity} \\overbrace{\\Pr(E=1)}^\\text{prevalence} + \\\\ \u0026amp; \\underbrace{\\Pr( Y=0 \\mid E=0)}_\\text{specificity} \\underbrace{\\Pr(E=0)}_\\text{1-prevalence} \\end{aligned} \\] Plugging in the numbers:\nCompute the denominator \\(\\Pr(Y=0) = (1-0.5) \\times 0.02 + 0.918 \\times (1-0.02) = 0.91\\) Substitute specificity and prevalence in the numerator:  \\[ \\Pr(E=0 \\mid Y=0) = \\frac{ \\overbrace{0.918}^\\text{specificity} \\times \\overbrace{0.98}^\\text{1-prevalence} } { \\underbrace{0.91}_\\text{prob. of -ve decision} } = 0.987 \\]\nThis hypothetical decision system is useful for correctly deciding on negative cases, but performs poorly on identifying positive cases.\n  2 Simulation Now suppose that we have two (or more) clinical tests to help identify patients at risk for a relatively rare but serious event; for example, \\(X_1\\) is a relatively cheap and easy-to-administer instrument or questionnaire. \\(X_2\\) is a semi-structured interview or clinical examination which is time consuming, requires expertise to administer and is therefore significantly more costly than \\(X_1\\).\nFurther, we have a development sample of 5000 people for which we have the results for \\(X_1\\), \\(X_2\\) and we know who in this sample experienced the serious event.\nWe next build a model that attempts to predict the rare, serious event (\\(Y = 1\\)) on the basis of a patient‚Äôs \\(X_1\\) results and denote this \\(y = F_{X_1}(x)\\). Note, no claim is made that this model is well designed.\nAssume the somewhat luxurious position that we have a further 5000 validation cases from the same population ‚Äì so we can examine the model‚Äôs performance on data it was not ‚Äòtrained‚Äô on.\nLet‚Äôs look at the calibration of the model on the validation sample:\nIt‚Äôs important to note that:\n the model \\(F_{X_1}\\) delivers a prediction in the form of a continuous estimate of the absolute probability of the serious event given the screening instrument \\(X_1\\). there is no decision rule here; so we can‚Äôt discuss PPV or NPV the model is poorly calibrated: which is unsurprising given the serious event is rare ‚Äì in the 5000 validation samples there were 240 serious events (\\(E = 1\\)) representing a small prevalence of 0.048 the model appears to under estimate the probability of a serious event; for example, if the model predicts a probability of a serious event of 0.25, the actual probability is closer to 0.5.  We can repeat the same analysis for the other, more costly instrument \\(X_2\\); as for the cheaper instrument, we train a model \\(F_{X_2}\\) and then we have access to a validation sample on which we can examine the calibration:\nAgain, not great calibration.\n 3 Decision Rules Returning to the idea that Kessler et al discussed, how can we design a decision rule that makes use of these two tests ?\n3.1 ROC Curves A common approach to designing a decision rule is to vary a threshold over the output of \\(F_{X_1}\\) and plot the ROC curve; then, find an ‚Äúoptimal‚Äù threshold that maximises the sensitivity/specificity tradeoff.\n## Setting levels: control = 0, case = 1 ## Setting direction: controls \u0026lt; cases The confusion matrix for the decision rule with the threshold = 0.074 shown in the ROC curve above is:\n   Event (E)      0  1     Decision (Y)    0  3953  44    1  807  196     The usual measures:\n Sensitivity = 0.8167 Specificity = 0.8305  We can compute the more clinically relevant probabilties as follows (as for Section 1.2):\n Positive predictive value: \\(\\Pr( E = 1 \\mid Y = 1 )\\) = 0.195 Negative predictive value: \\(\\Pr( E = 0 \\mid Y = 0 )\\) = 0.989  Critically, however, false negatives (44) are catastrophic here because the event, although rare, is serious (i.e.¬†the death of a patient); but \\(X_1\\) is correctly identifying a high number (3953) of negative cases correctly.\nAs discussed here and more persuasively here sensitivity and specificity do not take account of the loss or utility of the decision and neither do PPV and NPV.\nTo understand why this neglect of utility (or loss) is important, take the above confusion matrix and then assume the decision rule declares one additional false negative, so that:\n the number of false negatives (FN) = 45 the actual number of positive events is of course, unchanged at 240 so conversely, the true positive rate drops by one so TP = 195 the revised sensitivity is then 0.8125 ‚Äì a decrease in decision rule performance of 0.0042  Reverse the experiment, so that the decision rule improves marginally and declares one additional true positive:\n the number of true positives increases by one, TP = 197 the number of false negatives decreases by one, FN = 43 the revised sensitivity is then 0.8208 ‚Äì an increase in decision rule performance of 0.0041  The change in performance (the ‚Äúscore‚Äù) for one additional correct or incorrectly classified positive case is symmetric and of the order \\(1/N\\), where \\(N\\) is the sample size.\nClearly, an additional false negative should penalise the overall performance score differently than the reward for an additional true positive.\nOptimising the threshold (decision rule) by maximising the sensitivity-specificity tradeoff (e.g.¬†using the Yourdon J statistic) is not the only method of choosing the threshold and we might for example, choose a decision rule that favours performance of different cells of the confusion matrix. A decision theoretic framework like net benefit allows one systematic treatment.\n  4 Decision Curve Analysis Here, we are trying to implement a decision rule whereby a patient is triaged to a more costly ‚Äútest‚Äù (\\(X_2\\)) on the basis of a more available or less costly test \\(X_1\\).\nWe can adopt a decision-theoretic approach (see previous posts here) and design a loss (conversely, a utility) function for a decision rule (threshold).\nAssume that we continue to insist on a ‚Äúhard‚Äù decision rule that decides, on the basis of \\(F_{X_1}\\), whether to further investigate (triage to \\(X_2\\)) or, decide that the serious event is unlikely so no further follow-up is necessary and the patient can be discharged.\nIn this situation, we can construct the confusion matrix below:\n   Event (E)      0  1     Decision (Y)    0  TN  FN    1  FP  TP     And then assign a loss to each cell e.g.¬†the loss for a true negative is \\(L_{TN}\\), for a false negative \\(L_{FN}\\) and so on.\nFor a given decision rule (here, the decision rule can be equated with the threshold value \\(\\tau\\) over \\(F_{X_1}\\)) we can compute the expected loss: \\[ L( \\tau ) = \\frac{1}{N} \\left( \\#TN \\cdot L_{TN} + \\#FN \\cdot L_{FN} + \\#FP \\cdot L_{FP} + \\#TP \\cdot L_{TP} \\right) \\]\nwhere \\(\\#TN\\) is the number of true negatives under the decision rule \\(\\tau\\) etc. and \\(N\\) is the sample size. We then systematically vary \\(\\tau\\) and choose our final decision rule on the basis of minimum loss.\nThe difficulty is that it is often hard to quantify losses (or value, utility) either absolutely or relatively for each cell of the confusion matrix ‚Äì the example of Kessler et al examines predictive models for suicide, where a false negative would be catastrophic; is the loss incurred for a false negative 10, 100 or 1000 times ‚Äòworse‚Äô than a true negative ?\nAn alternative, proposed by Vickers, Van Calster, and Steyerberg (2016), is to first set up a decision tree representing decisions to intervene / not intervene for the combinations shown in the standard confusion matrix. We then assume that the loss of not intervening when we should (a false negative) is fixed at unity and the loss of a false positive is defined relative to this for a given threshold \\(\\tau\\). After some algebra, the loss attributable to a false positive is: \\[ \\frac{\\tau}{1-\\tau} \\] Then, the net benefit of a decision rule (value of \\(\\tau\\)) is then: \\[ NB(\\tau) = \\frac{\\#TP}{N} - \\frac{\\#FP}{N} \\left( \\frac{\\tau}{1-\\tau} \\right) \\] In this equation, true positives are weighted one, and false positives weighted \\(\\tau /( 1-\\tau)\\). As the cost of a false positive is a function of the threshold we can deduce the relative costs. For example, if \\(\\tau = 1/3\\), the cost of a false positive is half the cost of a true positive.\nA decision curve is then the plot of \\(NB(\\tau)\\) against \\(\\tau\\) as follows:\nThe solid black line is the net benefit of the model \\(F_{X_1}\\) at each threshold level. The grey horizontal line (at \\(NB(\\tau) = 0\\)) is the net benefit of assuming all patients are negative. The black dotted line is the net benefit of the decision rule: ‚Äúassume all patients are positive and intervene‚Äù which is of course wasteful, but offers comparison to the net benefit of each decision rule \\(\\tau\\). The red solid vertical line shows the threshold located by maximising the sensitivity/specificity tradeoff in Section 3.1. Finally, the red dashed line identifies the threshold at which net benefit departs (exceeds) the ‚Äúassume all positive‚Äù line.\nThe region for which \\(NB(\\tau)\\) is greater than zero are the thresholds for which the model outperforms ‚Äúassume all patients are negative.‚Äù An advantage of decision curve analysis is that one can vary \\(\\tau\\) and see the relationship between the model performance and a default strategy of assuming everyone requires intervention ‚Äì the point at which the black solid line departs from the black dotted line.\nOf note, decision curve analysis is not intended to be a method of locating a threshold; in fact, Vickers and Elkin (2006) discuss the method in the context of shared decision making where the clinician and patient‚Äôs prefence for the relative cost of a false positive are factored into deciding the utility of a decision to intervene.\nHowever, as an experiment, let‚Äôs choose a threshold at the point where the net benefit departs from the default ‚Äúassume all patients are positive and intervene‚Äù ‚Äì shown as the red dotted line in the right panel at 0.01. This results in zero false negatives (serious errors), 240 true positives, 506 true negatives and 4252 false positives.\nTo put this in the context of a sequential triage model, 4252 patients who are actually negative would be triaged for the \\(X_2\\) assessment.\n 5 Sequential Triage The proposal Kessler et al put forward is a sequential triage model; above, we have sketched (schematically) the two-stage approach described here.\nHere, \\(\\mathcal{P}_1\\) is the total sample (of size \\(N_1\\), containing \\(N^{+ve}_{1}\\) and \\(N^{-ve}_{1}\\) positive and negative cases respectively) who have been assessed using \\(X_1\\) and a decision made on the basis of the prediction \\(F_{X_1}\\) and decision rule \\(\\tau_1\\). Cases are then discharged on the basis of the decision; of those discharged, serious errors and appropriate discharges are analogous to the number of false negative \\(\\#FN_{X_1}\\) and true negative \\(\\#TN_{X_1}\\) decisions respectively.\nThose identified as likely positive by the decision \\(\\tau_1\\) form the triaged subset \\(\\mathcal{P}_2\\) of size \\(N_2\\), who proceed to the more resource intensive assessment \\(X_2\\). A similar decision system \\(F_{X_2}\\) with rule \\(\\tau_2\\) then either discharges or (in this example) recommends admission to hospital.\nNote that:\n \\(N_2 = N^{+ve}_2 + N^{-ve}_2\\) ‚Äì the size of triaged set \\(\\mathcal{P}_2\\) depends on the number of actually positive and negative cases triaged. In the sequential arrangement, \\(\\mathcal{P}_2\\) depends on the performance of \\(X_1\\). For example, if a case is incorrectly discharged at \\(X_1\\), then \\(X_2\\) does not have an opportunity to ‚Äòcorrect‚Äô that error, so: \\(N^{+ve}_{2} = N^{+ve}_{1} - \\#FN_{X_1}\\).  With this in mind, we now attempt to define measures of performance in terms safety and efficiency\n5.1 Safety From the discussion in Section 1.2, decision systems with favourable NPV (but poor PPV) might be helpful in screening out candidates at \\(X_1\\) and triaging suspected positive cases to \\(X_2\\).\nWe define a safe decision system as having these properties:\n at each stage (i.e.¬†at \\(X_1\\) and \\(X_2\\)), anyone with a high or uncertain probability of being positive is appropriately triaged serious errors are minimised by not discharging people inappropriately, which means it should minimise false negatives  We‚Äôll define the total safety of the system as a function of the number of serious errors made by both \\(F_{X_1}\\) and \\(F_{X_2}\\): \\[ \\begin{aligned} S_{Total}(\\tau_1, \\tau_2) \u0026amp;= 1 - \\frac{\\#FN_{X_1}+\\#FN_{X_2}}{N^{+ve}_{1}+N^{+ve}_{2}} \\\\ \u0026amp;= 1 - \\frac{\\#FN_{X_1}+\\#FN_{X_2}}{N^{+ve}_{1}+(N^{+ve}_{1} - \\#FN_{X_1})} \\\\ \u0026amp;= 1 - \\frac{\\#FN_{X_1}+\\#FN_{X_2}}{2N^{+ve}_{1} - \\#FN_{X_1}} \\end{aligned} \\] As a concrete example (with respect to the diagram above and using the same validation set used in the decision curve and ROC analysis above):\n \\(\\mathcal{P}_1\\) is of size \\(N_1\\) = 5000 with \\(N^{+ve}_1\\) = 240 actual positive and \\(N^{-ve}_1\\) = 4760 actual negative cases  After administering \\(X_1\\), using the threshold \\(\\tau_1 = 0.10\\) for \\(F_{X_1}\\) results in:\n 80 serious errors or inappropriate discharges equal to false negatives, \\(\\#FN_{X_1}\\) 4321 appropriate discharges ‚Äì equal to the true negatives, \\(\\#TN_{X_1}\\) Resulting in \\(N_2\\) = 599 cases triaged into \\(\\mathcal{P_2}\\) equating to the sum of false and true positive cases (i.e.¬†those declared positive by \\(F_{X_1}\\) with \\(\\tau\\) = 0.10) Of these 599 cases, \\(N^{+ve}_2\\) = 160 and \\(N^{-ve}_2\\) = 439  Now, let‚Äôs assume \\(\\tau_2 = 0.36\\), for \\(F_{X_2}\\) applied to \\(\\mathcal{P}_2\\). We arrive at:\n 57 serious errors, equating to \\(\\#FN_{X_2}\\) 430 appropriate discharges, the true negatives \\(\\#TN_{X_2}\\) Resulting in \\(N_3\\) = 112 cases which will be admitted, consisting of \\(N^{+ve}_3\\) = 103 actually positive cases (appropriate admissions) and \\(N^{-ve}_3\\) = 9 actually negative cases (inappropriate admissions)  Substituting these numbers in the equation above for safety:\n\\[ \\begin{aligned} S_{Total}(\\tau_1, \\tau_2) \u0026amp;= 1 - \\frac{\\#FN_{X_1}+\\#FN_{X_2}}{2N^{+ve}_{1} - \\#FN_{X_1}} \\\\ \u0026amp;= 1 - \\frac{80 + 57}{480 - 80} \\\\ \u0026amp;= 0.6575 \\end{aligned} \\] If the decisions made by \\(F_{X_1}\\) and \\(F_{X_2}\\) resulted in no serious errors, we would have zero false negatives and \\(S_{Total}\\) would attain a maximum of one.\n 5.2 Efficiency Now consider efficiency defined as the ratio of useful product to resource consumed.\nHere, the denominator ‚Äì resource consumption ‚Äì is as follows:\n all patients \\(N_1\\) will have \\(X_1\\) administered and be passed through the prediction model \\(F_{X_1}\\), so resource consumed is \\(N_1\\) a subset of patients declared positive by the decision rule \\(\\tau_1\\) are triaged to \\(\\mathcal{P}_2\\) which is composed of \\(N^{+ve}_2\\) and \\(N^{-ve}_2\\) actual positive and negative cases respectively all patients in \\(\\mathcal{P}_2\\) are administed \\(X_2\\) ‚Äì so resource consumed is \\(N_2\\)  The numerator ‚Äì useful product ‚Äì needs elaboration. First consider the efficiency of \\(\\tau_1\\), which will be perfectly efficient if all actual negative cases are discharged (and do not end up in \\(\\mathcal{P_2}\\)) and all positive cases are triaged.\n\\[ E_{X_1}( \\tau_1 ) = \\frac{\\#TP_{X_1} + \\#TN_{X_1}}{N_1} \\] Which attains a maximum efficiency of one when \\(\\#TN_{X_1} = N^{-ve}_{1}\\) and \\(\\#TP_{X_1} = N^{+ve}_{1}\\) (recall that \\(N_1 = N^{+ve}_1 + N^{-ve}_2\\))\nA similar definition holds for \\(X_2\\) :\n\\[ E_{X_2}( \\tau_2 ) = \\frac{\\#TP_{X_2} + \\#TN_{X_2}}{N_2} \\] And we allow \\(X_1\\) and \\(X_2\\) to contribute equally to a total efficiency in the range \\([0,1]\\) defined as: \\[ E_{Total}( \\tau_1, \\tau_2 ) = \\frac{1}{2} \\left[ E_{X_1}( \\tau_1 ) + E_{X_2}( \\tau_2 ) \\right] \\]\n 5.3 Performance of Sequential Triage The plots show safety (left) and efficiency (right) in contours of size 0.1. So, if \\(\\tau_1\\) is less than around 0.05 and \\(\\tau_2\\) is less than approximately 1.5, the sequential triage system has overall safety \u0026gt; 0.9.\nHowever, the tension is that: for the same range of decision thresholds, the efficiency can reach a maximum of 0.747 but this results in upto 31 serious errors.\n  6 Concluding Remarks A few observations:\nThere is no clear way of robustly setting the decision rules \\(\\tau_1\\) and \\(\\tau_2\\) based on performance measures for sequential triage unless one is prepared to accept that efficiency and safety trade-off Using decision curve analysis properly invites setting the decision threshold according to a tradeoff that rightly involves how much risk the patient and clinician want to take: i.e.¬†net benefit provides a weight to false positives (harm attributable to intervening when it is unnecessary) as a function of the decision threshold. The analyses above were all conducted on the improbably ideal situation where a) we had 5000 exemplars to train a system on, both with measurements/assessments \\(X_1\\) and \\(X_2\\) b) we had access to another ‚Äòbatch‚Äô of 5000 exemplars to validate the model on which we attempt to define performance and optimise the system‚Äôs performance When deploying this system, at best, patients arrive in small batches or singularly (certainly not in clusters of 100s or 1000s) ‚Äì the above analyses are then at best, informative for an actuarial or economic analysis if we were forced to choose a decision rule to evaluate the triage system‚Äôs performance  By their very nature, predictions \\(F_{X_1}\\) and \\(F_{X_2}\\) are probabilistic and discrete thresholds coerce these uncertain forecasts into definitive decisions. It seems unlikely that in the case of rare, serious events anyone would rely on a decision support system that gave discrete answers (and indeed, decision curve analysis emphasises the role of clinician expertise and patient preference in deciding on the intervention threshold)\n References Altman, Douglas G, and J Martin Bland. 1994. ‚ÄúStatistics Notes: Diagnostic Tests 2: Predictive Values.‚Äù Bmj 309 (6947): 102.  Blitzstein, Joseph K, and Jessica Hwang. 2019. Introduction to Probability. CRC Press.  Kessler, Ronald C, Robert M Bossarte, Alex Luedtke, Alan M Zaslavsky, and Jose R Zubizarreta. 2019. ‚ÄúSuicide Prediction Models: A Critical Review of Recent Research with Recommendations for the Way Forward.‚Äù Molecular Psychiatry, 1‚Äì12.  Vickers, Andrew J, and Elena B Elkin. 2006. ‚ÄúDecision Curve Analysis: A Novel Method for Evaluating Prediction Models.‚Äù Medical Decision Making 26 (6): 565‚Äì74.  Vickers, Andrew J, Ben Van Calster, and Ewout W Steyerberg. 2016. ‚ÄúNet Benefit Approaches to the Evaluation of Prediction Models, Molecular Markers, and Diagnostic Tests.‚Äù BMJ 352. https://doi.org/10.1136/bmj.i6.    ","date":1595376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595376000,"objectID":"8598048b1365ce1704c91793e68bf7b4","permalink":"/post/2021-05-23-sequential-triage/","publishdate":"2020-07-22T00:00:00Z","relpermalink":"/post/2021-05-23-sequential-triage/","section":"post","summary":"In clinical decision making for serious but rare events, there has been discussion about how to use predictive models as tools in decision making.\nOne example is in decision making for assessment and treatment in people at risk of suicide.","tags":["risk","decision support","thresholds","decision rules","positive predictive value","negative predictive value","decision curve analysis"],"title":"Sequential Clinical Decision Making","type":"post"},{"authors":null,"categories":["R code"],"content":"   In the previous post, loss functions where considered in the context of estimating measures of central tendency for distributions. In this post, I want to look at the computation of loss functions in situations that might arise in a clinical predictive model. This is all textbook stuff ‚Äì see Further Reading ‚Äì but I wanted to summarise it in a way I understood when in a year‚Äôs time, I wonder what the code does.\nI realised that the notation in the last post was sloppy, so for this post, I‚Äôll adopt the conventions in (Berger 1985).\nThe basic setup is:\n There is a finite set of available actions, \\(\\mathscr{A} = \\{ a_1, a_2, \\ldots, a_j \\}\\) ‚Äì in the examples that follow, we‚Äôll restrict our attention to a choice between two actions of ‚Äúdo nothing‚Äù or ‚Äúintervene/treat‚Äù respectively but there is no loss of generality in assuming this. There are uncertain quantities representing ‚Äústates of the world‚Äù \\(\\Theta = \\{ \\theta_1, \\theta_2, \\ldots, \\theta_i \\}\\) about which we can obtain data and that can affect our decision about which action to take. Here, these states will reflect something about a patient.  Then, our task will be to choose the best action from \\(\\mathscr{A}\\) given information about states \\(\\Theta\\).\nInformation about the states \\(\\Theta\\) will come from a putative predictive model: as in the previous post, measurements (the ‚Äúinput‚Äù to the model) for a given patient \\(x\\) are given to the predictive model \\(Y = F(x)\\) that delivers scores (the ‚Äúoutput‚Äù) as realisations of \\(Y \\in [0,1]\\). Importantly, for any \\(x\\), we can access samples from the posterior distribution \\(\\pi_{F}(Y|x)\\) (rather than relying on a single point prediction, such as the mean of the posterior).\n1 Setup To begin with, assume the simplest case of there being two states \\(\\Theta = \\{ \\theta_1, \\theta_2 \\}\\) which correspond to a patient being ‚Äúnegative‚Äù or ‚Äúpositive‚Äù (respectively) for some event or outcome. Our repetoire of actions is \\(\\mathscr{A} = \\{ a_1, a_2 \\}\\) representing ‚Äúdo nothing‚Äù and ‚Äúintervene/treat‚Äù respectively. This only serves a pedagogical need when developing the ideas, not because it represents a principled or sound modelling decision.\nFor a single example patient \\(x\\), the output of the model suggests the they are most likely negative (i.e.¬†the probability mass is concentrated near zero):\nlibrary(ggplot2) library(gridExtra) library(kableExtra) library(latex2exp) library(reshape2) # globals for presentation basictheme \u0026lt;- theme_minimal() + theme(axis.title = element_text(size = 14), axis.text = element_text(size = 12), plot.title = element_text(size = rel(1.25), face = \u0026quot;bold\u0026quot;, hjust = 0.5 )) set.seed(3141) range01 \u0026lt;- function(x){(x-min(x))/(max(x)-min(x))} samples \u0026lt;- range01( rgamma( 2000, shape = 2, scale = 2) ) df \u0026lt;- data.frame( y = samples ) ggplot( df, aes( y ) ) + geom_density( fill = \u0026quot;#fa9fb5\u0026quot;) + ylab(\u0026quot;Density\\n\u0026quot;) + xlab(\u0026quot;\\nScore (Y)\u0026quot;) + basictheme We now require a mapping from the samples \\(y \\sim \\pi_{F}(Y|x)\\) to \\(\\Theta\\) because the domain of \\(\\pi_{F}\\) will be the interval \\([0,1]\\) and to get started, we need to ‚Äúquantise‚Äù to two states.\nDefine the distribution over states \\(\\pi_{\\Theta}\\) as follows:\n\\[\\begin{aligned} \\pi_{\\Theta}(\\theta_{1}) \u0026amp;= \\Pr_{\\pi_{F}}( Y \\leq 0.5 ) \\\\ \\pi_{\\Theta}(\\theta_{2}) \u0026amp;= \\Pr_{\\pi_{F}}( Y \u0026gt; 0.5 ) \\end{aligned}\\] So, we basically histogram the samples into two bins either side of 0.5, representing the probability of a patient being negative (\\(\\theta_1\\)) or positive (\\(\\theta_2\\)). A terrible idea, which we will reverse later.\npdfFromSamples \u0026lt;- function(a, b, delta, samples) { H \u0026lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) ) ret \u0026lt;- data.frame( mids = H$mids, freq = H$counts ) ret$P \u0026lt;- ret$freq / sum(ret$freq) return(ret) } pdf.Y \u0026lt;- pdfFromSamples(0,1,delta = 1/2, samples) pdf.Y$theta \u0026lt;- factor( c(1,2) ) pdf.plot \u0026lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + geom_col( fill = \u0026quot;#fa9fb5\u0026quot; ) + scale_x_discrete(labels = pdf.Y$theta ) + xlab(TeX(\u0026quot;$\\\\theta\u0026quot;)) + ylab(TeX(\u0026#39;$\\\\pi(\\\\theta)\u0026#39;)) + basictheme print( pdf.plot ) According to our blunt assignment of states to output from the predictive model, the probability the patient is negative is \\(\\pi_{\\Theta}(\\theta_{1})\\) = 0.921 and positive \\(\\pi_{\\Theta}(\\theta_{2})\\) = 0.079.\nWith this setup, (two actions, two states) we can ‚Äútabulate‚Äù the combinations of actions and states (the Cartesian product: \\(\\Theta \\times \\mathscr{A}\\)):\nIn each ‚Äúcell‚Äù or combination \\((\\theta,a)\\) we then assign a loss \\(L(\\theta,a) \\leq 0\\) which describes the cost incurred for taking action \\(a\\) when the state \\(\\theta\\) obtains. Generally, we will adopt the convention that losses represent costs or penalties for actions with respect to states.\n 2 Example Loss Matrix Equipped with this toy example we assign losses:\nA \u0026lt;- c(\u0026quot;a1:\u0026lt;br\u0026gt;do nothing\u0026quot;,\u0026quot;a2:\u0026lt;br\u0026gt;intervene\u0026quot;) Theta \u0026lt;- c(\u0026quot;\u0026lt;b\u0026gt;theta1:\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;negative\u0026quot;, \u0026quot;\u0026lt;b\u0026gt;theta2:\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;positive\u0026quot;) loss.matrix \u0026lt;- matrix( c( 0.0, -0.5, -1.0, 0 ), nrow = 2, ncol = 2, byrow = TRUE) rownames(loss.matrix) \u0026lt;- Theta colnames(loss.matrix) \u0026lt;- A knitr::kable( loss.matrix, format = \u0026quot;html\u0026quot;, align = \u0026#39;c\u0026#39;, full_width = FALSE, position = \u0026#39;c\u0026#39;, escape = FALSE ) %\u0026gt;% kable_styling(position = \u0026quot;center\u0026quot;)    a1:\ndo nothing  a2:\nintervene      theta1:\nnegative  0  -0.5    theta2:\npositive  -1  0.0     As a use example, assume we decide \\(a_2\\) (intervene) and the state of the patient turns out to be \\(\\theta_1\\) (negative) we incur a loss of \\(L(\\theta_1,a_2) = -0.5\\) to reflect unnecessary costs of e.g.¬†further investigations, inconvenience to the patient etc. If we select \\(a_1\\) and the state is \\(\\theta_1\\) (equating to doing nothing and the patient is negative) we incur zero loss because this was an appropriate action given the circumstances.\n 3 Bayesian Expected Loss Following (Berger 1985), we define the Bayesian expected loss (BEL) for action \\(a_j\\) with respect to the discrete distribution \\(\\pi_{\\Theta}\\) as \\[ \\rho(\\pi_{\\Theta},a_j) = \\mathbb{E}_{\\pi_{\\Theta}} \\left[ L(\\theta,a_j\\right] = \\sum_{i}L(\\theta_i,a_j)\\pi_{\\Theta}(\\theta_i) \\]\n# Bayesian expected loss (BEL) BEL \u0026lt;- function( a, p.pi, loss.matrix ) { sum( loss.matrix[ , a ] * p.pi ) } # compute BEL for each action a: rho.A \u0026lt;- data.frame( A = factor(c(\u0026quot;a1\u0026quot;,\u0026quot;a2\u0026quot;)), rho = rep(NA,2) ) # for each action for( j in 1:2 ) { rho.A$rho[j] \u0026lt;- BEL( j, pdf.Y$P, loss.matrix ) } bel.plot \u0026lt;- ggplot( rho.A, aes(x = A, y = rho) ) + geom_col( fill = \u0026quot;#d6604d\u0026quot; ) + basictheme grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 ) Note that the upper bound of the BEL is zero.\n 4 Conditional Bayes Decision Principal Having established the BEL for each action, the conditional bayes decision principle (CBD) for deciding on an action (Berger 1985) is:\n choose \\(a_{j} \\in \\mathscr{A}\\) such that \\(a_j\\) minimises the BEL : \\(\\underset{j}{\\mathrm{arg\\,max}} \\; \\rho( \\pi_{\\Theta}, a_j )\\)  In code: the resulting vector for \\(\\rho( \\pi_{\\Theta}, a )\\)\nknitr::kable( rho.A, format = \u0026quot;html\u0026quot;, align = \u0026#39;c\u0026#39;, full_width = FALSE, position = \u0026#39;c\u0026#39;, escape = FALSE ) %\u0026gt;% kable_styling(position = \u0026quot;center\u0026quot;)   A  rho      a1  -0.0790    a2  -0.4605     And the action that minimises the BEL:\nprint( min.bel.CBD \u0026lt;- which.max( rho.A$rho ) ) ## [1] 1 In the example above, we find the action 1 (i.e.¬†\\(a_1\\) = ‚Äúdo nothing‚Äù) minimises the BEL. This fits with our intuition given the patient is most likely negative: \\(\\pi_{\\Theta}(\\theta_1) \u0026gt; \\pi_{\\Theta}(\\theta_1)\\).\n 5 Developing the Loss Function Consider a different patient where the posterior distribution \\(\\pi_{F}(Y|x)\\), the output of the predictive model, looks like:\nsamples \u0026lt;- range01( c(rnorm( 1000, mean = 0, sd = 2 ), rnorm( 1000, mean = 10, sd = 3) ) ) df \u0026lt;- data.frame( y = samples ) ggplot( df, aes( y ) ) + geom_density( fill = \u0026quot;#fa9fb5\u0026quot;) + ylab(\u0026quot;Density\\n\u0026quot;) + xlab(\u0026quot;\\nScore (Y)\u0026quot;) + basictheme In this example, there‚Äôs uncertainty about the patient being negative or positive.\nThis time, we‚Äôll quantise into three equal-sized intervals over the range \\([0,1]\\) (again, an unprincipled decision made only for demonstration) and map to three states:\npdf.Y \u0026lt;- pdfFromSamples(0,1,delta=1/3,samples) pdf.Y$theta \u0026lt;- factor( seq(1,3,by=1) ) pdf.plot \u0026lt;- ggplot( pdf.Y, aes( x = theta, y = P ) ) + geom_col( fill = \u0026quot;#fa9fb5\u0026quot; ) + scale_x_discrete(labels = pdf.Y$theta ) + xlab(TeX(\u0026quot;$\\\\theta\u0026quot;)) + ylab(TeX(\u0026#39;$\\\\pi(\\\\theta)\u0026#39;)) + basictheme print( pdf.plot ) The loss matrix will now be a \\(3 \\times 2\\) matrix:\nA \u0026lt;- c(\u0026quot;a1:\u0026lt;br\u0026gt;do nothing\u0026quot;,\u0026quot;a2:\u0026lt;br\u0026gt;intervene\u0026quot;) Theta \u0026lt;- c(\u0026quot;\u0026lt;b\u0026gt;theta1:\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;negative\u0026quot;, \u0026quot;\u0026lt;b\u0026gt;theta2:\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;equivocal\u0026quot;, \u0026quot;\u0026lt;b\u0026gt;theta3:\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;positive\u0026quot;) loss.matrix \u0026lt;- matrix( c( 0.0, -0.5, -0.5, -0.2, -1.0, 0 ), nrow = 3, ncol = 2, byrow = TRUE) rownames(loss.matrix) \u0026lt;- Theta colnames(loss.matrix) \u0026lt;- A knitr::kable( loss.matrix, format = \u0026quot;html\u0026quot;, align = \u0026#39;c\u0026#39;, full_width = FALSE, position = \u0026#39;c\u0026#39;, escape = FALSE ) %\u0026gt;% kable_styling(position = \u0026quot;center\u0026quot;)    a1:\ndo nothing  a2:\nintervene      theta1:\nnegative  0.0  -0.5    theta2:\nequivocal  -0.5  -0.2    theta3:\npositive  -1.0  0.0     Bearing in mind that states are uncertain, the logic behind this loss matrix is as follows:\nFor \\(a_1\\) (do nothing) : no cost is incurred if the patient is likely negative (\\(\\theta_1\\)). If the patient is most likely positive (\\(\\theta_3\\)) and we do nothing, this is evidently the wrong decision and we incur the maximum penalty of -1.0. If there is some equivocation \\((\\theta_2\\)) ‚Äì we penalise by half the maximum cost to discourage doing nothing (equating to loss aversion)\n For \\(a_2\\) (intervene) : for \\(\\theta_1\\) we incur a cost (-0.5) for intervening when unnecessary. Naturally, for \\(\\theta_3\\), the correct thing to do is intervene so this has no penalty associated. For the equivocal case, \\(\\theta_2\\), we should certainly not ignore these cases but simply intervening (i.e.¬†with zero penalty) is inappropriate. So we incur a small penalty (-0.2) to nudge us away from intervening as the default.\n  Notice that in designing the loss matrix, we are trying to capture domain knowledge about the deployment of the model ‚Äì for example, the loss attached to doing nothing (when there is equivocation about the negative/positive state of the patient) pushes us to be cautious and intervene.\nLet‚Äôs look at the resulting BEL:\n# compute BEL for each action a: rho.A \u0026lt;- data.frame( A = factor(c(\u0026quot;a1\u0026quot;,\u0026quot;a2\u0026quot;)), rho = rep(NA,2) ) # for each action for( j in 1:2 ) { rho.A$rho[j] \u0026lt;- BEL( j, pdf.Y$P, loss.matrix ) } bel.plot \u0026lt;- ggplot( rho.A, aes(x = A, y = rho) ) + geom_col( fill = \u0026quot;#d6604d\u0026quot; ) + basictheme grid.arrange( pdf.plot, bel.plot, nrow = 1, ncol = 2 ) knitr::kable( rho.A, format = \u0026quot;html\u0026quot;, align = \u0026#39;c\u0026#39;, full_width = FALSE, position = \u0026#39;c\u0026#39;, escape = FALSE ) %\u0026gt;% kable_styling(position = \u0026quot;center\u0026quot;)   A  rho      a1  -0.35125    a2  -0.30530     print( min.bel.CBD \u0026lt;- which.max( rho.A$rho ) ) ## [1] 2 The action that minimises \\(\\rho(\\pi_{\\Theta},a)\\) is \\(a_2\\) ‚Äì as can be seen, the probability mass for \\(\\theta_2\\) and \\(\\theta_3\\) (and the associated losses) is driving the decision to intervene i.e.¬†be cautious.\nWe can continue introducing more and more granularity in quantising the posterior predictions \\(\\pi_{F}(Y|x)\\) to arrive at mappings to states \\(\\Theta\\) and then specifying individual losses in the corresponding rows of the loss matrix. Instead, we‚Äôll specify a loss function (although for coding convenience, we‚Äôll continue with a matrix representation).\nSigmoid \u0026lt;- function( x, A, B, m, s ) { # x = vector of values # A = height of sigmoid # B = translation on y axis # m = value of x for which Sigmoid() = half max value # s = steepness of linear component exp.x \u0026lt;- exp( -(x-m)/s ) return( ( A + B * (1+exp.x) ) / (1+exp.x) ) } # plots to compare the quantised states to a more fine-grained version pdf.Y1 \u0026lt;- pdfFromSamples(0, 1, delta= 1/50, samples) pdf.plot.Y1 \u0026lt;- ggplot( pdf.Y1, aes( x = mids, y = P ) ) + geom_col( fill = \u0026quot;#fa9fb5\u0026quot; ) + xlab(TeX(\u0026quot;$\\\\theta\u0026quot;)) + ylab(TeX(\u0026#39;$\\\\pi(\\\\theta)\u0026#39;)) + basictheme pdf.Y2 \u0026lt;- pdfFromSamples(0, 1, delta= 1/3, samples) pdf.Y2$Theta \u0026lt;- factor(c(\u0026quot;theta1\u0026quot;,\u0026quot;theta2\u0026quot;,\u0026quot;theta3\u0026quot;)) pdf.plot.Y2 \u0026lt;- ggplot( pdf.Y2, aes( x = Theta, y = P ) ) + geom_col( fill = \u0026quot;#fa9fb5\u0026quot; ) + scale_x_discrete(labels = pdf.Y2$theta ) + xlab(TeX(\u0026quot;$\\\\theta\u0026quot;)) + ylab(TeX(\u0026#39;$\\\\pi(\\\\theta)\u0026#39;)) + basictheme # loss functions for a1 and a2 loss.fun.a1 \u0026lt;- Sigmoid(pdf.Y1$mids, A = -1.0, B = 0, m = 0.5, s = 0.15 ) loss.fun.a2 \u0026lt;- Sigmoid(pdf.Y1$mids, A = 0.5, B = -0.5, m = 0.3, s = 0.08 ) # build a tabular version of loss function loss.fun \u0026lt;- data.frame( Theta = pdf.Y1$mids, L.a1 = loss.fun.a1, L.a2 = loss.fun.a2 ) # show the loss function and 3 state quantised loss matrix loss.fun.plot \u0026lt;- ggplot( loss.fun, aes( x = Theta ) ) + geom_line( aes( y = L.a1 ), colour = \u0026quot;#fc8d59\u0026quot;, size = 1) + annotate( \u0026quot;label\u0026quot;, x = 0.9, y = -0.75, label = \u0026quot;a1\u0026quot; ) + geom_line( aes( y = L.a2 ), colour = \u0026quot;#91bfdb\u0026quot;, size = 1 ) + annotate( \u0026quot;label\u0026quot;, x = 0.9, y = -0.15, label = \u0026quot;a2\u0026quot; ) + ylab(\u0026quot;Loss\u0026quot;) + xlab(\u0026quot;\\nTheta\u0026quot;) + basictheme df.loss.matrix \u0026lt;- data.frame( Theta = factor( c(\u0026quot;theta1\u0026quot;,\u0026quot;theta2\u0026quot;,\u0026quot;theta3\u0026quot;) ), L.a1 = loss.matrix[,1], L.a2 = loss.matrix[,2] ) loss.matrix.plot \u0026lt;- ggplot( df.loss.matrix ) + geom_line( aes( x = Theta, y = L.a1, group = 1), colour = \u0026quot;#fc8d59\u0026quot;, size = 1) + geom_point( aes( x = Theta, y = L.a1, group = 1), colour = \u0026quot;#fc8d59\u0026quot;, size = 4) + annotate( \u0026quot;label\u0026quot;, x = 2.8, y = -0.7, label = \u0026quot;a1\u0026quot; ) + geom_line( aes( x = Theta, y = L.a2, group= 1), colour = \u0026quot;#91bfdb\u0026quot;, size = 1.5 ) + geom_point( aes( x = Theta, y = L.a2, group= 1), colour = \u0026quot;#91bfdb\u0026quot;, size = 4 ) + annotate( \u0026quot;label\u0026quot;, x = 2.8, y = -0.2, label = \u0026quot;a2\u0026quot; ) + ylab(\u0026quot;Loss\u0026quot;) + xlab(\u0026quot;\\nTheta\u0026quot;) + basictheme grid.arrange( pdf.plot.Y2, pdf.plot.Y1, loss.matrix.plot, loss.fun.plot, nrow = 2, ncol = 2 ) In the figure above, we show the three-state loss matrix underneath the distribution \\(\\pi_{\\Theta}\\) (the lines are to emphasise the trend in losses as we proceed from likely negative through positive). On the right, a finer-grained representation of the distribution \\(\\pi_{\\Theta} \\approx \\pi_{F}(Y|x)\\) with a sigmoid loss function over \\(\\Theta\\) interpolating between the points in the loss matrix at the extremes (negative, positive) and midpoint (equivocal). Now, we can effectively use the whole of the posterior \\(\\pi_{F}(Y|x)\\) more directly:\n# the loss function x probability pdf.Y1$L.a1 \u0026lt;- pdf.Y1$P * loss.fun$L.a1 pdf.Y1$L.a2 \u0026lt;- pdf.Y1$P * loss.fun$L.a2 # product of the posterior and loss function loss.fun.plot2 \u0026lt;- ggplot( pdf.Y1, aes( x = mids ) ) + geom_line( aes( y = L.a1 ), colour = \u0026quot;#fc8d59\u0026quot;, size = 1.5) + #annotate( \u0026quot;label\u0026quot;, x = 0.9, y = -0.75, label = \u0026quot;a1\u0026quot; ) + geom_line( aes( y = L.a2 ), colour = \u0026quot;#91bfdb\u0026quot;, size = 1.5 ) + #annotate( \u0026quot;label\u0026quot;, x = 0.9, y = -0.15, label = \u0026quot;a2\u0026quot; ) + ylab(\u0026quot;Loss\u0026quot;) + xlab(\u0026quot;\\nTheta\u0026quot;) + basictheme ## The actual BEL # we need a matrix representation of the loss function loss.fun.matrix \u0026lt;- as.matrix( loss.fun[,2:3] ) colnames( loss.fun.matrix ) \u0026lt;- c(\u0026quot;a1\u0026quot;,\u0026quot;a2\u0026quot;) # compute BEL for each action a: rho.A \u0026lt;- data.frame( A = factor(c(\u0026quot;a1\u0026quot;,\u0026quot;a2\u0026quot;)), rho = rep(NA,2) ) # for each action for( j in 1:2 ) { rho.A$rho[j] \u0026lt;- BEL( j, pdf.Y1$P, loss.fun.matrix ) } bel.plot \u0026lt;- ggplot( rho.A, aes(x = A, y = rho) ) + geom_col( fill = \u0026quot;#d6604d\u0026quot; ) + basictheme grid.arrange( pdf.plot.Y1, loss.fun.plot2, bel.plot, ncol = 2, nrow = 2) Above, the top-left panel shows a finer-grained distribution function \\(\\pi_{\\Theta}\\) and the top-right panel shows the loss function for \\(a_1\\) and \\(a_2\\) weighted by \\(\\pi_{\\Theta} \\approx \\pi_{F}(Y|x)\\) ‚Äì rather than the sum for each action as in \\(\\rho(\\theta, a)\\). This exposes that the Bayesian expected loss of an action is the integral over states (equivalently, the sum for discrete distributions) of the product of the loss function for an action in a certain state and the probability of that state. The bottom-left panel shows the resulting BEL where, as expected, \\(a2\\) minimises \\(\\rho(\\theta,a)\\).\n 6 Further Reading If I were to try this again (rather than trying to piece together an understanding from wikipedia), I would proceed in this order:\nStart with (Savage 1951) for foundations/first principles and tutorial approach. First four chapters of (Berger 1985) for a really clear exposition of the core ideas. For decision theory in point estimation from the perspective of sciences concerned with prediction and forecasting: (Gneiting 2011) provides a comprehensive review Risk/decision theory for classification Chapter 2 of (Duda, Hart, and Stork 2012) and Chapter 1.5 of (Bishop 2006).\n Foundations in Bayesian principles more generally: Chapter 2 and Appendix B of (Bernardo and Smith 2009)   References Berger, James O. 1985. Statistical Decision Theory and Bayesian Analysis. 2nd ed. Springer.  Bernardo, Jos√© M, and Adrian FM Smith. 2009. Bayesian Theory. Vol. 405. John Wiley \u0026amp; Sons.  Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.  Duda, Richard O, Peter E Hart, and David G Stork. 2012. Pattern Classification. John Wiley \u0026amp; Sons.  Gneiting, Tilmann. 2011. ‚ÄúMaking and Evaluating Point Forecasts.‚Äù Journal of the American Statistical Association 106 (494): 746‚Äì62.  Savage, Leonard J. 1951. ‚ÄúThe Theory of Statistical Decision.‚Äù Journal of the American Statistical Association 46 (253): 55‚Äì67.    ","date":1585785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585785600,"objectID":"7326d4845607b47d25c5566bca7ca189","permalink":"/post/decisions-and-loss-functions/","publishdate":"2020-04-02T00:00:00Z","relpermalink":"/post/decisions-and-loss-functions/","section":"post","summary":"In the previous post, loss functions where considered in the context of estimating measures of central tendency for distributions. In this post, I want to look at the computation of loss functions in situations that might arise in a clinical predictive model.","tags":["posterior distribution","loss functions","risk","decisions"],"title":"Decisions and Loss Functions - A more clinical focus","type":"post"},{"authors":null,"categories":["R code"],"content":"   For a while, I‚Äôve been thinking about the deployment of predictive algorithms in clinical decision support. Specifically, about the difference between what we understand about a model‚Äôs performance from the publication describing it and how this might be less informative when deployed. In short: what is the value of knowing that a model has good balanced accuracy or a high area under the ROC curve when sat with a patient and using the tool to make a clinical decision.\nFrequently in medical applications of machine learning we see summary measures of performance used to demonstrate the competence of the model typically reported as sensitivities, specificities, balanced accuracy, area under the ROC curve and so on. These measures assume that for an input \\(x\\) (a patient) we get an output \\(Y = F(x)\\) representing for example, ‚Äúnegative‚Äù or ‚Äúpositive‚Äù caseness.\nIf the system \\(F\\) is designed to classify patients, then it will deliver discrete ‚Äòyes / no‚Äô answers e.g.¬†\\(Y \\in \\{0,1\\}\\); canonical examples include the support-vector machine (SVM). Baked-into these algorithms is a decision rule operating over a continuous value; in SVMs, for example, the decision rule classifies each patient by dichotomising the signed distance of \\(x\\) from the class-seperating hyperplane. If using a classifier makes sense in the context of it‚Äôs clinical deployment you might want the algorithm to report a definitive dichotomised answer and summary measures like sensitivity and specificity (perhaps) make sense.\nCompare this with logistic regression (often used as if it were a classifier) where the purpose is to estimate the probability of an event, i.e.¬†that a patient is positive or negative. Often in these cases, the decision rule is ‚Äòbolted on‚Äô and that‚Äôs when people invoke ROC curves and compute accuracies at an operating threshold that maximises the trade-off between sensitivity and specificity e.g.¬†by maximising Youden‚Äôs J-statistic. It‚Äôs here that the decision rule and it‚Äôs deployment context matter.\nIt seems to me that patients and clinicians would probably want more information than a pure classifier provides and most likely would prefer to know the actual continuous score ‚Äì the ‚Äúoutput‚Äù\" of \\(F(x)\\).\nFurther, a patient and clinician might want to understand the decision rule and it‚Äôs assumptions. For example, assume for a deployed predictive model \\(F\\) and a new patient, \\(F(x) = 0.79\\) and this represents (or is proportional to) the likelihood of being a positive case. The predictive model being deployed has an operating threshold of \\(0.80\\) for declaring a positive case and this threshold was determined by maximising the trade-off between sensitivity and specificity. Recall that sensitivity is the ratio \\(TP/(TP+FN)\\) and specificity is the ratio \\(TN/(TN+FP)\\). Amongst other things, I‚Äôd want to know:\n in determining the operating threshold, were true positives and false negatives given equal weight ? For example, in the context of predicting a rare but serious event, a false negative can be orders-of-magnitude more ‚Äòcostly‚Äô than the model correctly determining cases that are true positives or true negatives.\n in the example above, \\(F(x) = 0.79\\) is 0.01 below the operating threshold for declaring a positive case ‚Äì boundary cases near the operating threshold, demand closer inspection and at least, consideration of the uncertainty in the output of \\(F\\).\n  A more elaborate version of this discussion is here and summarised in a short paper.\nSo, the problem of ‚Äòdeclaring‚Äô a prediction can be cast as a problem in statistical decision theory. Being only vaguely familiar with similar ideas from signal detection models in psychophysics (Green and Swets 1966), I decided to deep-dive into the details of using posterior distributions to arrive at decisions via loss and risk functions. I found the ideas are fairly intuitive and are well described e.g.¬†in (Duda, Hart, and Stork 2012) and (Bishop 2006). It was harder for me to operationalise these; for example, I frequently found explainations that the \\(L_0\\) ‚Äúzero-one‚Äù loss function delivers the mode of a (posterior) distribution, but examples of this in action were harder to come by.\n1 Predictions As above, assume we have a model \\(F(x)\\) that delivers a continuous score \\(Y\\) for an outcome/case/event given some ‚Äúinput‚Äù measurement or feature(s), \\(x\\) for a patient. Importantly, the model delivers outputs in the form of posterior probabilities \\(\\pi(Y|x) = \\Pr(Y=y|x)\\); for example, the probability of being a positive case is \\(\\pi(Y=1|x)\\) and \\(\\pi(Y=0|x) = 1 - \\pi(Y=1|x)\\) being the probability of a negative case.\nSo for any patient \\(x\\), we will have a posterior distribution ‚Äì rather than a single point summary.\nThis is an important distinction: after inferring (learning) a model that delivers posterior probabilities, we can then deliberately design and implement a decision process ‚Äì in contrast to solving the related problem of discrimination or classification, where we find a direct mapping from each input \\(x\\) to a discrete, often binary, output and sacrifice an estimate of uncertainty (see Ch. 1.5 of (Bishop 2006) for more detail).\nWe are used to seeing point estimates as outputs from predictive models given some input e.g.¬†\\(\\pi( Y = 1 | x) = 0.78\\) and sometimes, with a measure of uncertainty on that output (for example, the standard error on predicted values from predict.glm() in R).\nThis point value is a summary of the posterior distribution of \\(Y\\) (for example, the mean) and represents the output of a decision making process.\n 2 Setup Assume that for some predictive model, we present a single patient \\(x\\) and we are able to access the posterior distribution of \\(Y\\) given \\(x\\) i.e.¬†we can obtain samples from the posterior distribution for that patient denoted \\(\\pi(Y|x)\\); for example:\n# to ensure reproducible result set.seed(3141) # simulate a bimodal posterior distribution $\\pi(Y|x)$ samples \u0026lt;- range01( c(rnorm( 1000, mean = 0, sd = 1 ), rnorm( 1000, mean = 5, sd = 2) ) ) df \u0026lt;- data.frame( y = samples ) ggplot( df, aes( y ) ) + geom_density( fill = \u0026quot;#fa9fb5\u0026quot;) + ylab(\u0026quot;Density\\n\u0026quot;) + xlab(\u0026quot;\\nScore (Y)\u0026quot;) + basictheme We have 2000 samples from \\(\\pi(Y|x)\\) stored as samples (usually, these samples will be from the posterior predictive distribution obtained by e.g.¬†MCMC sampling):\nround( samples[1:10], 3 ) ## [1] 0.291 0.130 0.271 0.295 0.294 0.373 0.261 0.232 0.177 0.313 and the following code will give us a data.frame that represents an approximation to the probability distribution function as a lookup table (basically, a histogram):\npdfFromSamples \u0026lt;- function(a, b, delta, samples) { H \u0026lt;- hist( samples, plot = FALSE, breaks = seq(a, b, by = delta) ) ret \u0026lt;- data.frame( mids = H$mids, freq = H$counts ) ret$P \u0026lt;- ret$freq / sum(ret$freq) return(ret) } For example, we can examine \\(\\pi(Y|x)\\) in the region \\(Y \\in [0.5,0.6]\\) with a bin-width of delta = 1/50 as follows:\npdf.Y \u0026lt;- pdfFromSamples(0, 1, delta = 1/50, samples ) knitr::kable( pdf.Y[ pdf.Y$mids \u0026gt;= 0.5 \u0026amp; pdf.Y$mids \u0026lt;= 0.6, ], format = \u0026quot;html\u0026quot;, align = \u0026#39;c\u0026#39;, full_width = FALSE, row.names = FALSE ) %\u0026gt;% kable_styling(position = \u0026quot;center\u0026quot;)   mids  freq  P      0.51  57  0.0285    0.53  46  0.0230    0.55  57  0.0285    0.57  62  0.0310    0.59  48  0.0240     With some abuse of notation, we can state that (approximately) \\(\\pi(Y=0.55|x) = 0.0285\\) and \\(\\pi(Y=0.59|x) = 0.0240\\).\n 3 Loss Functions Consider the task of choosing a summary of the information contained in the posterior distribution \\(\\pi(Y|x)\\) as a single (point) value. We can see that the score \\(Y\\) ranges from 0 to 1 and we could potentially pick any one of an infinite number of values as our chosen point summary (of course, some will be meaningful and others less so).\nTo make this concrete, we‚Äôll cheat and look-ahead to the answer. One meaningful decision to summarise \\(\\pi(Y|x)\\) is to choose the mode:\npdf.Y \u0026lt;- pdfFromSamples(0, 1, delta = 1/100, samples ) mode.pdf.Y \u0026lt;- pdf.Y$mids[ order( pdf.Y$freq, decreasing = TRUE) ][1] pdf.plot \u0026lt;- ggplot( pdf.Y, aes( x = mids, y = P ) ) + geom_col( fill = \u0026quot;#fa9fb5\u0026quot; ) + xlab(\u0026quot;Score (Y)\u0026quot;) + ylab(TeX(\u0026#39;$\\\\pi(Y|x)\u0026#39;)) + basictheme mode.plot \u0026lt;- pdf.plot + geom_vline( xintercept = mode.pdf.Y, colour = \u0026quot;black\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Mode = \u0026quot;, mode.pdf.Y ), x = mode.pdf.Y, y = 0.01) print( mode.plot ) Now assume we don‚Äôt have this information but instead, let \\(a_{i}\\) be one such candidate decision for the point summary. The actual point summary is \\(y_{j} \\in [0,1]\\).\nWe define the cost of choosing \\(a_{i}\\) ‚Äì where the true value is \\(y_{j}\\) ‚Äì as the loss function \\(L(a_{i},y_{j})\\). Now, construct a loss function that penalizes any candidate \\(a_{i}\\) by a single unit if \\(a_{i}\\) is not equal to \\(y_{j}\\) (incorrect) and zero if \\(a_i\\) is equal to \\(y_{j}\\); this is the so-called ‚Äúzero-one‚Äù loss function:\n\\[ L_{0}(a_{i},y_{j}) = \\begin{cases} 0 \u0026amp; \\text{if}\\ a_{i} = y_{j} \\\\ 1 \u0026amp; \\text{otherwise} \\end{cases} \\]\nNow, we propose an arbitrary candidate \\(a_{1} = 0.555\\) and we want to know the associated loss over the range of possible values of \\(y_{j}\\):\n# the range of possible values $y_{j}$ y \u0026lt;- pdf.Y$mids # our estimate $a_{i}$ a1 \u0026lt;- 0.555 # the L_{0} loss function: loss0 \u0026lt;- function( y, a ) { ifelse( a == y, 0, 1 ) } # the loss function evaluated over the range $y_{j}$ l0.ex \u0026lt;- data.frame( mids = pdf.Y$mids, loss.ex1 = loss0( y, a1 ) ) loss.plot \u0026lt;- ggplot( l0.ex, aes( x = l0.ex$mids, y = loss.ex1) ) + geom_line( colour = \u0026quot;#636363\u0026quot; ) + xlab(TeX(\u0026#39;$y_{j}$\u0026#39;)) + ylab(TeX(\u0026#39;$L_{0}$\u0026#39;)) + basictheme print( loss.plot ) Let‚Äôs repeat the same process for two other (arbitrarily chosen) candidates \\(a_{2} = 0.095\\) and \\(a_{3} = 0.755\\):\na2 \u0026lt;- 0.095 a3 \u0026lt;- 0.755 # the loss function evaluated over the range $y_{j}$ l0.ex$loss.ex2 \u0026lt;- loss0( y, a2 ) l0.ex$loss.ex3 \u0026lt;- loss0( y, a3 ) loss.plot \u0026lt;- ggplot( l0.ex ) + geom_line( aes( x = mids, y = loss.ex1 ), colour = \u0026quot;#bdbdbd\u0026quot; ) + annotate( geom = \u0026quot;label\u0026quot;, label = TeX(\u0026#39;$a_{1}$\u0026#39;, output=\u0026quot;character\u0026quot;), parse = TRUE, x = a1, y = 0.25) + geom_line( aes( x = mids, y = loss.ex2 ), colour = \u0026quot;#969696\u0026quot; ) + annotate( geom = \u0026quot;label\u0026quot;, label = TeX(\u0026#39;$a_{2}$\u0026#39;, output=\u0026quot;character\u0026quot;), parse = TRUE, x = a2, y = 0.25) + geom_line( aes( x = mids, y = loss.ex3 ), colour = \u0026quot;#636363\u0026quot; ) + annotate( geom = \u0026quot;label\u0026quot;, label = TeX(\u0026#39;$a_{3}$\u0026#39;, output=\u0026quot;character\u0026quot;), parse = TRUE, x = a3, y = 0.25) + xlab(TeX(\u0026#39;$y_{j}$\u0026#39;)) + ylab(TeX(\u0026#39;$L_{0}$\u0026#39;)) + basictheme print( loss.plot ) Clearly using the loss function we get ‚Äòspikes‚Äô when our candidates coincide with a value in the range of \\(y_{j}\\).\nBut, what we care about is loss associated with a candidate with respect to the probability of each possible value \\(Y=y_j\\); so we weight the loss associated with each ‚Äúdecision‚Äù (each candidate \\(a_{i}\\)) by the posterior probability of \\(Y = y_{j}\\):\n\\[ L_0(a_{i},y_{j})\\pi(y_{j}|x) \\]\nThis leads us to consider the risk associated with each candidate. In essence, we want to know the loss associated with \\(a_{i}\\) when the true value is \\(y_{j}\\) weighted by how likely or how frequently we see \\(y_{j}\\).\n 4 Risk Functions We could continue randomly choosing candidates but instead, we‚Äôll be systematic and check all values of \\(a_{i}\\). At the same time, we‚Äôll shift representation and instead of plotting the loss function for each candidate against the range of \\(y_{j}\\), we‚Äôll compute a risk function for each candidate as the expected value of the loss function evaluated for each \\(a_{i}\\):\n\\[ \\mathbb{E}[L_0(a_{i},y_{j})] = \\sum_{j} \\underbrace{L_{0}(a_{i},y_{j})}_\\text{loss} \\underbrace{\\pi(y_{j}|x)}_\\text{posterior} \\]\nThen, instead of \\(y_{j}\\) on the horizontal axis and \\(L_{0}\\) on the vertical, we instead show candidates \\(a_{i}\\) on the horizontal with the risk \\(\\mathbb{E}[L]\\) on the vertical:\n# all candidate values for $a_{i}$ a \u0026lt;- pdf.Y$mids # the range of values for $y_{j}$ y \u0026lt;- pdf.Y$mids pdf.Y$risk.L0 \u0026lt;- rep(NA, nrow(pdf.Y)) for( i in 1:length( a ) ) { # compute risk function at each candidate $a_{i}$ # with reference to the above equation pdf.Y$risk.L0[i] \u0026lt;- sum( # sum over j loss0(y, a[i]) * # loss pdf.Y$P # posterior ) } risk.plot \u0026lt;- ggplot( pdf.Y ) + geom_line( aes( x = mids, y = risk.L0 ), colour = \u0026quot;#636363\u0026quot; ) + xlab(TeX(\u0026#39;$a_{i}$\u0026#39;)) + ylab(TeX(\u0026#39;$E(L_{0})$\u0026#39;)) + basictheme print(risk.plot)  5 Optimal Decision Our question is now: what is the best action ‚Äì or decision ‚Äì over our candidates \\(a_{i}\\) to choose as the point summary given the loss function \\(L_{0}\\) and the posterior \\(\\pi(Y|x)\\)?\nThe answer is, the \\(a_{i}\\) that minimises the risk (expected loss). Implementing this, we arrive at:\n# find the $a_{i}$ that minimises the risk function min.risk.L0 \u0026lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L0 ) ] risk.plot \u0026lt;- risk.plot + geom_vline( xintercept = min.risk.L0, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Minimum = \u0026quot;, min.risk.L0 ), x = mode.pdf.Y, y = 0.99) print( risk.plot ) Compare with the distribution of \\(\\pi(Y|x)\\):\ngrid.arrange(risk.plot, mode.plot, nrow = 2, ncol = 1) The result then, can be summarised as:\n Under the zero-one loss function, \\(L_0\\), the action/decision \\(a_{i}\\) (point summary) which minimises the risk function (expected loss) is the mode of the posterior distribution \\(\\pi(Y|x)\\)   6 Other Loss Functions We can repeat the same process as for section 3 through 5 with different loss functions.\nThe linear loss \\(L_{1}\\) loss is defined as: \\[ L_{1}(a_{i},y_{j}) = \\begin{cases} c_{1} |a_{i} - y_{j}| \u0026amp; \\text{ if } a_{i} \\leq y_{j} \\\\ c_{2} |a_{i} - y_{j}| \u0026amp; \\text{ if } a_{i} \u0026gt; y_{j} \\\\ \\end{cases} \\] Where \\(c_{1}\\) and \\(c_{2}\\) are constants. If \\(c_{1} = c_{2}\\) we arrive at the median of the posterior, as follows:\n# the linear loss function loss1 \u0026lt;- function( y, a, c1, c2 ) { ifelse( a \u0026lt;= y, c1 * abs( a - y ), c2 * abs( a - y ) ) } pdf.Y$risk.L1 \u0026lt;- rep(NA, nrow(pdf.Y)) # set constants equal c1 \u0026lt;- c2 \u0026lt;- 1 for( i in 1:length( a ) ) { # compute risk function at each candidate $a_{i}$ pdf.Y$risk.L1[i] \u0026lt;- sum( loss1(y, a[i], c1, c2) * pdf.Y$P ) } # find the minimum min.lossL \u0026lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1 ) ] The minimum of the risk function is 0.355:\nrisk.plot \u0026lt;- ggplot( pdf.Y ) + geom_line( aes( x = mids, y = risk.L1 ), colour = \u0026quot;#636363\u0026quot; ) + xlab(TeX(\u0026#39;$a_{i}$\u0026#39;)) + ylab(TeX(\u0026#39;$E(L_{1})$\u0026#39;)) + geom_vline( xintercept = min.lossL, colour = \u0026quot;black\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Minimum = \u0026quot;, round( min.lossL, 2) ), x = min.lossL, y = 0.4) + basictheme median.plot \u0026lt;- pdf.plot + geom_vline( xintercept = median( samples ), colour = \u0026quot;black\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Median = \u0026quot;, round( median(samples), 2 ) ), x = median(samples), y = 0.01) grid.arrange(risk.plot, median.plot, nrow = 2, ncol = 1) More generally, for positive constants, the \\(c_{1}/(c_{1}+c_{2})\\) quantile of the posterior distribution can be found. For example, we can obtain the 25th and 75th percentiles:\npdf.Y$risk.L1_75 \u0026lt;- rep(NA, nrow(pdf.Y)) pdf.Y$risk.L1_25 \u0026lt;- rep(NA, nrow(pdf.Y)) # set constants q95 \u0026lt;- 0.75 q05 \u0026lt;- 0.25 c1 \u0026lt;- 1 c2.q95 \u0026lt;- (c1/q95) - c1 c2.q05 \u0026lt;- (c1/q05) - c1 for( i in 1:length( a ) ) { # compute risk function at each candidate $a_{i}$ pdf.Y$risk.L1_75[i] \u0026lt;- sum( loss1(y, a[i], c1, c2.q95) * pdf.Y$P ) pdf.Y$risk.L1_25[i] \u0026lt;- sum( loss1(y, a[i], c1, c2.q05) * pdf.Y$P ) } # find the minima min.lossL_75 \u0026lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_75 ) ] min.lossL_25 \u0026lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L1_25 ) ] risk.plot \u0026lt;- ggplot( pdf.Y ) + geom_line( aes( x = mids, y = risk.L1_75 ), colour = \u0026quot;#1f78b4\u0026quot; ) + geom_line( aes( x = mids, y = risk.L1_25 ), colour = \u0026quot;#33a02c\u0026quot; ) + xlab(TeX(\u0026#39;$a_{i}$\u0026#39;)) + ylab(TeX(\u0026#39;$E(L_{1})$\u0026#39;)) + geom_vline( xintercept = min.lossL_75, colour = \u0026quot;#1f78b4\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Minimum = \u0026quot;, round( min.lossL_75, 3) ), x = min.lossL_75, y = 1.25) + geom_vline( xintercept = min.lossL_25, colour = \u0026quot;#33a02c\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Minimum = \u0026quot;, round( min.lossL_25, 3) ), x = min.lossL_25, y = 1.25) + basictheme fun.q75 \u0026lt;- as.numeric( round( quantile(samples, probs = c(0.75)), 3 ) ) fun.q25 \u0026lt;- as.numeric( round( quantile(samples, probs = c(0.25)), 3 ) ) quantile.plot \u0026lt;- pdf.plot + geom_vline( xintercept = fun.q75, colour = \u0026quot;#1f78b4\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;75th = \u0026quot;, fun.q75 ), x = fun.q75, y = 0.01) + geom_vline( xintercept = fun.q25, colour = \u0026quot;#33a02c\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;25th = \u0026quot;, fun.q25 ), x = fun.q25, y = 0.01) grid.arrange(risk.plot, quantile.plot, nrow = 2, ncol = 1) In the top panel of the figure above, the green and blue lines represents the risk using the linear loss function with constants configured to locate the 25th and 75th percentile respectively. The bottom panel shows the same result obtained directly from the quantile() function operating directly on the raw samples.\nTo complete the loss functions for common measures of central tendency, we have the \\(L_{2}\\) quadratic loss function:\n\\[ L_{2}(a_{i},y_{j}) = (a_{i} - y_{j})^2 \\]\nHere‚Äôs the result for quadratic loss:\n# the quadratic loss function loss2 \u0026lt;- function( y, a ) { return( (a - y)^2 ) } pdf.Y$risk.L2 \u0026lt;- rep(NA, nrow(pdf.Y)) for( i in 1:length( a ) ) { # compute risk function at each candidate $a_{i}$ pdf.Y$risk.L2[i] \u0026lt;- sum( loss2(y, a[i]) * pdf.Y$P ) } # find the minimum min.lossL2 \u0026lt;- pdf.Y$mids[ which.min( pdf.Y$risk.L2 ) ] risk.plot \u0026lt;- ggplot( pdf.Y ) + geom_line( aes( x = mids, y = risk.L2 ), colour = \u0026quot;#636363\u0026quot; ) + xlab(TeX(\u0026#39;$a_{i}$\u0026#39;)) + ylab(TeX(\u0026#39;$E(L_{2})$\u0026#39;)) + geom_vline( xintercept = min.lossL2, colour = \u0026quot;black\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Minimum = \u0026quot;, round( min.lossL2,3) ), x = min.lossL2, y = 0.25) + basictheme mean.plot \u0026lt;- pdf.plot + geom_vline( xintercept = mean( samples ), colour = \u0026quot;black\u0026quot;, size = 2 ) + annotate( geom = \u0026quot;label\u0026quot;, label = paste0( \u0026quot;Mean = \u0026quot;, round( mean( samples ), 3 ) ), x = mean(samples), y = 0.005) grid.arrange(risk.plot, mean.plot, nrow = 2, ncol = 1) The estimate from quantile() plotted on the bottom panel differs from the top panel (the mean estimated by minimising the risk function) due to the granularity (delta = 1/100) of the approximation of the distribution function used in the code.\n References Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.  Duda, Richard O, Peter E Hart, and David G Stork. 2012. Pattern Classification. John Wiley \u0026amp; Sons.  Green, David Marvin, and John A Swets. 1966. Signal Detection Theory and Psychophysics. Wiley New York.    ","date":1584748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584748800,"objectID":"9c0025cfbd1514202e0a5cf3c937547e","permalink":"/post/loss-functions-and-posteriors/","publishdate":"2020-03-21T00:00:00Z","relpermalink":"/post/loss-functions-and-posteriors/","section":"post","summary":"For a while, I‚Äôve been thinking about the deployment of predictive algorithms in clinical decision support. Specifically, about the difference between what we understand about a model‚Äôs performance from the publication describing it and how this might be less informative when deployed.","tags":["posterior distribution","loss functions","risk"],"title":"Loss, Risk and Point Summaries of Posterior Distributions","type":"post"},{"authors":["Evangelos Papanastasiou","Elias Mouchlianitis","Dan W. Joyce","Philip McGuire","Celia Boussebaa","Tobias Banaschewski","Arun L. W. Bokde","Christian B√ºchel","Erin Quinlan","Sylvane Desrivi√®res","Herta Flor","Antoine Grigis","Hugh Garavan","Philip Spechler","Penny Gowland","Andreas Heinz","Bernd Ittermann","Marie-Laure Paill√®re Martinot","Eric Artiges","Frauke Nees","Dimitri Papadopoulos Orfanos","Tom√°≈° Paus","Luise Poustka","Sabina Millenet","Juliane H. Fr√∂hner","Michael N. Smolka","Henrik Walter","Robert Whelan","Gunter Schumann","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"05a5d840755fdfe47b11ada84e7758fa","permalink":"/publication/papanastasiou-examination-2020/","publishdate":"2020-04-05T17:39:33.998111Z","relpermalink":"/publication/papanastasiou-examination-2020/","section":"publication","summary":"Contemporary theories propose that dysregulation of emotional perception is involved in the aetiology of psychosis. 298 healthy adolescents were assessed at age 14- and 19-years using fMRI while performing a facial emotion task. Psychotic-like experiences (PLEs) were assessed with the CAPE-42 questionnaire at age 19. The high PLEs group at age 19 years exhibited an enhanced response in right insular cortex and decreased response in right prefrontal, right parahippocampal and left striatal regions; also, a gradient of decreasing response to emotional faces with age, from 14 to 19 years, in the right parahippocampal region and left insular cortical area. The right insula demonstrated an increasing response to emotional faces with increasing age in the low PLEs group, and a decreasing response over time in the high PLEs group. The change in parahippocampal/amygdala and insula responses during the perception of emotional faces in adolescents with high PLEs between the ages of 14 and 19 suggests a potential 'aberrant' neurodevelopmental trajectory for critical limbic areas. Our findings emphasize the role of the frontal and limbic areas in the aetiology of psychotic symptoms, in subjects without the illness phenotype and the confounds introduced by antipsychotic medication.","tags":null,"title":"Examination of the neural basis of psychotic-like experiences in adolescence during processing of emotional faces","type":"publication"},{"authors":["Bruno B. Ortiz","Cinthia H. Higuchi","Cristiano Noto","Dan W. Joyce","Christoph U. Correll","Rodrigo A. Bressan","Ary Gadelha"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"0afa7ccc86ce5499520349f62c039f6f","permalink":"/publication/ortiz-symptom-2020/","publishdate":"2020-04-05T17:39:34.007952Z","relpermalink":"/publication/ortiz-symptom-2020/","section":"publication","summary":"Early identification of symptoms that can predict treatment-resistant schizophrenia (TRS) could help clinicians to avoid delays in clozapine therapy. This study aims to investigate symptom patterns that could predict TRS using a discovery/replication study design. First, we followed a cohort of inpatients with schizophrenia (n¬†=¬†164) in which the most discriminative items at baseline of the Positive and Negative Syndrome Scale (PANSS) were determined using logistic regression with TRS status as an outcome. Using Receiver Operating Characteristic (ROC) curves, we tested the prediction performance of multiple combinations of the identified items. The same items' combination was tested in an independent replication sample of (n¬†=¬†207) outpatients with schizophrenia. In the discovery sample, the best combination to predict TRS at the discharge was the sum of three baseline PANSS items ‚Äì conceptual disorganization (P2), difficulty in abstract thinking (N5), and unusual thought content (G9). The P2¬†+¬†N5¬†+¬†G9 model yielded an area under the curve (AUC) of 0.881, a sensitivity of 77.8%, and a specificity of 83.3%. In the outpatient sample, the model P2¬†+¬†N5¬†+¬†G9 predictive accuracy for TRS was only in the range of ‚Äúacceptable‚Äù with an AUC of 0.756 and sensitivity of 72.3% and a specificity of 74.4%. Overall, the P2¬†+¬†N5¬†+¬†G9 model corresponds to the construct of formal thought disorder composed of disorganized thinking, concrete thinking, and bizarre-idiosyncratic thinking. Pronounced levels of these symptoms are easily identifiable in clinical practice and may be a feasible strategy in TRS. Replicating in first-episode cohorts is desirable to understand the likely clinical utility.","tags":["Clozapine","Predictors","Schizophrenia","Treatment-resistant schizophrenia"],"title":"A symptom combination predicting treatment-resistant schizophrenia ‚Äì A strategy for real-world clinical practice","type":"publication"},{"authors":["Dan W. Joyce","John Geddes"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"29bd9998d2a590031fb4561044edb1a4","permalink":"/publication/joyce-when-2020/","publishdate":"2020-04-05T17:39:34.007652Z","relpermalink":"/publication/joyce-when-2020/","section":"publication","summary":"The last decade‚Äôs growth in artificial intelligence, machine learning, and statistical methods for high-dimensional data has driven a zeitgeist of prediction (or forecasting) in medicine and psychiatry. Algorithms for prediction require a model that is governed by parameters whose values are estimated from exemplar training cases. Estimation (or training) of parameters ingrains uncertainty into the resulting algorithm arising from model assumptions in addition to bias and error in the data. The trained algorithm‚Äôs proficiency is tested on separate validation cases (not seen during training) and summarized as representative of the expected performance when used for making predictions about actual patients. The trained model yields a continuous score that is proportional to the probability of some outcome, commonly a diagnosis or the occurrence of an event. Most often, this continuous score is compared with an operating threshold (or cutoff) that implicitly defines a dichotomizing decision rule because this is compatible with summary measures of performance (SMP) such as the area under the receiver operating characteristic curve (AUROC), sensitivity/specificity, and balanced accuracy. Sometimes, the continuous scores are instead summarized as the Brier score, ranging from 0 (perfect) to 1 (worst). In this Viewpoint, we discuss an important but neglected issue: summary measures of performance obscure uncertainty in the algorithm‚Äôs predictions that may be relevant when deployed for clinical decision-making.","tags":null,"title":"When Deploying Predictive Algorithms, Are Summary Performance Measures Sufficient?","type":"publication"},{"authors":[],"categories":["electronic health records","workflows","practices"],"content":"The motivation for this post is a series of discussions I\u0026rsquo;ve had with colleagues over the past few weeks involving the potential for electronic health/medical records (EHRs or EMRs) for improving clinical care and their utility for research. All of these conversations have been in the context of how we use clinical data to better record and understand patient\u0026rsquo;s state, trajectories, treatments/interventions, adverse events and outcomes. In this post, I discuss an example of how EHRs are used in a workflow for clinical practice and highlight opportunities for improving EHRs.\nFrom my perspective, a central theme is how given a set of available tools (including an EHR), my workflow in clinical practice includes work-arounds and routines analogous to what Herbert Simon called \u0026ldquo;satisficing\u0026rdquo; \u0026ndash; finding satisfactory (often sub-optimal) solutions to account for the realities of the environment.\nIntroduction There were some dominant themes emerging from discussions with colleagues around the use of EHRs:\n clinical records contained in the EHR represent a kind of ground-truth e.g. about clinical states, trajectories, diagnoses and treatments that free-text records in the EHR contain sufficient signal (relative to noise) and that we can develop research tools which can reliably extract usable research content, for example, using natural language processing (by both rule-based as well as statistical/probabilistic methods) that structured data in the EHR is routinely collected and can be considered more reliable than free-text \u0026ndash; this includes demographics, coded diagnostic data, structured psychometric instruments/questionnaires/scales and patient-reported outcome measures as well as service utilisation data (e.g. general practitioner contacts, temporal event data representing contact with services such as admissions to hospital) non-native data (e.g. generated outside the EHR) data is repositoried in the EHR (e.g. clinic letters, admission/discharge summaries) by being uploaded and represent a further source of clinical data  One important and recurring theme in each conversation was that clinician workflow and the situated practice of clinical use of EHRs was unexplored. This is important because workflow (importantly, the work-arounds that clinicians find and use) impacts on the content of the EHR. Below, I describe my current workflow with an EHR that I use every day and highlight some of the problems arising and opportunities for developing EHRs in a way that actively assists clinical recording and decision making.\nEHRs are supposed to be an improvement on paper medical records \u0026ndash; my experience suggests this is absolutely true \u0026ndash; but there are new problems that emerge from how these systems are embedded in and used by clinical teams. As a preview, my principle complaint with EHRs is that they often default to being a document-oriented database but without any of the query, retrieval or data-modelling sophistication. They end up being a \u0026lsquo;dumping ground\u0026rsquo; for recording every kind of clinical activity, in part I suspect, because of their medico-legal status.\nOver the past decade, I\u0026rsquo;ve used 13 different systems (excluding more narrow-functionality, specialty-specific systems e.g. for radiology):\n three different commercial, proprietary EHR systems designed for secondary care in psychiatry/mental health three different commercial, proprietary EHRs for acute (general, secondary care) hospitals four different emergency department EHRs for acute hospitals (each commercial and proprietary) two \u0026ldquo;custom\u0026rdquo; EHRs built in-house for two different secondary care, acute hospitals a single proprietary primary care (general practice) EHR  Common to the my experience and use of all of these EHRs, is I have to find a workflow and set of practices around these systems and this has remained relatively constant: All of the intellectual work is done using applications separate from the actual EHR because:\n  generally, none of the EHRs provide tools for the kinds of information processing that I need in clinical practice (more below)\n  I have not used any EHR that seamlessly inter-operates with another \u0026ndash; for example, if I want to combine radiology, pathology and my clinical notes, I have to copy-and-paste between applications hoping that data displayed in each application can be made compatible by exchanging it via a lowest common denominator or minimal common exchange format \u0026ndash; which is invariably plain, free-text.\n  One consequence of the above is that clinicians tend to treat the EHR as a data repository (particularly, for the free-text and non-native format storage) and as a medico-legal document (i.e. if you haven\u0026rsquo;t recorded some activity as an entry on the EHR, then it didn\u0026rsquo;t happen)\n  none of the EHRs that I\u0026rsquo;ve used is engineered to be reliable enough that I can trust it \u0026ndash; for example, my current EHR runs as a web application \u0026ndash; but appears to only be guaranteed (tested) to work on a single browser platform; it either crashes frequently (often losing draft or in-progress work) or \u0026lsquo;times out\u0026rsquo; my connection (for governance and information security reasons, because I\u0026rsquo;ve been inactive for a period of around 15 minutes) without a robust draft/auto-save facility.\n  Example Workflow Over the years, I\u0026rsquo;ve developed a workflow which is optimal for me given the above constraints. My hope is to highlight how these represent work-arounds for the problems highlighted above. The context here is a weekly out-patient clinic; people attending the clinic are a combination of new referrals (for example, from GPs or other clinicians in secondary care) and people being followed-up in treatment.\nTypically, there are between 6 and 8 patients in a 4 hour clinic; people who are new or I\u0026rsquo;ve not met before are often given 45 minutes to 1 hour appointments and people attenting for \u0026lsquo;check up\u0026rsquo; or treatment monitoring are usually given 30 minute appointments.\nFor New Patients As I describe this process, I\u0026rsquo;ll keep a running total the number of unique applications and instances/windows for these applications that I need.\nFor people new to the clinic (or that I\u0026rsquo;ve not met before):\n  I\u0026rsquo;m going to need access to my personal library of prescribing guidelines, literature and instruments/scales/ questionnaires. I open the file system browser and navigate to my networked storage. I prefer to do web browsing using a different browser to one our EHR runs on, so I launch this often navigating straight to the British National Formulary (BNF) so it\u0026rsquo;s easily available to me. That\u0026rsquo;s two applications off the bat.\n  I launch the EHR (open the default web-browser, navigate to the EHR link) and my goal is to establish a history of the person\u0026rsquo;s contact with services and to try and understand what the historical and current difficulites are. So we now have three applications open and running.\n  My EHR is a multi-tabbed web application where the main display panel reflects the currently-selected tab. The EHR is organised (roughly) into\n structured data (outcomes, questionnaires, psychometric scales/instruments; diagnostic coding; demographics) the \u0026ldquo;Correspondence\u0026rdquo; section/tab which acts as a repository for any non-native data (scanned documents/letters; investigations including bloods, ECGs, radiology reports from systems not compatible with the EHR; admission/discharge summaries usually in an office application format; email correspondence) the free-text \u0026ldquo;Clinical Notes\u0026rdquo; section where most clinical activity is recorded and (importantly) replicates information from the two other sections  I usually avoid the tabs for \u0026ldquo;Clinical Notes\u0026rdquo; and go straight to a menu at the top of the EHR application where I can opt for the main panel to show me the entire record of these free-text entries. As this free-text record often replicates and brings together in a single location much of the data recorded in different tabs I prefer to use this as a starting point. I read the free-text entries in chronological order \u0026ndash; as the first point of contact/referral is recorded at the end of this document I read from the bottom-up. Of note, this is less time consuming than clicking to read each entry individually in the default list format for the \u0026ldquo;Clinical Notes\u0026rdquo;. Also, when reading a single note/entry individually, the EHR application displays formatting meta-data (e.g. numbered or unordered lists) as plain text, without intepretation or formatting for display. Most importantly, I can exploit the browser\u0026rsquo;s free-text search facility to e.g. quickly navigate to keywords like \u0026ldquo;medication\u0026rdquo;, \u0026ldquo;ecg\u0026rdquo;, \u0026ldquo;bloods\u0026rdquo; \u0026ndash; because the EHR web application doesn\u0026rsquo;t have a search facility.\n I launch another instance of the EHR browser window so I can have access to other tabs in parallel; notably, I want the \u0026ldquo;Correspondence\u0026rdquo; tab which contains scanned documents (usually PDFs, sometimes bitmapped TIFF or GIF images arising from scanned paper documents) including the original referral letter. Often, ECGs are scanned and placed here also. So I open a PDF reader so I can view these. We\u0026rsquo;re at three applications and for the EHR application, two instances or windows.\n  I open another browser window, and navigate to our pathology system (a separate web application) where I can find the most recent blood tests, letters from other specialties, radiology reports and records of admissions or clinic attendances to other local acute general hospitals. I use a separate login identity and password because this system is operated by another NHS Trust. This brings us to four applications and two instances for the EHR\n  While I\u0026rsquo;m reviewing these notes, I open a word processor, and start making brief notes summarising the salient points from the EHR. This \u0026ldquo;main clinic document\u0026rdquo; is the center-piece of my workflow. I also copy and paste the NHS number from the EHR (a person\u0026rsquo;s mostly unique identifier across the NHS), which is formatted as numeric data with the format \u0026ldquo;XXX XXX XXXX\u0026rdquo; \u0026ndash; then I edit out the whitespace because to copy-paste the NHS number to the pathology system, it expects a single, uninterrupted string of numerals. This is more efficient than trying to search the pathology system on name, date of birth etc. Now we have five applications and two instances.\n  On average, for each clinic appointment, I\u0026rsquo;m spending between 15 minutes to an hour (for very complex cases) working with this workflow by quick-tabbing (ALT+Tab) between applications/windows, making notes, navigating the EHR, copy-pasting, looking up results and so on.\nThe focus of this information gathering exercise is the word processor document (the \u0026ldquo;main clinic document\u0026rdquo;), where I\u0026rsquo;m collating together all information that has direct utility for the forthcoming clinic appointment for that patient.\nImportantly, I\u0026rsquo;m copying and pasting information from the pathology application, the EHR and PDF reader (when the documents are not bitmapped, scanned images \u0026ndash; where in these cases I have to retype the data). I have my word processor set to default to \u0026lsquo;paste as plain text\u0026rsquo; to avoid formatting problems when e.g. copying tabulated data from the pathology web application to my evolving clinical notes for the clinic appointment.\nI\u0026rsquo;ll keep this workflow \u0026lsquo;open\u0026rsquo; and repeat the process for each person to be seen in clinic. As I proceed, the \u0026ldquo;main clinic document\u0026rdquo; is a single, growing document which contains page-separated summaries for every person to be seen. For each person, this typically takes the form of: the person\u0026rsquo;s recent history with health services, referral details, history of and presenting complaint/current difficulites, a history of their investigations/test results to date (or pending), relevant medical history, medications history, side-effects/history of adverse reactions or allergies, personal / social circumstances and some of my initial thoughts about issues that need addressing in clinic, outstanding investigations and an initial management plan. Most of this information is gathered from the free-text \u0026ldquo;Clinical Notes\u0026rdquo; function of the EHR, navigated using the free-text search facility provided by the browser.\nFor \u0026lsquo;Known\u0026rsquo; Patients The above workflow is modified for a person I\u0026rsquo;ve met before:\n  Instead of reading the entire set of clinical notes from the earliest to the most recent entry, I\u0026rsquo;ll still request the EHR displays all clinical notes, but read from the top (most recent). I\u0026rsquo;ll review what has happened since I last met with this person for example, notes from social workers, occupational therapists, psychologists, pharmacists, care coordinators.\n  In the second of the EHR browser instances, I\u0026rsquo;ll look in \u0026ldquo;Correspondence\u0026rdquo; where I\u0026rsquo;ll locate my last clinic letter. I\u0026rsquo;ll open this letter which launches another document window in the word processor application. From here, I\u0026rsquo;ll copy the \u0026lsquo;proforma\u0026rsquo; part of the letter (which contains demographics, clinicians involved in their care, medication lists and diagnoses) and paste into my evolving \u0026ldquo;main clinic document\u0026rdquo;. I\u0026rsquo;ll also copy the last \u0026ldquo;Plan\u0026rdquo; we made so I can refer to and update it with any progress notes. Finally, I\u0026rsquo;ll also copy and paste the last mental state examination and the list of \u0026ldquo;early warning signs\u0026rdquo; (for relapse) which are contained in the clinic letter so I can refer to these during the clinic appointment.\n  When relevant, I\u0026rsquo;ll switch to the browser window containing the EHR \u0026ldquo;Clinical Notes\u0026rdquo; and summarise any investigations which any of the team have placed in the free-text record. This might be, for example, entries summarising blood results planned from the last appointment, imaging or other assessments that help formulation/diagnosis or will influence management. Sometimes, I\u0026rsquo;ll go to the separate browser running the pathology web application if I can\u0026rsquo;t locate information or it\u0026rsquo;s summarised too briefly for example, when someone has checked some bloods and written \u0026ldquo;Bloods checked, NAD\u0026rdquo; as a placeholder to record the activity rather than the results themselves.\n  Also, when relevant or required, I\u0026rsquo;ll need to look at the \u0026ldquo;Correspondence\u0026rdquo; tab to see written communication from other teams or specialties and make notes in the main clinic document so it\u0026rsquo;s all available in one place during the clinic appointment. It\u0026rsquo;s worth noting that very often, email correspondence between appointments with other professionals are copied into the \u0026ldquo;Clinical Notes\u0026rdquo; free-text to formally record the outcomes of inter-professional or agency working.\n  Finally, I\u0026rsquo;ll usually open a spreadsheet application and open an existing (or create a new) spreadsheet for each person where I\u0026rsquo;ve previously completed quetionnaires or instruments (usually, these are clinician and patient-reported structured data which can\u0026rsquo;t be easily managed inside the EHR because they aren\u0026rsquo;t \u0026lsquo;standard forms\u0026rsquo;). These are most often structured, quantitative data on depressive/affective symptoms, tools like the Scale for Assessment of Negative Symptoms (SANS), PANSS and cognitive tests/screening. These spreadsheets are all located on my personal Trust-based secure storage for compliance with information governance.\n  In total, to prepare for a clinic I\u0026rsquo;ll have six applications running.\nBefore and During A Clinic I might tidy up my desktop environment a bit before clinic \u0026ndash; for example, closing the PDF reader, one or two browser instances and so on. Before the clinic gets started, I\u0026rsquo;ll have a separate calender application open (our EHR provides a calender / scheduling tool, but it\u0026rsquo;s not used by clinicians or the administration team) \u0026ndash; this shows me the timing for the clinic, and should reflect same order as the \u0026ldquo;main clinic document\u0026rdquo; in my word processor. I\u0026rsquo;ll also print any hardcopies of materials I\u0026rsquo;ll need for each patient (for example, I\u0026rsquo;ll get any mandatory or useful clinical questionnaires printed) \u0026ndash; we don\u0026rsquo;t have tools for electronic administration of the clinical instruments and questionnaires I commonly use.\nDuring clinic, with the person\u0026rsquo;s permission, I\u0026rsquo;ll make short-hand notes about important issues directly into the main clinic document as we go along. I\u0026rsquo;ll also record the plans / changes to management we agree in clinic including any planned investigations, appointments with other members of the team and important communication to their GP. I\u0026rsquo;ll collate paperwork completed during the clinic for entry later on (there\u0026rsquo;s not enough time between appointments). However, between appointments, in the calender application I\u0026rsquo;ll often put in calender reminders / events to prompt me to follow up parts of the plan made during the previous appointment.\nAfter A Clinic Typically, I\u0026rsquo;ll go back to the start of the main clinic document, and for each person, complete the following steps to arrive at a summative document which will form the basis of a clinic letter to other professionals and \u0026lsquo;double up\u0026rsquo; as a record to go in the EHR \u0026ldquo;Clinical Notes\u0026rdquo; free-text:\n  Collate any paperwork (quetionnaires, scales) and manually enter these into a spreadsheet which captures historical and current measures that are important. For example, the person\u0026rsquo;s currently reported depression symptoms inventory, results from brief cognitive tests and so on.\n  I\u0026rsquo;ll complete the mental state examination (MSE) for each person seen by expanding on the notes I made during clinic. While doing this, I compare this to the previous MSE (which are usually pasted in the main clinic document) for that person \u0026ndash; noting any important changes. I\u0026rsquo;ll end the MSE with an \u0026lsquo;impression\u0026rsquo; which summarises important information on the current MSE and often, highlighting changes with respect to previous MSEs.\n  The first part of the clinic letter is then written; summarising the last time / duration since the person was seen and a narrative description including any issues brought to clinic by the patient, recent issues/events I\u0026rsquo;ve collated from reading the EHR notes (for example, I might highlight any recent investigations). The latter part of the letter will summarise any quantitative (structured) information usually from my spreadsheet-based data \u0026ndash; for example, I might note the current salient items in a symptoms inventory commenting on changes from any previous measures.\n  The letter concludes with an updated plan, noting any actions to be completed (by whom and when) from this clinic appointment and highlighting anything that remains pending from previous clinic plans.\n  By the end of this process, I usually have between one and three pages of information which will include the clinic letter and then:\n I will send the letter content for each patient via email to the team\u0026rsquo;s administrators, who will format, electronically sign and place the relevent letter heads onto the document. They also direct the correspondence out by post or email to the patient and any clinicians indicated in the \u0026lsquo;cc\u0026rsquo; list. The administrators will also upload the finalised letter to the \u0026ldquo;Correspondence\u0026rdquo; section of a patient\u0026rsquo;s EHR. This often happens within 24-48 hours depending on how many clinicians are requesting help with administration as well as when the clinic finishes (often, for an afternoon clinic, the post-clinic work will take place after office hours).\n  I copy the text of the letter into the EHR \u0026ldquo;Clinical Notes\u0026rdquo; \u0026ndash; while, strictly, this should be redundant (as the administrators will evenutally place it on \u0026ldquo;Correspondence\u0026rdquo;) it is often necessary for contemporaneous recording; for example, should the patient be seen out of hours in a crisis by other clinicians. Often, in doing this, I use the minimal common exchange format \u0026ndash; plain text \u0026ndash; because I can\u0026rsquo;t trust the EHR to format information beyond this. I can\u0026rsquo;t use graphical or tabular data reliably.\n  This process is repeated for each person seen in clinic. It requires four separate applications (word processor, spreadsheet, web browser for the EHR and an email/calender application). Notice how the intellectual and administrative parts of this work are independent of the EHR; the main clinic document is the focal point, and the other applications allow for collating and summarising data and communicating the output of this process with other team members (notably, the administration team). Only at the end of this process does the EHR factor in as a repository for a document.\nTime Costs For the pre-clinic workup:\n  To prepare for a clinic where everyone is new, I\u0026rsquo;ll need between 2 and 4 hours. At the end of this, I\u0026rsquo;ll have one document (in the word processor) containing all the relevant information for each patient.\n  When their\u0026rsquo;s a mixture of new and known people, the cumulative information contained in previous clinic letters helps reduce the time \u0026ndash; but it comes in around 2 hours on average.\n  For the post-clinic work:\n  Depending on complexity, I can write and compare MSEs in around 10-20 minutes for each person. It\u0026rsquo;s worth noting that the MSE should contain enough information to facilitate my comparison / estimate of change as well as communicate with other professionals.\n  The remainder of the letter takes a further 10-15 minutes.\n  In total, for a four-hour clinic, there\u0026rsquo;s an average of between 2-4 hours of post-clinic work.\n  Core Components of the Workflow Central \u0026lsquo;Working Space\u0026rsquo; This role is fulfilled by the \u0026ldquo;main clinic document\u0026rdquo; in the word processing application that collects together information from at least two locations in the EHR (most often, the free-text \u0026ldquo;Clinical Notes\u0026rdquo; and the \u0026ldquo;Correspondence\u0026rdquo; repository), a separate pathology system and previous clinic letters.\nIt is required because:\n the EHR does not provide a facility to collate information together from multiple sources in a \u0026ldquo;workspace\u0026rdquo; or scratch-pad that can be used to display, edit, synthesise information and to begin formulating a record of the clinical encounter. For example, I can\u0026rsquo;t ask the EHR to display my last MSE so I can conveniently compare to the current clinic MSE. Very often, the important clinical information is the change in the clinic MSE relative to the last recorded MSE.  In an ideal world, I\u0026rsquo;d like the MSE to be a structured record from which a \u0026lsquo;narrative\u0026rsquo; version (for the clinic letter) can be generated \u0026ndash; this would reduce the time spent (redundantly) re-typing the same thing over and over.\n  the EHR\u0026rsquo;s method of recording medications or interventions trialled is cumbersome to the point no-one uses it; instead, the free-text \u0026ldquo;Clinical Notes\u0026rdquo; become the primary source of information \u0026ndash; it is, of course, unstructured and subject to idiosyncratic recording between clinicians. As an example, if I want to compile a history of medications trialled, I have to manually search, using the browser search facility, and copy and paste information to my main clinic document.\n  clinic letters and the \u0026ldquo;Correspondence\u0026rdquo; tab (really, just a repository of anything that is not free text in the \u0026ldquo;Clinical Notes\u0026rdquo; tab) are the best cumulative source of information for the person\u0026rsquo;s trajectory during treatment \u0026ndash; they contain plans (which evolve), describe change in the person\u0026rsquo;s condition, highlight unmet needs and often contain the clinical reasoning that justifies changes in management.\n  during clinics, I have most information in one place \u0026ndash; I don\u0026rsquo;t have to rely on other web-based applications which are unstable, don\u0026rsquo;t inter-operate or have cumbersome user interfaces to quickly find the information I need prompt access to (i.e. to minimise disruption to the patient\u0026rsquo;s time during the clinic appointment)\n  the EHR is not stable enough to rely upon during clinic appointments or to remain stable for the time after a clinic \u0026ndash; in short, if you don\u0026rsquo;t store your notes in a word processor application, you\u0026rsquo;ll likely loose them before you have a chance to finalise them.\n  Spreadsheets for Structured / Quantitative Data The EHR I work with can record some structured clinical data (for example, important patient-reported outcome measures (PROMs) have their own structured form-based input), but it\u0026rsquo;s hard to locate and find the facility quickly and most importantly, there is no way of visualising patterns and changes in serial measurements.\nAs an example, currently, to use PROMs for audit or research on outcomes, there is a research assistant who manually re-enters the same data into spreadsheets and then uses the spreadsheet application to produce visualisations. So, for one questionnaire, we have a paper form that is entered onto two systems (the EHR and a standalone spreadsheet), by two separate people simply to visualise, understand trends and compile meaningful outcome data. The team has to repeat a similar exercise when compiling compliance data for national audits \u0026ndash; our EHR doesn\u0026rsquo;t provide functionality that means we don\u0026rsquo;t have to \u0026lsquo;parallel record\u0026rsquo; this information.\nFurther, I would have to wait for an EHR implementation team to create a structured form should we decide to use something targetted to a particular clinical population, team/clinic or condition.\nSo, the tactical solution of using one\u0026rsquo;s own spreadsheet-based implementation for structured data provides:\n a familiar user interface for tabular data entry for questionnaires, PROMs and structured clinical instruments quick and low-cost implementation of tools without waiting for a team to implement in the EHR a way of visualising patterns / changes over time (imperfectly, because we\u0026rsquo;re really using a generic office application but it\u0026rsquo;s certainly better than nothing) some (limited) flexibility in how this data is then migrated to the \u0026ldquo;main clinic document\u0026rdquo; to form part of the clinic letter / summary of the clinical encounter  The Current Role of the EHR As described in my workflow (and this seems familiar to other clinicians I work with):\n  the focus of the intellectual and administration work is \u0026ndash; almost exclusively \u0026ndash; the word processing application which acts as a common workspace for the whole operation; for each person seen in clinic, the \u0026ldquo;main clinic document\u0026rdquo; summarises recent history (e.g. from previous clinic letters and notes entered onto the free-text \u0026ldquo;Clinical Notes\u0026rdquo; tab), previous plans, recent investigation results and acts as the location where this information and the current clinical encounter come together\n  the spreadsheet application runs \u0026lsquo;parallel\u0026rsquo; to facilitate structured data collection that may be provided in the EHR, but where the utilisation of this data (and exchange of data with the \u0026ldquo;main clinic document\u0026rdquo;) is not provided for.\n  Finally, notice how the EHR enters this workflow at only two points:\n  First, in the pre-clinic workup, where it is used as a source repository for a) recent clinical encounters (which are, mostly, composed using the same word processing application because the EHR is unstable) and b) centralised repository of any clinical documentation or communication between professionals, teams and agencies\n  Second, right at the end of the workflow, copies of the recent clinical encounter are pasted as a free-text entry (in the \u0026ldquo;Clinical Notes\u0026rdquo; tab) and the the clinic letter uploaded to the \u0026ldquo;Correspondence\u0026rdquo; tab.\n  In what I\u0026rsquo;ve described here, the EHR functions primarily as a document-based storage facility. What is notably absent is functionality in the EHR that supports clinical decision making. To compensate, we develop a workflow and supplement functionality from other applications \u0026ndash; more harshly, it\u0026rsquo;s a kludge. As a final post-script, notice the frequency with which information and data is manipulated and manually moved between different applications in an attempt to facilitate the actual clinical encounter. This is largely because of the EHR-imposed need to have a minimal, common exchange format and that happens to be plain text for the most part.\n","date":1561507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620505417,"objectID":"0c7af5e5a914a0b2d970a8e80a490ac9","permalink":"/post/routines-and-the-ehr/","publishdate":"2019-06-26T00:00:00Z","relpermalink":"/post/routines-and-the-ehr/","section":"post","summary":"The motivation for this post is a series of discussions I\u0026rsquo;ve had with colleagues over the past few weeks involving the potential for electronic health/medical records (EHRs or EMRs) for improving clinical care and their utility for research.","tags":[],"title":"Routines and Workflow: Adaptations around the Electronic Health Record","type":"post"},{"authors":null,"categories":["clinical state","trajectories","dynamical systems"],"content":"   In this series of blogposts, we look at some models of clinical state. The motivation is to document exploratory work with a colleague (Nick Meyer, who runs the SleepSight study) as we try and apply some theoretical ideas ‚Äì for example (Nelson et al. 2017; Scheffer et al. 2009) ‚Äì to ‚Äòreal-life‚Äô data. This problem is interesting because the psychiatric literature more often than not uses an aggregate measure of either state or trajectory, and sometimes, there is no distinction made between the person‚Äôs clinical state, a measurement of this state and an ‚Äòoutcome.‚Äô As examples, most will be familiar with measuring the total (aggregate) score over some scale or instrument (e.g.¬†HAMD in depression, PANSS in psychotic disorders). Often, we measure this at two time-points ‚Äì such as before and after treatment ‚Äì and describe the outcome as a change in this aggregated score. Sometimes, we plot a time-series of these total scores, and call this a trajectory. However, this results in a loss of information (see here) and is driven by the requirement to be compatible with standard (or perhaps more accurately, off-the-shelf) procedures for analysing such data (i.e.¬†defining a discrete ‚Äòresponse‚Äô / ‚Äòno-response‚Äô univariate outcome for investigating the efficacy/effectiveness of an intervention).\n1 Basics Throughout, we will assume that there are measurements of clinical state, obtained by some instrument, usually with some noise added. Further, for the purposes of this post, we assume that there is some latent process being measured by these instruments and we use clinical symptoms as a concrete example (but the principles generalise to anything that can be measured and taken to represent state). For pedagogical reasons, the easiest example is to consider just two dimensions - for example, in psychosis, we might measure the positive and negative symptom burden.\nTo begin, take a time-ordered set of measurements for positive (\\(P\\)) and negative (\\(N\\)) symptoms respectively:\n\\[ \\begin{aligned} P \u0026amp;= (29,24,17, \\ldots, 12, 11) \\\\ N \u0026amp;= (26,24,19, \\ldots, 22, 25) \\end{aligned} \\]\nGraphically, this looks like:\nIndividually, we can see that positive symptoms generally decrease over time, but the negative symptoms ‚Äòoscillate.‚Äô Next we define a native state space where instead of treating the two sequences as independent, we form a vector \\(x(t) = (p_t, n_t)\\) with \\(p_t \\in P\\) and \\(n_t \\in N\\):\n\\[ \\begin{aligned} x(t) \u0026amp;= \\big[ (29,26), (24,24), (17,19), \\ldots,(12,22), (11,25) \\big] \\end{aligned} \\] So, if we want the state at \\(t=7\\) we get \\(x(7) = (12,22)\\) and similarly, \\(x(2) = (24,24)\\). Each of these states is naturally a point in two dimensions, visualised as a plane:\nIn this example, the state space is a finite plane (two-dimensional) which contains all possible configurations of \\((P,N)\\), and a single time-ordered sequence of states (numbered 1 through 8, in orange) shows the state trejectory for a single person. We can equip this state space with a metric that imports mathematical tools for notions such as the distance between two states. This means we can model the patient‚Äôs trajectory in this native state space (preserving information) and only when we absolutely need to, apply mathematical tools to reduce this multi-dimensional representation to a convenient form that enables us to e.g.¬†inspect change or build statistical models.\n 2 Dynamical System Approach As a starting point, (Nelson et al. 2017) consider and survey some theoretical proposals for moving toward dynamic (instead of static) models of the onset of disorders ‚Äì notably, they review dynamical systems and network models. Similarly, (Mason, Eldar, and Rutledge 2017) explore a model of how mood oscillations occur in bipolar disorder and their proposal is superifically similar to a dynamical system with periodic behaviour. The influential work of (Scheffer et al. 2009) is also relevant: if one can identify a latent process with a dynamical systems formulation, then a whole theory of critical transitions can be mobilised to explain how perturbations from the system‚Äôs equilibirum can ‚Äòbreak‚Äô the inherent stability of a system leading to a catastrophic change (i.e.¬†relapse). Our starting point here is how operationalize these ideas.\nHere, we assume that underlying the measured clinical state is some process which behaves according to a putative model. The example used here, and in the literature, is of damped oscillators. A physical analogy helps: imagine a mass attached to a fixed point by a spring. At rest, this system is in equilibrium. If the mass is pulled or pushed (displaced) by a certain amount, the system is moved from equilibrium and will ‚Äòbounce‚Äô up and down with a frequency and amplitude determined by the amount of displacement, the ‚Äòstiffness‚Äô of the spring and any ‚Äòdamping‚Äô introduced by the viscosity of the medium. This has been proposed as a model of e.g.¬†mood dysregulation (Odgers et al. 2009) and symptom trajectory is modelled using a damped oscillator that is fit to data using for example, regression (Boker and Nesselroade 2002).\nTo begin, we denote the clinical state at time \\(t\\) by \\(x(t)\\) (note this is a uni- rather than multi-variate state representation, so for example, consider only the ‚Äònegative symptoms‚Äô plot above). For more discussion of models of damped oscillators, see (Hayek 2003) for an applied physical systems discussion, and for a helpful mathematical tutorial, Chapter 2.4 of (Lebl 2019).\nA general damped oscillator is described by a linear second-order ordinary differential equation (ODE):\n\\[ \\frac{d^2x}{dt^2} - \\zeta \\frac{dx}{dt} - \\eta x = 0 \\]\nWith coefficient \\(\\zeta\\) modelling the ‚Äòdecay‚Äô of the oscillations, and \\(\\eta\\) describing the ‚Äòfrequency‚Äô of oscillations.\nTo simplify the presentation, we use ‚Äòdot‚Äô notation where \\(\\ddot{x}\\) and \\(\\dot{x}\\) are the second and first derivatives respectively: \\[ \\ddot{x}(t) - \\zeta \\dot{x}(t) - \\eta x(t) = 0 \\]\nRearranging for the second-order term: \\[ \\ddot{x}(t) = \\zeta \\dot{x}(t) + \\eta x(t) \\] Generally, we only have access to numerical data that we suppose is generated from an underlying damped oscillator process; so we use numerical differentiation to obtain the locally-linear approximation to the derivatives of \\(x\\).\nTo use ODEs as our model, we need to be able to solve them (for example, for fitting and then reconstructing the model for a given set of data). To use off-the-shelf ODE solvers, we need to convert the second order ODE into two first-order equations by writing substitutions:\n\\[ \\begin{aligned} y_1 \u0026amp;= x \\\\ y_2 \u0026amp;= \\dot{x} = \\dot{y_1} \\\\ \\end{aligned} \\] So : \\[ \\begin{aligned} \\dot{y_2} \u0026amp;= \\zeta y_2 + \\eta y_1 \\\\ \\dot{y_1} \u0026amp;= y_2 \\end{aligned} \\]\nThe analytic solution for this system is well understood and depends on the parameters \\(\\zeta\\) and \\(\\eta\\) ‚Äì there are three solutions for \\(x(t)\\) depending on whether the system is damped, under-damped or critically damped ‚Äì (Lebl 2019) gives a helpful tutorial. However, as we won‚Äôt know the parameters in advance, we need to use numerical methods (an ODE solver) reassured that we can plug in any set of parameters to construct and visualise \\(x(t)\\).\n 3 Simulated Example We generate some simulated data with the following parameters:\n \\(\\zeta\\) = -0.1 (the ‚Äòdamping‚Äô or ‚Äòamplification‚Äô) \\(\\eta\\) = -0.5 (the ‚Äòfrequency‚Äô of oscillations) initial displacement (‚Äòbaseline‚Äô) value \\(x(0)\\) = 5 and initial ‚Äòvelocity‚Äô \\(\\dot{x}(0)\\) = -2.5  To emphasise how the system looks when amplifying (rather than damping) the oscilations, we invert the sign: \\(\\zeta\\) = 0.1 resulting in:\n 4 Model Fitting The modelling approach from (Boker and Nesselroade 2002) ‚Äì used in (Odgers et al. 2009) ‚Äì is to treat \\(\\ddot{x}\\) as the dependent variable in a linear regression model with independent variables \\(\\ddot{x}\\) and \\(x\\). We note that (Boker and Nesselroade 2002) intends for their method to fit a population-level model ‚Äì that is, extracting a common \\(\\zeta\\) and \\(\\eta\\) for a group of people‚Äôs trajectories such that ‚ÄúWhen a stable interrelationship between a variable and its own derivatives occurs, the variable is said to exhibit intrinsic dynamics‚Äù (Boker and Nesselroade 2002). We‚Äôll only consider fitting to a single individual here.\nThe steps are as follows.\n4.1 Compute Gradients Using numerical approximation, and the simulated damped oscillator data, the columns are x = \\(x(t)\\), dx1 = \\(\\dot{x}\\) and dx2 = \\(\\ddot{x}\\):\n  Time  x  dx1  dx2      1  5.00000  -3.34588  -0.21820    2  1.65412  -3.56407  0.11456    3  -2.12815  -3.11675  1.13715    4  -4.57938  -1.28977  2.03432    5  -4.70768  0.95190  1.91782    6  -2.67558  2.54586  0.93726    7  0.38405  2.82642  -0.37767    8  2.97725  1.79053  -1.39512      4.2 Estimate Parameters We can quickly and easily estimate using the ‚Äòregression‚Äô approach, which will be an ordinary least squares solution. The resulting point estimates \\(\\hat{\\zeta}\\) and \\(\\hat{\\eta}\\), are displayed below, alongside the actual parameters (i.e.¬†for the simulated damped oscillator above):\n   Estimated  Actual      Zeta  -0.151  -0.1    Eta  -0.381  -0.5      4.3 Reconstruct Time Series The final step is to visualise the resulting model by numerically integrating the ODEs again, but this time, using the estimated \\(\\hat{\\zeta}\\) and \\(\\hat{\\eta}\\) to ‚Äòreconstruct‚Äô the time series \\(\\hat{x}(t)\\) and compare with the original:\nThe time series resulting from the estimated model parameters (shown in red) is predictably different ‚Äì and there are at least two systematic reasons for this:\nThe numerical approximation of the true derivatives is systematically over or under-estimating the ‚Äòtrue‚Äô derivatives These errors are propogated further by obtaining (essentially) an ordinary-least-squares point estimate of the parameters from the data  The estimates \\(\\hat{\\zeta}\\) and \\(\\hat{\\eta}\\) are derived from these numerical derivatives, so unsuprisingly they are close (but significantly) different from the ‚Äòtheoretical‚Äô or known \\(\\zeta\\) and \\(\\eta\\). We can quantify the mean square error between the reconstructed and original time series:\n\\[ \\text{MSE} \\left( \\hat{x}(t), x(t) \\right) = \\frac{1}{N} \\sum_{t} \\big[ \\hat{x}(t)-x(t) \\big]^2 \\]\nwhere \\(N\\) is the number of time points in the original \\(x(t)\\). The MSE is then 0.9552. This is useful as a baseline for what follows.\n  5 Estimating Parameters by Non-Linear Least Squares Optimisation Using an ordinary least squares solution for \\(\\ddot{x}(t) \\sim \\dot{x}(t) + x(t)\\) ‚Äì we obtained a relatively poor estimate for \\(\\hat{\\zeta}\\) and \\(\\hat{\\eta}\\), and this was reflected in the MSE for the reconstructed (versus original) time series. A more traditional method would be to use a non-linear optimisation algorithm to search the parameter space of \\((\\zeta, \\eta)\\), so we try using the Levenberg-Marquardt (LM) method. This method finds an estimate for \\((\\hat{\\zeta}, \\hat{\\eta})\\) by minimising an objective function, which in our case, are the values of the parameters that minimise the sum of squares of the deviations (essentially, the MSE). Like many optimisation methods, we run the risk of locating local (rather than global) minimum ‚Äì that is, an estimate of \\((\\hat{\\zeta}, \\hat{\\eta})\\) which minimises the MSE, but where if we were to ‚Äòexplore‚Äô the surface of the MSE over a broader range of parameters values, a better (more global) minimum might be found.\nThe LM algorithm is iterative, proceding by gradually refining the estimates \\((\\hat{\\zeta}, \\hat{\\eta})\\) from an initial, user specified ‚Äòfirst estimate.‚Äô If this first ‚Äúguess‚Äù is close to the global minimum the algorithm is more likely to converge to the global solution. It therefore makes sense to use domain knowledge to establish a plausible starting point for the LM search. In our case, we will start the search by initialising the parameter estimate to be negative real numbers which corresponds to our expectation that we will be observing a damped (rather than amplifying) oscillator.\nAs we only have two parameters to estimate, we can manually evaluate the MSE by systematically varying \\((\\hat{\\zeta}, \\hat{\\eta})\\) over a coarse grid of values to get an idea of what the error surface looks like, and further, we can then extract the best estimate (as we will have evaluated the error at each combination of \\((\\hat{\\zeta}, \\hat{\\eta})\\)).\nOn the left, we see that the error surface on a coarse grid over [-1,0] in steps of 0.05 for \\((\\zeta,\\eta)\\) shows that the error is very large around \\((0,0)\\) but otherwise appears ‚Äòflat.‚Äô The white lines and dot show the parameter values for the minimum MSE ‚Äì but at this coarse resolution, we can not see the shape of the error surface near the optimum solution. The panel on the right shows the error surface ‚Äòzoomed‚Äô for \\(\\hat{\\zeta} \\in [-0.15,-0.05]\\) and \\(\\hat{\\eta} \\in [-0.55,-0.45]\\), (note the difference in the MSE scale) and we can see that best esimates are \\(\\hat{\\zeta} = -0.1\\) and \\(\\hat{\\eta} = -0.5\\).\nThis brute-force method gives us the correct answer, and allows a visualisation of the error surface we expect the LM algorithm to search iteratively. We now compare with the LM solution setting our ‚Äúinitial guess‚Äù to \\(\\hat{\\zeta} = -1\\) and \\(\\hat{\\eta} = -1\\) which corresponds to the bottom-right of the parameter space above.\nAnd find:\n \\(\\hat{\\zeta}\\) = -0.1 \\(\\hat{\\eta}\\) = -0.5  Finally, we reconstruct the original time-series to compare:\nA much better fit, with an MSE approaching 0.\n 6 Some Real Data So far, we‚Äôve been using ‚Äòideal‚Äô simulated data where there is no measurement error and the underlying (hypothetical) damped oscillating process is in a one-one correspondence with the time series \\(x(t)\\). Here, we use some data on daily self-reported symptoms from Nick Meyer‚Äôs SleepSight study. Nick‚Äôs data is a natural fit for the state-space models espoused at the start, but to apply a dynamical system model, we need to start small (with one variable). We pick one symptom (self-reported sleep duration) for an examplar participant, and then scale and center the data, before detrending (i.e.¬†removing any linear ‚Äòdrift‚Äô in the time series). We then add a lowess smoother (shown in red) to try and expose any underlying pattern:\nNote the somewhat periodic behaviour but there is no clear frequency or progression of damping of oscillations, so it is unlikely that a ‚Äòpure‚Äô damped oscillator will fit. Nonetheless, we use the LM method to try and fit a damped oscillator:\nResulting in a poorly fitting model. One way to understand this is that to look at the phase plane for the system. First, take our first simulated oscillator:\nOn the left, we have \\(x(t)\\) on the vertical axis as time progresses. On the right, we plot \\(\\dot{x}(t)\\) on the vertical and \\(x(t)\\) on the horizontal axes (using the ‚Äòmass on a spring‚Äô analogy, we are looking at the relationship between the velocity and displacement). The purple line is our original oscillator, and the orange line the same system (\\(\\zeta\\) and \\(\\eta\\)), but with different initial values (i.e.¬†the initial state is \\(x(t) = 2\\) with initial ‚Äòvelocity‚Äô \\(\\dot{x}(t) = 1.5\\)). The right panel shows the phase plane ‚Äì the evolution of the \\(x(t)\\) and \\(\\dot{x}(t)\\) over time. Notice how (despite different initial values) the two damped oscillators converge to a stable attractor in the middle (which corresponds to the equilibrium state of the system around as \\(t \\rightarrow 100\\)).\nIf we look at the behaviour of the real (sleep duration) data using the numerical derivatives: The colour gradient shows time so we can compare the left and right panels: we can see that while the system tends towards a region around \\(x(t) = 0.2\\) and \\(\\dot{x}(t) = 0\\), it is not stable and the trajectory diverges rather than converging (in contrast to the simulated damped oscillator). The difference in behaviours shown by the phase planes for the real data and the idealised, simulated data (from an actual damped oscillator) tell us why the model fit was so poor.\n 7 Directions The dynamical systems framework is appealing because it provides a model for a latent process which might underly measured / observed data. It requires a model (e.g.¬†a damped oscillator) and a method to fit the data. If the model fits the data, we then import a whole theory that enables us to understand and test the qualitative properties of the model ‚Äì for example, in terms of stability, attractors and critical transitions (Scheffer et al. 2009).\nThe examples used in this post are all linear systems but there is evidently more complexity to the real data we examined ‚Äì bluntly, a single damped oscillator cannot model the dynamics of self-reported sleep in our application (and it might be naive to assume that it would). As we alluded to at the start, we would prefer to treat individual variables as components of a larger system ‚Äì and we have not explored this here (in part, because systems of coupled oscillators require a more sophisticated analysis), instead focusing on principles and how they apply to data.\nIn future posts, we‚Äôll try different approaches that inherit the ideas of state spaces, but attempt to model them without such strong assumptions about the dynamics.\n References Boker, Steven M, and John R Nesselroade. 2002. ‚ÄúA Method for Modeling the Intrinsic Dynamics of Intraindividual Variability: Recovering the Parameters of Simulated Oscillators in Multi-Wave Panel Data.‚Äù Multivariate Behavioral Research 37 (1): 127‚Äì60.  Hayek, Sabih I. 2003. ‚ÄúMechanical Vibration and Damping.‚Äù In Digital Encyclopedia of Applied Physics. https://doi.org/10.1002/3527600434.eap231.  Lebl, Ji≈ô√≠. 2019. Notes on Diffy Qs. https://www.jirka.org/diffyqs/.  Mason, Liam, Eran Eldar, and Robb B Rutledge. 2017. ‚ÄúMood Instability and Reward Dysregulation‚Äîa Neurocomputational Model of Bipolar Disorder.‚Äù JAMA Psychiatry 74 (12): 1275‚Äì76.  Nelson, Barnaby, Patrick D McGorry, Marieke Wichers, Johanna TW Wigman, and Jessica A Hartmann. 2017. ‚ÄúMoving from Static to Dynamic Models of the Onset of Mental Disorder: A Review.‚Äù JAMA Psychiatry 74 (5): 528‚Äì34.  Odgers, Candice L, Edward P Mulvey, Jennifer L Skeem, William Gardner, Charles W Lidz, and Carol Schubert. 2009. ‚ÄúCapturing the Ebb and Flow of Psychiatric Symptoms with Dynamical Systems Models.‚Äù American Journal of Psychiatry 166 (5): 575‚Äì82.  Scheffer, Marten, Jordi Bascompte, William A Brock, Victor Brovkin, Stephen R Carpenter, Vasilis Dakos, Hermann Held, Egbert H Van Nes, Max Rietkerk, and George Sugihara. 2009. ‚ÄúEarly-Warning Signals for Critical Transitions.‚Äù Nature 461 (7260): 53.    ","date":1557680400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557680400,"objectID":"582dc630eb4b22c23baa532923db29f6","permalink":"/post/2021-05-23-clinical-trajectories-pt-one/","publishdate":"2019-05-12T18:00:00+01:00","relpermalink":"/post/2021-05-23-clinical-trajectories-pt-one/","section":"post","summary":"In this series of blogposts, we look at some models of clinical state. The motivation is to document exploratory work with a colleague (Nick Meyer, who runs the SleepSight study) as we try and apply some theoretical ideas ‚Äì for example (Nelson et al.","tags":["state","modelling"],"title":"Clinical State: Part One - Dynamical Systems","type":"post"},{"authors":["Thomas J Reilly","Vanessa C Sagnay de la Bastida","Dan W Joyce","Alexis E Cullen","Philip McGuire"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"0ceafdab7c0df665393e3d492a8c4f05","permalink":"/publication/reilly-exacerbation-2019/","publishdate":"2020-04-05T21:13:10.145771Z","relpermalink":"/publication/reilly-exacerbation-2019/","section":"publication","summary":"Psychotic disorders can be exacerbated by the hormonal changes associated with childbirth, but the extent to which exacerbations occur with the menstrual cycle is unclear. We addressed this issue by conducting a systematic review. Embase, Medline, and PsychINFO databases were searched for studies that measured exacerbations of psychotic disorders in relation to the menstrual cycle. We extracted exacerbation measure, definition of menstrual cycle phase, and measurement of menstrual cycle phase. Standard incidence ratios were calculated for the perimenstrual phase based on the observed admissions during this phase divided by the expected number of admissions if the menstrual cycle had no effect. Random effects models were used to examine pooled rates of psychiatric admission in the perimenstrual phase. Nineteen studies, comprising 1193 participants were eligible for inclusion. Eleven studies examined psychiatric admission rates, 5 examined symptoms scores, 2 examined self-reported exacerbation, and 1 examined both admission rates and symptom scores. A random effects model demonstrated the rate of admissions during the perimenstrual phase was 1.48 times higher than expected (95% CI: 1.31‚Äì1.67), with no significant heterogeneity detected. Four of six symptom score studies reported perimenstrual worsening, but lack of consistency in timepoints precluded meta-analysis. Two studies examining self-reported menstrual exacerbations reported prevalences ranging from 20% to 32.4%. Psychiatric admission rates are significantly higher than expected during the perimenstrual phase. There is some evidence that a worsening of psychotic symptoms also occurs during this phase, but further research with more precise measurement of the menstrual cycle and symptomatology is required.","tags":null,"title":"Exacerbation of Psychosis During the Perimenstrual Phase of the Menstrual Cycle: Systematic Review and Meta-analysis","type":"publication"},{"authors":null,"categories":["change","trajectories"],"content":"The motivation for looking at this topic was primarily my naivety and getting counter-intuitive results when I was looking at serial measures of response to treatment. In particular, I wanted to use outcome measures based on published conventions in the literature for threshold response (the problems with this approach are a topic for another day). I found some pointers on Frank Harrell\u0026rsquo;s blog on the topic of percentages, which mirrored some of the discussion in Chapter 8 of (Senn 2008) which looks at assumptions of additvity in treatment effects.\nHere\u0026rsquo;s the context in clinical trials in psychiatry: To define a clinically-meaningful outcome (e.g. to a treatment/intervention) the literature frequently uses percentage change from baseline \u0026ndash; for example (Leucht et al. 2009; Munk-J√∏rgensen 2011) in the context of PANSS scores in psychosis. I\u0026rsquo;ll avoid the debate about why this may not be appropriate for statistical modelling (e.g. using change from baseline severity rather than using the baseline as a covariate in a statistical model).\nTo make this concrete, let A be the first measurement, and B the second (i.e. pre-treatment and post-treatment respectively, or baseline and endpoint). Leaving aside the correction to ensure minimum symptoms severity is 0, Leucht\u0026rsquo;s formula for percentage reduction from baseline PANSS is essentially:\n$$ 100 \\times \\frac{A - B}{A} $$\nTo make the example concrete, one of the criteria for defining treatment resistance in schizophrenia (Howes et al. 2016) is: \u0026ldquo;less than 20% symptom reduction during a prospective trial\u0026rdquo; (of treatment) using Leucht et al\u0026rsquo;s formula (above). Re-wording this (to make it compatible with Leucht): failure of a treatment trial is that the percentage reduction from baseline is less than 20%.\nPercentages The obvious reason for favouring percentage change from baseline is that improvement (or worsening) is relative the patient\u0026rsquo;s initial level of symptom severity. Assuming higher scores represents higher symptom burden, a patient moving from a baseline A = 97 to an endpoint B = 77 represents a 20% change for that patient. A patient starting from a more modest symptom burden, say A = 65, improving to B = 52 similarly represents a 20% improvement.\nHowever, percentages are not symmetric \u0026ndash; for example, if the first measurement is A = 97, and the second (post-treatment) improved score is B = 67 we have a percentage reduction of $$ 100 \\times \\frac{97-67}{97} \\approx 31 \\% $$\nSwitching this around, for another patient who gets worse by exactly the same absolute amount after treatment: A = 67 and B = 97, yields $$ 100 \\times \\frac{67-97}{67} \\approx -45 \\% $$\nNotice that both the sign and magnitude of the change are different, but the absolute change in units is 30 in both cases.\nThe next problem \u0026ndash; percentages are not additive; assume a 30% improvement from an initial score of A = 100 \u0026ndash; the endpoint (post-treatment) would be B = 70:\n$$ 100 \\times \\frac{100 - 70}{100} = 30\\% $$\nNow assume we follow the same patient from B to another time point, C, and they\u0026rsquo;ve gotten worse, returning to their baseline of 100: $$ 100 \\times \\frac{70 - 100}{70} \\approx -42.9\\% $$\nOn absolute scale units we have a series of measurements A = 100, B = 70 and C = 100, but we have seen a percentage change of 30% (A to B) followed by a percentage change of -42.9% (B to C) despite A = C. In other words, if we take the baseline A, add the change from A to B and the change from B to C, we should have zero net change from the baseline, A to the last measurement C:\n$$ \\begin{aligned} A + (A-B) + (B-C) \u0026amp;= 100 + (100 - 70) + (70 - 100) \\\\\\\n\u0026amp;= 100 + 30 - 30 \\\\\\\n\u0026amp;= 100 \\end{aligned} $$ But if we use percentages, this is not the case; assuming that A represents 100% \u0026ndash; the baseline level of symptom severity \u0026ndash; and abusing notation, we let (A-B) stand for the percentage change from A to B, and similarly for (B-C): $$ \\begin{aligned} A + (A-B) + (B-C) \u0026amp;= 100 + 30 - 42.9 \\\\\\\n\u0026amp;= 87.1\\% \\end{aligned} $$ Percentage change from baseline is intuitive because it allows for a familiar and uniform representation of change relative to the patient\u0026rsquo;s baseline, but it\u0026rsquo;s counter-intuitive because it is asymmetric and non-additive.\nHere\u0026rsquo;s a graphical representation: let the baseline (A) and endpoint (B) values range from 0 to 100 respectively \u0026ndash; the interactive graph below shows the behaviour of percentage change as both A and B vary:\n Sympercent The idea proposed in (Cole 2000), neatly illustrated in (T. J. Cole and Altman 2017b; T. J. Cole and Altman 2017a) is to exploit a property of natural logarithms where because $$ \\ln(A)‚àí\\ln(B)=\\ln(A/B) $$ the ratio A divided by B can be expressed as a sum, retaining additivity and symmetry, and it turns out, approximating a \u0026lsquo;percentage\u0026rsquo; change. The proposed name for this measure is \u0026ldquo;sympercent\u0026rdquo; and (Cole 2000) gave it the symbol s% and the formula $$ 100‚ÄÖ√ó‚ÄÖ[\\ln(A)‚àí\\ln(B)] $$ Repeating the examples above; first of all A = 97, B = 67 which gave us a 31% (percent) improvement instead gives us the sympercent change:\n$$ 100‚ÄÖ√ó‚ÄÖ[\\ln(97)‚àí\\ln(67)]‚ÄÑ‚âà‚ÄÑ37 s\\% $$\nA different numerical value (37, versus 31). Note, however, that we gain symmetry \u0026ndash; so with A = 67 and B = 97 (worsening symptoms from A to B):\n$$ 100‚ÄÖ√ó‚ÄÖ[\\ln(67)‚àí\\ln(97)]‚ÄÑ‚âà‚ÄÑ‚àí37 s\\% $$\nThe magnitude is the same (37) but the sign changes to represent worsening, rather than improvement.\nRepeating the additivity example; improvement from A = 100 to B = 70 but with subsequent worsening back to C = 100. We\u0026rsquo;ll drop the factor of 100 to simplify presentation: $$ \\begin{aligned} \\ln(A) + \\big[ \\ln(A)-\\ln(B) \\big] + \\big[\\ln(B)-\\ln(C) \\big] \u0026amp;= 4.61 + 0.36 - 0.36 \\\\\\\n\u0026amp;= 4.61 \\end{aligned} $$\nThat is, a net change of zero from the baseline.\nSwitching to an alternative representation affords a more interpretable and intuitive measure of change that preserves the idea of change relative to the patient\u0026rsquo;s baseline.\nBelow, the behaviour of sympercent is shown over the range [0,100], analogous to the percent change shown above:\n The Original Problem Let\u0026rsquo;s apply the \u0026lsquo;sympercent\u0026rsquo; idea for the problem originally introduced \u0026ndash; a threshold of 20% improvement for symptoms.\nTo simplify presentation, we drop the factor 100 and instead, work in fractions of 1 (i.e. rather 20% we say 0.2 or 1/5). If we require at least a 20% reduction in symptom scores to qualify as a response, then we are saying:\n$$ \\begin{aligned} \\frac{A-B}{A} \u0026amp;= 0.2 \\\\\\\nA-B \u0026amp;= 0.2 \\times A \\end{aligned} $$ i.e. the difference between symptom scores at time points A and B should be a fraction (0.2, or one-fifth) of the baseline A:\nReusing our first example, where A = 97: $$ \\begin{aligned} 97-B = 0.2 \\times 97 \\end{aligned} $$ So, for symptoms to have improved by at least 20% from a baseline of A = 97, the symptom score at B should be 77.6 or lower.\nWhat would this reduction look like in s% (sympercents)?\nIf A = 97, and we know a 20% reduction (or 0.2) would represent B = 77.6 then:\n$$ 100‚ÄÖ√ó‚ÄÖ[\\ln(97)‚àí\\ln(77.6)]‚ÄÑ‚âà‚ÄÑ22.3 s\\% $$\nReferences Cole, Tim J, and Douglas G Altman. 2017a. ‚ÄúStatistics Notes: Percentage Differences, Symmetry, and Natural Logarithms.‚Äù BMJ 358. British Medical Journal Publishing Group: j3683.\n‚Äî‚Äî‚Äî. 2017b. ‚ÄúStatistics Notes: What Is a Percentage Difference?‚Äù BMJ 358. British Medical Journal Publishing Group: j3663.\nCole, TJ. 2000. ‚ÄúSympercents: Symmetric Percentage Differences on the 100 loge Scale Simplify the Presentation of Log Transformed Data.‚Äù Statistics in Medicine 19 (22). Wiley Online Library: 3109‚Äì25.\nHowes, Oliver D, Rob McCutcheon, Ofer Agid, Andrea De Bartolomeis, Nico JM Van Beveren, Michael L Birnbaum, Michael AP Bloomfield, et al. 2016. ‚ÄúTreatment-Resistant Schizophrenia: Treatment Response and Resistance in Psychosis (Trrip) Working Group Consensus Guidelines on Diagnosis and Terminology.‚Äù American Journal of Psychiatry 174 (3). Am Psychiatric Assoc: 216‚Äì29.\nLeucht, S, JM Davis, RR Engel, W Kissling, and JM Kane. 2009. ‚ÄúDefinitions of Response and Remission in Schizophrenia: Recommendations for Their Use and Their Presentation.‚Äù Acta Psychiatrica Scandinavica 119. Wiley Online Library: 7‚Äì14.\nMunk-J√∏rgensen, Povl. 2011. ‚ÄúCorrigendum.‚Äù Acta Psychiatrica Scandinavica 124 (1): 82‚Äì82. doi:10.1111/j.1600-0447.2011.01720.x.\nSenn, Stephen S. 2008. Statistical Issues in Drug Development. Vol. 69. John Wiley \u0026amp; Sons.\n","date":1555613276,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555613276,"objectID":"72e4dd53685c2d695070262d6f668bde","permalink":"/post/2021-05-23-symm-percent/add_symm_change_score/","publishdate":"2019-04-18T19:47:56+01:00","relpermalink":"/post/2021-05-23-symm-percent/add_symm_change_score/","section":"post","summary":"The motivation for looking at this topic was primarily my naivety and getting counter-intuitive results when I was looking at serial measures of response to treatment. In particular, I wanted to use outcome measures based on published conventions in the literature for threshold response (the problems with this approach are a topic for another day).","tags":["measures"],"title":"Alternatives to Percentage Change","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Alexis E. Cullen","Scarlett Holmes","Thomas A. Pollak","Graham Blackman","Dan W. Joyce","Matthew J. Kempton","Robin M. Murray","Philip McGuire","Valeria Mondelli"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"607a77dea4d24fb820f97dd8c744a197","permalink":"/publication/cullen-associations-2019/","publishdate":"2020-04-05T21:13:10.145402Z","relpermalink":"/publication/cullen-associations-2019/","section":"publication","summary":"BACKGROUND: A relationship between non-neurological autoimmune (NNAI) disorders and psychosis has been widely reported but not yet subjected to meta-analysis. We conducted the first meta-analysis examining the association between NNAI disorders and psychosis and investigated the effect of 1) temporality (as determined by study design), 2) psychiatric diagnosis, and 3) specific autoimmune disorders. METHODS: Major databases were searched for articles published until April 2018; 31 studies, comprising data for textgreater25 million individuals, were eligible. Using random-effects models, we examined the overall association between all NNAI disorders and psychosis; rheumatoid arthritis was examined separately given the well-established negative association with psychosis. Stratified analyses investigated the effect of temporality, psychiatric diagnosis, and specific NNAI disorders. RESULTS: We observed a positive overall association between NNAI disorders and psychosis (odds ratio [OR] = 1.26; 95% confidence interval [CI], 1.12-1.41) that was consistent across study designs and psychiatric diagnoses; however, considerable heterogeneity was detected (I(2) = 88.08). Patterns varied across individual NNAI disorders; associations were positive for pernicious anemia (OR = 1.91; 95% CI, 1.29-2.84), pemphigoid (OR = 1.90; 95% CI, 1.62-2.24), psoriasis (OR = 1.70; 95% CI, 1.51-1.91), celiac disease (OR = 1.53; 95% CI, 1.12-2.10), and Graves' disease (OR = 1.33; 95% CI, 1.03-1.72) and negative for ankylosing spondylitis (OR = 0.72; 95% CI, 0.54-0.98) and rheumatoid arthritis (OR = 0.65; 95% CI, 0.50-0.84). CONCLUSIONS: While we observed a positive overall association between NNAI disorders and psychosis, this was not consistent across all NNAI disorders. Specific factors, including distinct inflammatory pathways, genetic influences, autoantibodies targeting brain proteins, and exposure to corticosteroid treatment, may therefore underlie  this association.","tags":["*Autoimmune","*Epidemiology","*Inflammation","*Meta-analysis","*Psychosis","*Schizophrenia","Autoantibodies/*blood","Autoimmune Diseases/*complications","Autoimmunity","Brain/*immunology/physiopathology","Humans","Psychotic Disorders/*complications","Risk Factors"],"title":"Associations Between Non-neurological Autoimmune Disorders and Psychosis: A Meta-analysis.","type":"publication"},{"authors":["Dan W. Joyce","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"37d219ac4d82c8d1f4d2932862558197","permalink":"/publication/joyce-integration-2018/","publishdate":"2020-04-05T21:13:10.14693Z","relpermalink":"/publication/joyce-integration-2018/","section":"publication","summary":"","tags":null,"title":"Integration is not necessarily at odds with reductionism.","type":"publication"},{"authors":["Evangelos Papanastasiou","Elias Mouchlianitis","Dan W. Joyce","Philip McGuire","Tobias Banaschewski","Arun L. W. Bokde","Uli Bromberg","Christian Buchel","Erin Burke Quinlan","Sylvane Desrivieres","Herta Flor","Vincent Frouin","Hugh Garavan","Philip Spechler","Penny Gowland","Andreas Heinz","Bernd Ittermann","Jean-Luc Martinot","Marie-Laure Paillere Martinot","Eric Artiges","Frauke Nees","Dimitri Papadopoulos Orfanos","Luise Poustka","Sabina Millenet","Juliane H. Frohner","Michael N. Smolka","Henrik Walter","Robert Whelan","Gunter Schumann","Sukhwinder Shergill"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"9c9d08807d36c3680d41a957935f4621","permalink":"/publication/papanastasiou-examination-2018/","publishdate":"2020-04-05T21:13:10.146403Z","relpermalink":"/publication/papanastasiou-examination-2018/","section":"publication","summary":"Importance: Psychoticlike experiences (PLEs) are subclinical manifestations of psychotic symptoms and may reflect an increased vulnerability to psychotic disorders. Contemporary models of psychosis propose that dysfunctional reward processing is involved in the cause of these clinical illnesses. Objective: To examine the neuroimaging profile of healthy adolescents at 14 and 19 years old points with PLEs, using a reward task. Design, Setting, and Participants: A community-based cohort study, using both a cross-sectional and longitudinal design, was conducted in academic centers in London, Nottingham, United Kingdom,  and Dublin, Ireland; Paris, France; and Berlin, Hamburg, Mannheim, and Dresden, Germany. A group of 1434 healthy adolescent volunteers was evaluated, and 2 subgroups were assessed at ages 14 and 19 years. Those who scored as either high  or low PLE (based on the upper and lower deciles) on the Community Assessment of  Psychic Experiences Questionnaire (CAPE-42) at age 19 years were included in the  analysis. The study was conducted from January 1, 2016, to January 1, 2017. Main  Outcomes and Measures: Participants were assessed at age 14 and 19 year points using functional magnetic resonance imaging while performing a monetary incentive delay reward task. A first-level model focused on 2 predefined contrasts of anticipation and feedback of a win. The second-level analysis examined activation within the reward network using an a priori-defined region of interest approach.  The main effects of group, time, and their interaction on brain activation were examined. Results: Of the 1434 adolescents, 2 groups (n = 149 each) (high PLEs, n = 149, 50 [33.6%] male; low PLEs, n = 149, 84 [56.4%] male) were compared at ages 14 and 19 years. Two regions within the left and right middle frontal gyri showed a main effect of time on brain activation (F1, 93 = 5.559; P = .02; F1, 93 = 5.009; P = .03, respectively); there was no main effect of group. One region within the right middle frontal gyrus demonstrated a significant time x group interaction (F1, 93 = 7.448; P = .01). Conclusion and Relevance: The findings are consistent with evidence implicating alterations in prefrontal and striatal function during reward processing in the etiology of psychosis. Given the nature  of this nonclinical sample this may reflect a combination of aberrant salience yielding abnormal experiences and a compensatory cognitive control mechanism necessary to contextualize them.","tags":["Humans","*Reward","Adolescent","Adult","Cross-Sectional Studies","Delay Discounting/*physiology","Female","Functional Neuroimaging/*methods","Longitudinal Studies","Magnetic Resonance Imaging","Male","Neostriatum/diagnostic imaging/*physiopathology","Prefrontal Cortex/diagnostic imaging/*physiopathology","Psychotic Disorders/diagnostic imaging/*physiopathology","Young Adult"],"title":"Examination of the Neural Basis of Psychoticlike Experiences in Adolescence During Reward Processing.","type":"publication"},{"authors":["D. W. Joyce","D. K. Tracy","S. S. Shergill"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"c1ef1cc5e9003eb7981572c6424b6439","permalink":"/publication/joyce-are-2018/","publishdate":"2020-04-05T21:13:10.146122Z","relpermalink":"/publication/joyce-are-2018/","section":"publication","summary":"Clinical trials in psychiatry inherit methods for design and statistical analysis from evidence-based medicine. However, trials in other clinical disciplines benefit from a more specific relationship between instruments that measure disease state (e.g. biomarkers, clinical signs), the underlying pathology and diagnosis such that primary outcomes can be readily defined. Trials in psychiatry use diagnosis (i.e. a categorical label for a syndrome) as a proxy for the underlying disorder, and outcomes are defined, for example, as a percentage change in a univariate total score on some clinical instrument. We label this approach to defining outcomes weak aggregation of disease state. Univariate measures are necessary, because statistical methodology is both tractable and well-developed for scalar outcomes, but we show that weak aggregate approaches do not capture disease state sufficiently, potentially leading to loss of information about response to intervention. We demonstrate how multivariate disease state can be captured using geometric concepts of spaces defined over routine clinical instruments, and show how clinically meaningful disease states (e.g. representing different profiles of symptoms, recovery or remission) can be  defined as prototypes (geometric locations) in these spaces. Then, we show how to derive univariate (scalar) measures, which capture patient's relationships to these prototypes and argue these represent strong aggregates of disease state that may be a better basis for outcome measures. We demonstrate our proposal using a large publically available dataset. We conclude by discussing the impact  of strong aggregates for analyses in traditional and novel trial designs.","tags":["Humans","*Data Interpretation","Statistical","Clinical Trials as Topic/*standards","dimensional definitions of disorder","Mental Disorders/*therapy","methodology","Outcome Assessment","Health Care/*standards","outcomes","Psychiatric Status Rating Scales/*standards","Psychiatry/*standards","Research Design/*standards","Trials"],"title":"Are we failing clinical trials? A case for strong aggregate outcomes.","type":"publication"},{"authors":["Cristal Oxley","Omer S. Moghraby","Rani Samuel","Dan W. Joyce"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d0df616e9ac964117c06264f21669380","permalink":"/publication/oxley-improving-2018/","publishdate":"2020-04-05T21:13:10.1467Z","relpermalink":"/publication/oxley-improving-2018/","section":"publication","summary":"Attention deficit hyperactivity disorder (ADHD) is a common neurodevelopmental disorder characterised by a persistent, pervasive pattern of inattention, impulsivity and hyperactivity. Stimulant medication such as methylphenidate has an established evidence base in the treatment of children and adolescents with ADHD. However, it is also associated with a risk of side effects which may include decreased appetite, increased blood pressure and possible reduced growth. Monitoring physical health in children and adolescents prescribed medication for  ADHD is a key clinical responsibility and includes a number of parameters as outlined in the National Institute for Health and Care Excellence Guidelines. Ascertaining the centiles of physical observations is essential to put these into developmental context and accurately inform treatment decisions. This quality improvement project aimed to improve physical health monitoring in children and adolescents prescribed stimulant medication for ADHD within a large specialist urban inner-city Child and Adolescent Mental Health Service (CAMHS) in South London and Maudsley NHS Foundation Trust. Baseline data were obtained to establish the quality of physical monitoring including blood pressure, height, weight and centiles. Targeted interventions included the development of a novel web-based application designed to calculate and record centiles. We report an improvement in total proportion compliance with physical health monitoring from 24% to 75%. The frequency of recording baseline blood pressure centiles increased from 0% to 62%; recording baseline height centiles increased from 37% to 81% and  recording baseline weight centiles increased from 37% to 81%. Improvement in the  delivery of high-quality care was achieved and sustained through close collaboration with clinicians involved in the treatment pathway in order to elicit and respond effectively to feedback for improvement and codevelop interventions which were highly effective within the clinical system. We believe  this model to be replicable in other CAMHS services and ADHD clinics to improve the delivery of high-quality clinical care.","tags":["decision-making","healthcare quality improvement","medication safety","patient safety","quality improvement"],"title":"Improving the quality of physical health monitoring in CAMHS for children and adolescents prescribed medication for ADHD.","type":"publication"},{"authors":["Angie A. Kehagia","Rong Ye","Dan W. Joyce","Orla M. Doyle","James B. Rowe","Trevor W. Robbins"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"f3af8c1512c8dbd174b6234d1337fa22","permalink":"/publication/kehagia-parsing-2017/","publishdate":"2020-04-05T21:13:10.147222Z","relpermalink":"/publication/kehagia-parsing-2017/","section":"publication","summary":"Cognitive control has traditionally been associated with pFC based on observations of deficits in patients with frontal lesions. However, evidence from patients with Parkinson disease indicates that subcortical regions also contribute to control under certain conditions. We scanned 17 healthy volunteers  while they performed a task-switching paradigm that previously dissociated performance deficits arising from frontal lesions in comparison with Parkinson disease, as a function of the abstraction of the rules that are switched. From a  multivoxel pattern analysis by Gaussian Process Classification, we then estimated the forward (generative) model to infer regional patterns of activity that predict Switch/Repeat behavior between rule conditions. At 1000 permutations, Switch/Repeat classification accuracy for concrete rules was significant in the BG, but at chance in the frontal lobe. The inverse pattern was obtained for abstract rules, whereby the conditions were successfully discriminated in the frontal lobe but not in the BG. This double dissociation highlights the difference between cortical and subcortical contributions to cognitive control and demonstrates the utility of multivariate approaches in investigations of functions that rely on distributed and overlapping neural substrates.","tags":["Humans","Adult","Female","Magnetic Resonance Imaging","Male","Young Adult","*Brain Mapping","Analysis of Variance","Attention/*physiology","Basal Ganglia/diagnostic imaging/*physiology","Cues","Frontal Lobe/diagnostic imaging/*physiology","Healthy Volunteers","Image Processing","Computer-Assisted","Oxygen/blood","Psychomotor Performance/*physiology","Reaction Time/physiology"],"title":"Parsing the Roles of the Frontal Lobes and Basal Ganglia in Task Control Using Multivoxel Pattern Analysis.","type":"publication"},{"authors":["Dan W. Joyce","Angie A. Kehagia","Derek K. Tracy","Jessica Proctor","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"fc912dd6165ae271fa61568441c85a55","permalink":"/publication/joyce-realising-2017/","publishdate":"2020-04-05T21:13:10.147621Z","relpermalink":"/publication/joyce-realising-2017/","section":"publication","summary":"BACKGROUND: Stratified or personalised medicine targets treatments for groups of  individuals with a disorder based on individual heterogeneity and shared factors  that influence the likelihood of response. Psychiatry has traditionally defined diagnoses by constellations of co-occurring signs and symptoms that are assigned  a categorical label (e.g. schizophrenia). Trial methodology in psychiatry has evaluated interventions targeted at these categorical entities, with diagnoses being equated to disorders. Recent insights into both the nosology and neurobiology of psychiatric disorder reveal that traditional categorical diagnoses cannot be equated with disorders. We argue that current quantitative methodology (1) inherits these categorical assumptions, (2) allows only for the discovery of average treatment response, (3) relies on composite outcome measures and (4) sacrifices valuable predictive information for stratified and personalised treatment in psychiatry. METHODS AND FINDINGS: To achieve a truly 'stratified psychiatry' we propose and then operationalise two necessary steps: first, a formal multi-dimensional representation of disorder definition and clinical state, and second, the similar redefinition of outcomes as multidimensional constructs that can expose within- and between-patient differences in response. We use the categorical diagnosis of schizophrenia-conceptualised as a label for heterogeneous disorders-as a means of introducing operational definitions of stratified psychiatry using principles from multivariate analysis. We demonstrate this framework by application to the Clinical Antipsychotic Trials of Intervention Effectiveness dataset, showing heterogeneity in both patient clinical states and their trajectories after treatment that are lost in the traditional categorical approach with composite outcomes. We then systematically review a decade of registered clinical trials for cognitive deficits in schizophrenia highlighting existing assumptions of categorical diagnoses and aggregate outcomes while identifying a small number of  trials that could be reanalysed using our proposal. CONCLUSION: We describe quantitative methods for the development of a multi-dimensional model of clinical state, disorders and trajectories which practically realises stratified psychiatry. We highlight the potential for recovering existing trial data, the implications for stratified psychiatry in trial design and clinical treatment and finally, describe different kinds of probabilistic reasoning tools necessary to implement stratification.","tags":["Humans","Mental Disorders/*therapy","*Methodology","*Multivariate","*Precision Medicine","*Psychiatry","*Stratified psychiatry","*Trials","Cognition","Multivariate Analysis","Schizophrenia/diagnosis/therapy"],"title":"Realising stratified psychiatry using multidimensional signatures and trajectories.","type":"publication"},{"authors":["Lucy D. Vanes","Thomas P. White","Rebekah L. Wigton","Dan Joyce","Tracy Collier","Sukhi S. Shergill"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"4daa0e627fa25fb881769b9deedb918a","permalink":"/publication/vanes-reduced-2016/","publishdate":"2020-04-05T21:13:10.148746Z","relpermalink":"/publication/vanes-reduced-2016/","section":"publication","summary":"Schizophrenia is characterised by the presence of abnormal complex sensory perceptual experiences. Such experiences could arise as a consequence of dysfunctional multisensory integration. We used the sound-induced flash illusion  paradigm, which probes audiovisual integration using elementary visual and auditory cues, in a sample of individuals with schizophrenia (n=40) and matched controls (n=22). Signal detection theory analyses were performed to characterise  patients' and controls' sensitivity in distinguishing 1 and 2 flashes under varying auditory conditions. Both groups experienced significant fission illusions (whereby one visual flash, accompanied by two auditory beeps, is misperceived as two flashes) and fusion illusions (whereby two flashes, accompanied by one beep, are perceived as one flash). Patients showed significantly lower fusion illusion rates compared to HC, while the fission illusion occurred similarly frequently in both groups. However, using an SDT approach, we compared illusion conditions with unimodal visual conditions, and found that illusory visual perception was overall more strongly influenced by auditory input in HC compared to patients for both illusions. This suggests that  multisensory integration may be impaired on a low perceptual level in SZ.","tags":["*Schizophrenia","Humans","Adult","Female","Male","*Schizophrenic Psychology","*Acoustic Stimulation","*Auditory Perception","*Fission illusion","*Fusion illusion","*Multisensory integration","*Optical Illusions","*Photic Stimulation","Middle Aged","Perceptual Disorders/*diagnosis/*psychology","Psychotic Disorders/*diagnosis/*psychology","Schizophrenia/*diagnosis"],"title":"Reduced susceptibility to the sound-induced flash fusion illusion in schizophrenia.","type":"publication"},{"authors":["I. Koychev","D. Joyce","E. Barkus","U. Ettinger","A. Schmechtig","C. T. Dourish","G. R. Dawson","K. J. Craig","J. F. W. Deakin"],"categories":null,"content":"","date":1462060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462060800,"objectID":"219ac24f041ce5fd7c48c0b21f430f91","permalink":"/publication/koychev-cognitive-2016/","publishdate":"2020-04-05T21:13:10.148439Z","relpermalink":"/publication/koychev-cognitive-2016/","section":"publication","summary":"The development of drugs to improve cognition in patients with schizophrenia is a major unmet clinical need. A number of promising compounds failed in recent clinical trials, a pattern linked to poor translation between preclinical and clinical stages of drug development. Seeking proof of efficacy in early Phase 1 studies in surrogate patient populations (for example, high schizotypy individuals where subtle cognitive impairment is present) has been suggested as a strategy to reduce attrition in the later stages of drug development. However, there is little agreement regarding the pattern of distribution of schizotypal features in the general population, creating uncertainty regarding the optimal control group that should be included in prospective trials. We aimed to address  this question by comparing the performance of groups derived from the general population with low, average and high schizotypy scores over a range of cognitive and oculomotor tasks. We found that tasks dependent on frontal inhibitory mechanisms (N-Back working memory and anti-saccade oculomotor tasks), as well as  a smooth-pursuit oculomotor task were sensitive to differences in the schizotypy  phenotype. In these tasks the cognitive performance of 'low schizotypes' was significantly different from 'high schizotypes' with 'average schizotypes' having an intermediate performance. These results indicate that for evaluating putative  cognition enhancers for treating schizophrenia in early-drug development studies  the maximum schizotypy effect would be achieved using a design that compares low  and high schizotypes.","tags":["Humans","Female","Male","*Schizophrenic Psychology","Cognitive Dysfunction/classification/*drug therapy/physiopathology/psychology","Drug Discovery","Eye Movement Measurements","Nootropic Agents/*therapeutic use","Schizophrenia/classification/*drug therapy/physiopathology","Translational Medical Research"],"title":"Cognitive and oculomotor performance in subjects with low and high schizotypy: implications for translational drug development studies.","type":"publication"},{"authors":["Thomas P. White","Rebekah Wigton","Dan W. Joyce","Tracy Collier","Alex Fornito","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"187c57b2b81a413c1f34d7128d1b107e","permalink":"/publication/white-dysfunctional-2016/","publishdate":"2020-04-05T21:13:10.148069Z","relpermalink":"/publication/white-dysfunctional-2016/","section":"publication","summary":"The prevalence of treatment-resistant schizophrenia points to a discrete illness  subtype, but to date its pathophysiologic characteristics are undetermined. Information transfer from ventral to dorsal striatum depends on both striato-cortico-striatal and striato-nigro-striatal subcircuits, yet although the functional integrity of the former appears to track improvement of positive symptoms of schizophrenia, the latter have received little experimental attention in relation to the illness. Here, in a sample of individuals with schizophrenia stratified by treatment resistance and matched controls, functional pathways involving four foci along the striatal axis were assessed to test the hypothesis  that treatment-resistant and non-refractory patients would exhibit contrasting patterns of resting striatal connectivity. Compared with non-refractory patients, treatment-resistant individuals exhibited reduced connectivity between ventral striatum and substantia nigra. Furthermore, disturbance to corticostriatal connectivity was more pervasive in treatment-resistant individuals. The occurrence of a more distributed pattern of abnormality may contribute to the failure of medication to treat symptoms in these individuals. This work strongly  supports the notion of pathophysiologic divergence between individuals with schizophrenia classified by treatment-resistance criteria.","tags":["Humans","Adult","Female","Magnetic Resonance Imaging","Male","Brain Mapping","Corpus Striatum/*physiopathology","Mental Status Schedule","Neural Pathways/physiopathology","Schizophrenia/diagnosis/drug therapy/*physiopathology","Schizophrenic Psychology","Substantia Nigra/*physiopathology"],"title":"Dysfunctional Striatal Systems in Treatment-Resistant Schizophrenia.","type":"publication"},{"authors":["Derek K. Tracy","Dan W. Joyce","S. Neil Sarkar","Maria-Jesus Mateos Fernandez","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"ff219684f0195e4191872c38bde2c3f8","permalink":"/publication/tracy-skating-2015/","publishdate":"2020-04-05T21:13:10.149219Z","relpermalink":"/publication/tracy-skating-2015/","section":"publication","summary":"BACKGROUND: Clozapine is the treatment of choice for medication refractory psychosis, but it does not benefit half of those put on it. There are numerous studies of potential post-clozapine strategies, but little data to guide the order of such treatment in this common clinical challenge. We describe a naturalistic observational study in 153 patients treated by a specialist psychosis service to identify optimal pharmacotherapy practice, based on outcomes. METHODS: Medication and clinical data, based on the OPCRIT tool, were examined on admission and discharge from the national psychosis service. The primary outcome measure was the percentage change in mental state examination symptoms between admission and discharge and the association with medication on discharge. Exploratory analyses evaluated the specificity of individual medication effects on symptom clusters. RESULTS: There were fewer drugs prescribed at discharge relative to admission, suggesting an optimisation of medication, and a doubling of the number of patients treated with clozapine. Treatment with clozapine on discharge was associated with maximal decrease in symptoms from admission. In the group of patients that did not respond to clozapine monotherapy, the most effective drug combinations were clozapine augmentation with 1) sodium valproate, 2) lithium, 3) amisulpride, and 4) quetiapine. There was no support for a dose-response relationship for any drug combination. CONCLUSIONS: Clozapine monotherapy is clearly the optimal medication in medication refractory schizophrenia and it is possible to maximise its use. In patients unresponsive to clozapine monotherapy, augmentation with sodium valproate, lithium, amisulpride and quetiapine, in that order, is a reasonable treatment algorithm. Reducing the number of ineffective drugs is possible without a detrimental effect on symptoms. Exploratory data indicated that clozapine was beneficial across a range of symptoms domains, whereas olanzapine was beneficial  specifically for hallucinations and lamotrigine for comorbid affective symptoms.","tags":["Humans","Adult","Female","Male","*Schizophrenic Psychology","Middle Aged","*Drug Prescriptions","Amisulpride","Antipsychotic Agents/*administration \u0026 dosage","Benzodiazepines/administration \u0026 dosage","Clozapine/*administration \u0026 dosage","Drug Therapy","Combination","Olanzapine","Quetiapine Fumarate/administration \u0026 dosage","Schizophrenia/*diagnosis/*drug therapy","Sulpiride/administration \u0026 dosage/analogs \u0026 derivatives","Valproic Acid/administration \u0026 dosage"],"title":"Skating on thin ice: pragmatic prescribing for medication refractory schizophrenia.","type":"publication"},{"authors":["Owen G. O'Daly","Daniel Joyce","Derek K. Tracy","Klaas E. Stephan","Robin M. Murray","Sukhwinder Shergill"],"categories":null,"content":"","date":1409529600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409529600,"objectID":"c92148cedceab46f963bf7d8508fd032","permalink":"/publication/odaly-amphetamine-2014-1/","publishdate":"2020-04-05T21:13:10.150305Z","relpermalink":"/publication/odaly-amphetamine-2014-1/","section":"publication","summary":"Amphetamine sensitisation (AS) is an established animal model of the hypersensitivity to psychostimulants seen in patients with schizophrenia. AS also models the dysregulation of mesolimbic dopamine signalling which has been implicated in the development of psychotic symptoms. Recent data suggest that the enhanced excitability of mesolimbic dopamine neurons in AS is driven by a hyperactivity of hippocampal (subiculum) neurons, consistent with a strong association between hippocampal dysfunction and schizophrenia. While AS can be modelled in human volunteers, its functional consequences on dopaminoceptive brain regions (i.e. striatum and hippocampus) remains unclear. Here we describe the effects of a sensitising dosage pattern of dextroamphetamine on the neural correlates of motor sequence learning in healthy volunteers, within a randomised, double-blind, parallel-groups design. Behaviourally, sensitisation was characterised by enhanced subjective responses to amphetamine but did not change  performance (i.e. learning rate) during an explicit sequence learning task. In contrast, functional magnetic resonance imaging (fMRI) measurements showed that repeated intermittent amphetamine exposure was associated with increased blood-oxygen-level dependent (BOLD) signal within the medial temporal lobe (MTL)  (subiculum/entorhinal cortex) and midbrain, in the vicinity of the substantia nigra/ventral tegmental area (SN/VTA) during sequence encoding. Importantly, MTL  hyperactivity correlated with the sensitisation of amphetamine-induced attentiveness. The MTL-midbrain hyperactivity reported here mirrors observations  in sensitised rodents and is consistent with contemporary models of schizophrenia and behavioural sensitisation. These findings of meso-hippocampal hyperactivity during AS thus link pathophysiological concepts of dopamine dysregulation to cognitive models of psychosis.","tags":["Humans","Magnetic Resonance Imaging","Healthy Volunteers","Double-Blind Method","dopamine","Amphetamine","Attention/drug effects/physiology","Central Nervous System Sensitization/*drug effects/*physiology","Central Nervous System Stimulants/pharmacology","Dextroamphetamine/*pharmacology","Functional Neuroimaging","medial temporal lobe","Memory/drug effects/*physiology","Mesencephalon/drug effects/*physiology","midbrain","motor learning","sensitisation","Serial Learning/drug effects/physiology","Temporal Lobe/drug effects/*physiology"],"title":"Amphetamine sensitisation and memory in healthy human volunteers: a functional magnetic resonance imaging study.","type":"publication"},{"authors":["Owen G. O'Daly","Daniel Joyce","Derek K. Tracy","Adnan Azim","Klaas E. Stephan","Robin M. Murray","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"25a95ded27733a560ace4e2c62a9f854","permalink":"/publication/odaly-amphetamine-2014/","publishdate":"2020-04-05T21:13:10.149782Z","relpermalink":"/publication/odaly-amphetamine-2014/","section":"publication","summary":"Dysregulation of mesolimbic dopamine transmission is implicated in a number of psychiatric illnesses characterised by disruption of reward processing and goal-directed behaviour, including schizophrenia, drug addiction and impulse control disorders associated with chronic use of dopamine agonists. Amphetamine sensitization (AS) has been proposed to model the development of this aberrant dopamine signalling and the subsequent dysregulation of incentive motivational processes. However, in humans the effects of AS on the dopamine-sensitive neural  circuitry associated with reward processing remains unclear. Here we describe the effects of acute amphetamine administration, following a sensitising dosage regime, on blood oxygen level dependent (BOLD) signal in dopaminoceptive brain regions during a rewarded gambling task performed by healthy volunteers. Using a  randomised, double-blind, parallel-groups design, we found clear evidence for sensitization to the subjective effects of the drug, while rewarded reaction times were unchanged. Repeated amphetamine exposure was associated with reduced dorsal striatal BOLD signal during decision making, but enhanced ventromedial caudate activity during reward anticipation. The amygdala BOLD response to reward outcomes was blunted following repeated amphetamine exposure. Positive correlations between subjective sensitization and changes in anticipation- and outcome-related BOLD signal were seen for the caudate nucleus and amygdala, respectively. These data show for the first time in humans that AS changes the functional impact of acute stimulant exposure on the processing of reward-related information within dopaminoceptive regions. Our findings accord with pathophysiological models which implicate aberrant dopaminergic modulation of striatal and amygdala activity in psychosis and drug-related compulsive disorders.","tags":["Humans","*Reward","Adult","Male","Young Adult","Oxygen/blood","Amygdala/*drug effects/physiopathology","Anticipation","Psychological/drug effects/physiology","Caudate Nucleus/drug effects/physiopathology","Central Nervous System Stimulants/*pharmacology","Corpus Striatum/*drug effects/physiopathology","Decision Making/drug effects/physiology","Dextroamphetamine/administration \u0026 dosage/*pharmacology","Dopamine/*physiology","Dopaminergic Neurons/drug effects","Double-Blind Method","Gambling/*physiopathology/psychology","Games","Experimental","Magnetic Resonance Imaging/methods","Motivation/physiology","Reaction Time/drug effects/physiology"],"title":"Amphetamine sensitization alters reward processing in the human striatum and amygdala.","type":"publication"},{"authors":["Thomas P. White","Rebekah L. Wigton","Dan W. Joyce","Tracy Bobin","Christian Ferragamo","Nisha Wasim","Stephen Lisk","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"96c5d88518dfab0f649fd97ee92a188c","permalink":"/publication/white-eluding-2014/","publishdate":"2020-04-05T21:13:10.150041Z","relpermalink":"/publication/white-eluding-2014/","section":"publication","summary":"Perceptions are inherently probabilistic; and can be potentially manipulated to induce illusory experience by the presentation of ambiguous or improbable evidence under selective (spatio-temporal) constraints. Accordingly, perception of the McGurk effect, by which individuals misperceive specific incongruent visual and auditory vocal cues, rests upon effective probabilistic inference. Here, we report findings from a behavioral investigation of illusory perception and related metacognitive evaluation during audiovisual integration, conducted in individuals with schizophrenia (n = 30) and control subjects (n = 24) matched in  terms of age, sex, handedness and parental occupation. Controls additionally performed the task after an oral dose of amisulpride (400 mg). Individuals with schizophrenia were observed to exhibit illusory perception less frequently than controls, despite non-significant differences in perceptual performance during control conditions. Furthermore, older individuals with schizophrenia exhibited reduced rates of illusory perception. Subsequent analysis revealed a robust inverse relationship between illness chronicity and the illusory perception rate  in this group. Controls demonstrated non-significant modulation of perception by  amisulpride; amisulpride was, however, found to elicit increases in subjective confidence in perceptual performance. Overall, these findings are consistent with the idea that impairments in probabilistic inference are exhibited in schizophrenia and exacerbated by illness chronicity. The latter suggests that associated processes are a potentially worthwhile target for therapeutic intervention.","tags":["dopamine","McGurk effect","multisensory integration","probabilistic inference","schizophrenia"],"title":"Eluding the illusion? Schizophrenia, dopamine and the McGurk effect.","type":"publication"},{"authors":["Sukhwinder S. Shergill","Thomas P. White","Daniel W. Joyce","Paul M. Bays","Daniel M. Wolpert","Chris D. Frith"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"17b24b0da8fbd68d2614c2965a66885a","permalink":"/publication/shergill-functional-2014/","publishdate":"2020-04-05T21:13:10.14949Z","relpermalink":"/publication/shergill-functional-2014/","section":"publication","summary":"IMPORTANCE: Forward models predict the sensory consequences of planned actions and permit discrimination of self- and non-self-elicited sensation; their impairment in schizophrenia is implied by an abnormality in behavioral force-matching and the flawed agency judgments characteristic of positive symptoms, including auditory hallucinations and delusions of control. OBJECTIVE:  To assess attenuation of sensory processing by self-action in individuals with schizophrenia and its relation to current symptom severity. DESIGN, SETTING, AND  PARTICIPANTS: Functional magnetic resonance imaging data were acquired while medicated individuals with schizophrenia (n = 19) and matched controls (n = 19) performed a factorially designed sensorimotor task in which the occurrence and relative timing of action and sensation were manipulated. The study took place at the neuroimaging research unit at the Institute of Cognitive Neuroscience, University College London, and the Maudsley Hospital. RESULTS: In controls, a region of secondary somatosensory cortex exhibited attenuated activation when sensation and action were synchronous compared with when the former occurred after an unexpected delay or alone. By contrast, reduced attenuation was observed in the schizophrenia group, suggesting that these individuals were unable to predict the sensory consequences of their own actions. Furthermore, failure to attenuate secondary somatosensory cortex processing was predicted by current hallucinatory severity. CONCLUSIONS AND RELEVANCE: Although comparably reduced attenuation has been reported in the verbal domain, this work implies that a more general physiologic deficit underlies positive symptoms of schizophrenia.","tags":["Humans","Adult","Female","Male","*Functional Neuroimaging","*Magnetic Resonance Imaging","Case-Control Studies","Cerebellum/physiology/physiopathology","Feedback","Sensory/*physiology","Hallucinations/physiopathology","Psychomotor Performance/physiology","Schizophrenia/*physiopathology","Sensation/physiology","Severity of Illness Index","Somatosensory Cortex/physiology/*physiopathology"],"title":"Functional magnetic resonance imaging of impaired sensory prediction in schizophrenia.","type":"publication"},{"authors":["D. W. Joyce","B. B. Averbeck","C. D. Frith","S. S. Shergill"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1383264000,"objectID":"968d8258bf455447d1d73182b3c2de05","permalink":"/publication/joyce-examining-2013/","publishdate":"2020-04-05T21:13:10.151582Z","relpermalink":"/publication/joyce-examining-2013/","section":"publication","summary":"BACKGROUND: People with psychoses often report fixed, delusional beliefs that are sustained even in the presence of unequivocal contrary evidence. Such delusional  beliefs are the result of integrating new and old evidence inappropriately in forming a cognitive model. We propose and test a cognitive model of belief formation using experimental data from an interactive 'Rock Paper Scissors' (RPS) game. METHOD: Participants (33 controls and 27 people with schizophrenia) played  a competitive, time-pressured interactive two-player game (RPS). Participants' behavior was modeled by a generative computational model using leaky integrator and temporal difference methods. This model describes how new and old evidence is integrated to form a playing strategy to beat the opponent and to provide a mechanism for reporting confidence in one's playing strategy to win against the opponent. RESULTS: People with schizophrenia fail to appropriately model their opponent's play despite consistent (rather than random) patterns that can be exploited in the simulated opponent's play. This is manifest as a failure to weigh existing evidence appropriately against new evidence. Furthermore, participants with schizophrenia show a 'jumping to conclusions' (JTC) bias, reporting successful discovery of a winning strategy with insufficient evidence.  CONCLUSIONS: The model presented suggests two tentative mechanisms in delusional  belief formation: (i) one for modeling patterns in other's behavior, where people with schizophrenia fail to use old evidence appropriately, and (ii) a metacognitive mechanism for 'confidence' in such beliefs, where people with schizophrenia overweight recent reward history in deciding on the value of beliefs about the opponent.","tags":["Humans","Adolescent","Adult","Female","Male","Young Adult","*Schizophrenic Psychology","Middle Aged","Case-Control Studies","Games","Experimental","*Schizophrenia/complications","Cognition Disorders/etiology/*psychology","Delusions/*psychology","Models","Psychological","Self Concept"],"title":"Examining belief and confidence in schizophrenia.","type":"publication"},{"authors":["Paula M. Gromann","Dirk J. Heslenfeld","Anne-Kathrin Fett","Dan W. Joyce","Sukhi S. Shergill","Lydia Krabbendam"],"categories":null,"content":"","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"7edc7b8004d5ac73adba1b553b16dd4b","permalink":"/publication/gromann-trust-2013/","publishdate":"2020-04-05T21:13:10.150566Z","relpermalink":"/publication/gromann-trust-2013/","section":"publication","summary":"Psychosis is characterized by an elementary lack of trust in others. Trust is an  inherently rewarding aspect of successful social interactions and can be examined using neuroeconomic paradigms. This study was aimed at investigating the underlying neural basis of diminished trust in psychosis. Functional magnetic resonance imaging data were acquired from 20 patients with psychosis and 20 healthy control subjects during two multiple-round trust games; one with a cooperative and the other with a deceptive counterpart. An a priori region of interest analysis of the right caudate nucleus, right temporo-parietal junction and medial prefrontal cortex was performed focusing on the repayment phase of the games. For regions with group differences, correlations were calculated between the haemodynamic signal change, behavioural outcomes and patients' symptoms. Patients demonstrated reduced levels of baseline trust, indicated by smaller initial investments. For the caudate nucleus, there was a significant game x group interaction, with controls showing stronger activation for the cooperative  game than patients, and no differences for the deceptive game. The temporo-parietal junction was significantly more activated in control subjects than in patients during cooperative and deceptive repayments. There were no significant group differences for the medial prefrontal cortex. Patients' reduced activation within the caudate nucleus correlated negatively with paranoia scores. The temporo-parietal junction signal was positively correlated with positive symptom scores during deceptive repayments. Reduced sensitivity to social reward  may explain the basic loss of trust in psychosis, mediated by aberrant activation of the caudate nucleus and the temporo-parietal junction.","tags":["Humans","*Reward","Adolescent","Adult","Male","Young Adult","Middle Aged","Magnetic Resonance Imaging/methods","*Interpersonal Relations","*Trust/psychology","Brain/*metabolism","fMRI","neuroeconomics","Paranoid Disorders/*metabolism/psychology","psychosis","Psychotic Disorders/*metabolism/psychology","social cognition","trust"],"title":"Trust versus paranoia: abnormal response to social reward in psychotic illness.","type":"publication"},{"authors":["Michael Cliff","Dan W. Joyce","Melissa Lamar","Thomas Dannhauser","Derek K. Tracy","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1367366400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367366400,"objectID":"2c45dc89a3be041e55278a5856066e3c","permalink":"/publication/cliff-aging-2013/","publishdate":"2020-04-05T21:13:10.151241Z","relpermalink":"/publication/cliff-aging-2013/","section":"publication","summary":"INTRODUCTION: Traditionally, studies investigating the functional implications of age-related structural brain alterations have focused on higher cognitive processes; by increasing stimulus load, these studies assess behavioral and neurophysiological performance. In order to understand age-related changes in these higher cognitive processes, it is crucial to examine changes in visual and  auditory processes that are the gateways to higher cognitive functions. This study provides evidence for age-related functional decline in visual and auditory processing, and regional alterations in functional brain processing, using non-invasive neuroimaging. METHODS: Using functional magnetic resonance imaging (fMRI), younger (n=11; mean age=31) and older (n=10; mean age=68) adults were imaged while observing flashing checkerboard images (passive visual stimuli) and  hearing word lists (passive auditory stimuli) across varying stimuli presentation rates. RESULTS: Younger adults showed greater overall levels of temporal and occipital cortical activation than older adults for both auditory and visual stimuli. The relative change in activity as a function of stimulus presentation rate showed differences between young and older participants. In visual cortex, the older group showed a decrease in fMRI blood oxygen level dependent (BOLD) signal magnitude as stimulus frequency increased, whereas the younger group showed a linear increase. In auditory cortex, the younger group showed a relative increase as a function of word presentation rate, while older participants showed a relatively stable magnitude of fMRI BOLD response across all rates. When analyzing participants across all ages, only the auditory cortical activation showed a continuous, monotonically decreasing BOLD signal magnitude as a function of age. CONCLUSIONS: Our preliminary findings show an age-related decline in demand-related, passive early sensory processing. As stimulus demand increases, visual and auditory cortex do not show increases in activity in older compared to younger people. This may negatively impact on the fidelity of information available to higher cognitive processing. Such evidence may inform future studies focused on cognitive decline in aging.","tags":["Humans","Adult","Female","Male","Young Adult","Middle Aged","Sensation/physiology","*Magnetic Resonance Imaging/methods","*Visual Perception","Aged","Aged","80 and over","Aging/*physiology","Auditory Cortex/*physiology","Brain Mapping/methods","Photic Stimulation","Visual Cortex/*physiology"],"title":"Aging effects on functional auditory and visual processing using fMRI with variable sensory loading.","type":"publication"},{"authors":["Sukhwinder S. Shergill","Thomas P. White","Daniel W. Joyce","Paul M. Bays","Daniel M. Wolpert","Chris D. Frith"],"categories":null,"content":"","date":1364774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1364774400,"objectID":"7a8f467929acf6a07b58608be8f03370","permalink":"/publication/shergill-modulation-2013/","publishdate":"2020-04-05T21:13:10.150846Z","relpermalink":"/publication/shergill-modulation-2013/","section":"publication","summary":"Psychophysical evidence suggests that sensations arising from our own movements are diminished when predicted by motor forward models and that these models may also encode the timing and intensity of movement. Here we report a functional magnetic resonance imaging study in which the effects on sensation of varying the occurrence, timing and force of movements were measured. We observed that tactile-related activity in a region of secondary somatosensory cortex is reduced when sensation is associated with movement and further that this reduction is maximal when movement and sensation occur synchronously. Motor force is not represented in the degree of attenuation but rather in the magnitude of this region's response. These findings provide neurophysiological correlates of previously-observed behavioural forward-model phenomena, and advocate the adopted approach for the study of clinical conditions in which forward-model deficits have been posited to play a crucial role.","tags":["Humans","Adult","Magnetic Resonance Imaging","Movement/*physiology","Sensation/*physiology","Somatosensory Cortex/*physiology","Touch/physiology"],"title":"Modulation of somatosensory processing by action.","type":"publication"},{"authors":["S. H. A. Andersson","J. Rymer","D. W. Joyce","C. Momoh","C. M. Gayle"],"categories":null,"content":"","date":1354320000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1354320000,"objectID":"7162b2ef64c3a6242b94481332b75720","permalink":"/publication/andersson-sexual-2012/","publishdate":"2020-04-05T21:13:10.152109Z","relpermalink":"/publication/andersson-sexual-2012/","section":"publication","summary":"OBJECTIVE: To investigate the sexual quality of life of women who have undergone  female genital mutilation (FGM) and compare them with a similar group who has not undergone FGM. DESIGN: Case-control study. SETTING: A large central London teaching hospital. POPULATION: A total of 73 women who had undergone FGM and 37 control women, who had not undergone FGM but were from a similar cultural background where FGM is practiced. METHODS: The women completed a questionnaire containing the Sexual Quality of Life-Female (SQOL-F) questionnaire. MAIN OUTCOME MEASURES: SQOL-F score. RESULTS: Women who have undergone FGM of any type have a  significantly lower (P textless 0.001) overall SQOL-F score than control women (mean = 62.44, SD = 27.93 versus mean = 88.84, SD = 13.73). Women who were sexually active and had undergone FGM type III differed the most from sexually active controls (P textless 0.05) in their SQOL-F score. Women who were sexually inactive but who had undergone FGM reported significantly lower overall SQOL-F scores (P = 0.015) than sexually inactive controls, but were not differentiated by type of FGM. CONCLUSION: FGM significantly reduces women's sexual quality of life, based  on the results of the SQOL-F questionnaire.","tags":["Humans","Adult","Female","Middle Aged","Case-Control Studies","*Quality of Life","Africa South of the Sahara/ethnology","Circumcision","Female/*adverse effects","Emigrants and Immigrants","Health Surveys","London","Sexual Behavior/psychology","Sexual Dysfunction","Physiological/*etiology","Sexual Dysfunctions","Psychological/*etiology","Surveys and Questionnaires"],"title":"Sexual quality of life in women who have undergone female genital mutilation: a case-control study.","type":"publication"},{"authors":["Anne-Kathrin J. Fett","Sukhi S. Shergill","Dan W. Joyce","Arno Riedl","Martin Strobel","Paula M. Gromann","Lydia Krabbendam"],"categories":null,"content":"","date":1330560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1330560000,"objectID":"4126e460d690bb79f4660351c3f4ecae","permalink":"/publication/fett-trust-2012/","publishdate":"2020-04-05T21:13:10.151846Z","relpermalink":"/publication/fett-trust-2012/","section":"publication","summary":"Psychotic illness is a disorder of social interaction unique to humans. However,  up to now research has failed to pin down the exact determinants of the complex and interactive processes associated with the development of trust and reciprocity in psychosis. Utilizing a novel multi-round version of an interactive trust game experiment, we show that patients with psychosis and healthy relatives with a heightened risk for the illness exhibit lower baseline levels of trust compared with healthy controls. This effect partly overlapped with a reduced general intelligence. Furthermore, patients were unable to modify their trusting  behaviour neither in response to information about the general trustworthiness of their interaction partner, nor in response to their partners' specific direct behavioural feedback. Relatives, in contrast, modified their trusting behaviour towards similar levels as healthy subjects in response to both. The results show  that behavioural flexibility in response to socially relevant information is a critical determinant of success in the instantiation and maintenance of social relationships. A lack thereof may drive social dysfunction and the progression from subclinical symptoms to a full-blown psychosis. This offers a testable mechanistic hypothesis for progression from prodrome to psychotic illness, and may provide a therapeutic avenue to grapple the psychotic symptoms of social dysfunction.","tags":["Humans","Adolescent","Adult","Female","Male","Young Adult","Schizophrenic Psychology","Middle Aged","Games","Experimental","*Interpersonal Relations","Data Interpretation","Statistical","Disease Progression","Family","Feedback","Psychological","Intelligence Tests","Intelligence/physiology","Neuropsychological Tests","Paranoid Disorders/psychology","Psychiatric Status Rating Scales","Psychotic Disorders/*psychology","Regression Analysis","Social Behavior","Trust/*psychology"],"title":"To trust or not to trust: the dynamics of social interaction in psychosis.","type":"publication"},{"authors":["Owen Gareth O'Daly","Daniel Joyce","Klaas Enno Stephan","Robin McGregor Murray","Sukhwinder S. Shergill"],"categories":null,"content":"","date":1306886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306886400,"objectID":"39ac45be9ffd5f2133c2506a414f740d","permalink":"/publication/odaly-functional-2011/","publishdate":"2020-04-05T21:13:10.152369Z","relpermalink":"/publication/odaly-functional-2011/","section":"publication","summary":"CONTEXT: Recent work suggests that the amphetamine sensitization model of schizophrenia can safely be induced in healthy volunteers and is associated both  with behavioral and dopaminergic hypersensitivity to amphetamine. However, the effects of a sensitization on brain function remain unclear. OBJECTIVE: To assess the impact of a sensitizing dosage regimen of dextroamphetamine on human cortical functioning and cognition. DESIGN: Randomized, double-blind, parallel-groups design using pharmacological functional magnetic resonance imaging. SETTING: The  neuroimaging research unit at the Institute of Psychiatry, King's College London, London, England. PARTICIPANTS: Healthy male volunteers (n = 22). INTERVENTIONS: Dextroamphetamine (20 mg) or placebo administration at 4 testing sessions, using  a dosage regimen shown to induce sensitization (ie, 3 doses administered with a","tags":["Humans","Adult","Male","Schizophrenic Psychology","*Magnetic Resonance Imaging","Double-Blind Method","Neuropsychological Tests","Amphetamine/*therapeutic use","Brain/drug effects/*physiopathology","Caudate Nucleus/physiopathology","Cognition/drug effects","Dopamine Uptake Inhibitors/*therapeutic use","Prefrontal Cortex/physiopathology","Psychomotor Performance/*drug effects","Reaction Time","Schizophrenia/*drug therapy/physiopathology","Temporal Lobe/physiopathology","Thalamus/physiopathology","Treatment Outcome"],"title":"Functional magnetic resonance imaging investigation of the amphetamine sensitization model of schizophrenia in healthy male volunteers.","type":"publication"},{"authors":["Kenny R. Coventry","Dermot Lynott","Angelo Cangelosi","Lynn Monrouxe","Dan Joyce","Daniel C. Richardson"],"categories":null,"content":"","date":1267401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1267401600,"objectID":"850dd0bcd1cfa42a8260ffef8a3e521f","permalink":"/publication/coventry-spatial-2010/","publishdate":"2020-04-05T21:13:10.152884Z","relpermalink":"/publication/coventry-spatial-2010/","section":"publication","summary":"Spatial language descriptions, such as The bottle is over the glass, direct the attention of the hearer to particular aspects of the visual world. This paper asks how they do so, and what brain mechanisms underlie this process. In two experiments employing behavioural and eye tracking methodologies we examined the  effects of spatial language on people's judgements and parsing of a visual scene. The results underscore previous claims regarding the importance of object function in spatial language, but also show how spatial language differentially directs attention during examination of a visual scene. We discuss implications for existing models of spatial language, with associated brain mechanisms.","tags":["Humans","Analysis of Variance","Eye Movement Measurements","*Visual Perception","Photic Stimulation","Models","Psychological","Neuropsychological Tests","*Attention","*Language","*Space Perception","Fixation","Ocular","Judgment","Language Tests","Psycholinguistics","Time Factors"],"title":"Spatial language, visual attention, and perceptual simulation.","type":"publication"},{"authors":["D. K. Tracy","O. O'Daly","D. W. Joyce","P. G. Michalopoulou","B. B. Basit","G. Dhillon","D. M. McLoughlin","S. S. Shergill"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1262304000,"objectID":"bc85695b27ee199900414f0ddbea21be","permalink":"/publication/tracy-evoked-2010/","publishdate":"2020-04-05T21:13:10.152625Z","relpermalink":"/publication/tracy-evoked-2010/","section":"publication","summary":"BACKGROUND: Auditory verbal hallucinations (AVH) are the most prevalent symptom in schizophrenia. They are associated with increased activation within the temporoparietal cortices and are refractory to pharmacological and psychological  treatment in approximately 25% of patients. Low frequency repetitive transcranial magnetic stimulation (rTMS) over the temporoparietal cortex has been demonstrated to be effective in reducing AVH in some patients, although results have varied. The cortical mechanism by which rTMS exerts its effects remain unknown, although  data from the motor system is suggestive of a local cortical inhibitory effect. We explored neuroimaging differences in healthy volunteers between application of a clinically utilized rTMS protocol and a sham rTMS equivalent when undertaking a prosodic auditory task. METHOD: Single-blind placebo controlled fMRI study of 24  healthy volunteers undertaking an auditory temporoparietal activation task, who received either right temporoparietal rTMS or sham RTMS. RESULTS: The main effect of group was bilateral inferior parietal deactivation following real rTMS. An interaction of group and task type showed deactivation during real rTMS in the right superior temporal gyrus (STG), left thalamus, left postcentral gyrus and cerebellum. However, the left parietal lobe showed an increase in activation following right sided real rTMS, but this increase was specific to a non-linguistic, tone-sequence task. CONCLUSION: rTMS does cause local inhibitory  effects, not only in the underlying region of application, but also in functionally connected cortical regions. However, there is also a related, task dependent, increase in activation within selected cortical areas in the contralateral hemisphere; these are likely to reflect compensatory mechanisms, and such cortical activation may in some cases contribute to, or retard, some of  the therapeutic effects seen with rTMS.","tags":["Humans","Adolescent","Adult","Female","Male","Young Adult","*Brain Mapping","Analysis of Variance","Oxygen/blood","Middle Aged","Magnetic Resonance Imaging/methods","Acoustic Stimulation/methods","Brain/*blood supply/*physiology","Evoked Potentials","Auditory/*physiology","Hallucinations/*pathology","Image Processing","Computer-Assisted/methods","Single-Blind Method","Transcranial Magnetic Stimulation/methods"],"title":"An evoked auditory response fMRI study of the effects of rTMS on putative AVH pathways in healthy volunteers.","type":"publication"},{"authors":["A. F. de C. Hamilton","D. W. Joyce","J. R. Flanagan","C. D. Frith","D. M. Wolpert"],"categories":null,"content":"","date":1167609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1167609600,"objectID":"a400804fd0c372e38833d624e8ed983b","permalink":"/publication/hamilton-kinematic-2007/","publishdate":"2020-04-05T21:13:10.153174Z","relpermalink":"/publication/hamilton-kinematic-2007/","section":"publication","summary":"When accepting a parcel from another person, we are able to use information about that person's movement to estimate in advance the weight of the parcel, that is,  to judge its weight from observed action. Perceptual weight judgment provides a powerful method to study our interpretation of other people's actions, but it is  not known what sources of information are used in judging weight. We have manipulated full form videos to obtain precise control of the perceived kinematics of a box lifting action, and use this technique to explore the kinematic cues that affect weight judgment. We find that observers rely most on the duration of the lifting movement to judge weight, and make less use of the durations of the grasp phase, when the box is first gripped, or the place phase,  when the box is put down. These findings can be compared to the kinematics of natural box lifting behaviour, where we find that the duration of the grasp component is the best predictor of true box weight. The lack of accord between the optimal cues predicted by the natural behaviour and the cues actually used in the perceptual task has implications for our understanding of action observation  in terms of a motor simulation. The differences between perceptual and motor behaviour are evidence against a strong version of the motor simulation hypothesis.","tags":["Humans","Adolescent","Adult","*Biomechanical Phenomena","*Cues","*Judgment","*Lifting","*Weight Perception","Videotape Recording"],"title":"Kinematic cues in perceptual weight judgement and their origins in box lifting.","type":"publication"},{"authors":["Dan Joyce","Derek Tracy","Thomas Dannhauser","Sukhwinder Shergill"],"categories":null,"content":"","date":1136073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1136073600,"objectID":"2749016fcb4032964ee67067d424d728","permalink":"/publication/joyce-passive-2006/","publishdate":"2020-04-05T21:13:10.153692Z","relpermalink":"/publication/joyce-passive-2006/","section":"publication","summary":"","tags":null,"title":"Passive processing of visual and auditory stimuli in the young and elderly: a neuroimaging study","type":"publication"},{"authors":["Kenny R Coventry","Angelo Cangelosi","Rohanna Rajapakse","Alison Bacon","Stephen Newstead","Dan Joyce","Lynn V Richards"],"categories":null,"content":"","date":1072915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1072915200,"objectID":"c6d30658d91ad2a213e511d6d23b2eb7","permalink":"/publication/coventry-spatial-2004/","publishdate":"2020-04-05T21:13:10.153469Z","relpermalink":"/publication/coventry-spatial-2004/","section":"publication","summary":"","tags":null,"title":"Spatial prepositions and vague quantifiers: Implementing the functional geometric framework","type":"publication"},{"authors":["D Joyce","L Richards","Angelo Cangelosi","Kenny R Coventry"],"categories":null,"content":"","date":1041379200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1041379200,"objectID":"70ee36886a77446ab3fd4aa26bfe27a2","permalink":"/publication/joyce-foundations-2003/","publishdate":"2020-04-05T21:13:10.153928Z","relpermalink":"/publication/joyce-foundations-2003/","section":"publication","summary":"","tags":null,"title":"On the foundations of perceptual symbol systems: Specifying embodied representations via connectionism","type":"publication"},{"authors":["Dan W Joyce","Lynn V Richards","Angelo Cangelosi","Kenny R Coventry"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"b7b9b5181e2c90d6192942f88525d0e8","permalink":"/publication/joyce-object-2002/","publishdate":"2020-04-05T21:13:10.154112Z","relpermalink":"/publication/joyce-object-2002/","section":"publication","summary":"","tags":null,"title":"Object representation-by-fragments in the visual system: A neurocomputational model","type":"publication"},{"authors":["Kenny R Coventry","Angelo Cangelosi","Dan Joyce","Lynn V Richards"],"categories":null,"content":"","date":1009843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1009843200,"objectID":"634dabeaddbebd34a63bea9f4bf59cc0","permalink":"/publication/coventry-putting-2002/","publishdate":"2020-04-05T21:13:10.154319Z","relpermalink":"/publication/coventry-putting-2002/","section":"publication","summary":"","tags":null,"title":"Putting geometry and function together-towards a psychologically-plausible computational model for spatial language comprehension","type":"publication"},{"authors":["Luc Moreau","Nick Gibbins","David DeRoure","Samhaa El-Beltagy","Wendy Hall","Gareth Hughes","Dan Joyce","Sanghee Kim","Danius Michaelides","Dave Millard"],"categories":null,"content":"","date":946684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":946684800,"objectID":"21bb028fe554a00c8632fe233273c135","permalink":"/publication/moreau-sofar-2000/","publishdate":"2020-04-05T21:13:10.154675Z","relpermalink":"/publication/moreau-sofar-2000/","section":"publication","summary":"","tags":null,"title":"SoFAR with DIM agents: An agent framework for distributed information management","type":"publication"},{"authors":["Mark Dobie","Robert Tansley","Dan Joyce","Mark Weal","Paul Lewis","Wendy Hall"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"2e8fd4611a50da1f03222b8d7fcd7d4d","permalink":"/publication/dobie-flexible-1999/","publishdate":"2020-04-05T21:13:10.155586Z","relpermalink":"/publication/dobie-flexible-1999/","section":"publication","summary":"","tags":null,"title":"A flexible architecture for content and concept based multimedia information exploration","type":"publication"},{"authors":["Dan W Joyce","Paul H Lewis"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"8fe8b16e6ca07f18f5a165652dd342cb","permalink":"/publication/joyce-connectionist-1999/","publishdate":"2020-04-05T21:13:10.154856Z","relpermalink":"/publication/joyce-connectionist-1999/","section":"publication","summary":"","tags":null,"title":"Connectionist Models for Learning Choice Behaviour in Reactive Software","type":"publication"},{"authors":["Mark Dobie","Robert Tansley","Dan Joyce","Mark Weal","Paul Lewis","Wendy Hall"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"0eddd827f03f74f6856945168f1b46ef","permalink":"/publication/dobie-mavis-1999/","publishdate":"2020-04-05T21:13:10.155359Z","relpermalink":"/publication/dobie-mavis-1999/","section":"publication","summary":"","tags":null,"title":"MAVIS 2: a new approach to content and concept based navigation","type":"publication"},{"authors":["Dan W Joyce","Paul H Lewis"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"01af4b770b330a4ad6c4e55559fffeaa","permalink":"/publication/joyce-modelling-1999/","publishdate":"2020-04-05T21:13:10.155023Z","relpermalink":"/publication/joyce-modelling-1999/","section":"publication","summary":"","tags":null,"title":"Modelling Learning for Intelligent Software Agents: A Connectionist Approach","type":"publication"},{"authors":["Dan W Joyce","Paul H Lewis","Robert H Tansley","Mark R Dobie","Wendy Hall"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"1801b180ffbb6187ad5d192a9d83eb42","permalink":"/publication/joyce-semiotics-1999/","publishdate":"2020-04-05T21:13:10.154495Z","relpermalink":"/publication/joyce-semiotics-1999/","section":"publication","summary":"","tags":null,"title":"Semiotics and agents for integrating and navigating through multimedia representations of concepts","type":"publication"},{"authors":["Joseph KP Kuan","Dan W Joyce","Paul H Lewis"],"categories":null,"content":"","date":915148800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":915148800,"objectID":"8f4d2f49f47146cb3888856c400ca09a","permalink":"/publication/kuan-texture-1999/","publishdate":"2020-04-05T21:13:10.15519Z","relpermalink":"/publication/kuan-texture-1999/","section":"publication","summary":"","tags":null,"title":"Texture Content-Based Retrieval Using Content Descriptions","type":"publication"},{"authors":["Joseph KP Kuan","Dan W Joyce","Paul H Lewis"],"categories":null,"content":"","date":883612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":883612800,"objectID":"212f7ee751942241b5d50e45aa65ee71","permalink":"/publication/kuan-flexible-1998/","publishdate":"2020-04-05T21:13:10.155778Z","relpermalink":"/publication/kuan-flexible-1998/","section":"publication","summary":"","tags":null,"title":"Flexible Features in Texture Similarity","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]