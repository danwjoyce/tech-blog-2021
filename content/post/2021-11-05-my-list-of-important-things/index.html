---
title: Stuff I Always Look Up ...
author: Dan W. Joyce
date: '2021-11-05'
slug: my-list-of-important-things
categories: [computation, maths]
tags: [probability, regression]
subtitle: ''
summary: ''
authors: [dwj]
draft: false
lastmod: '2021-11-05T22:05:06Z'
featured: yes
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []

output:
  blogdown::html_page:
    number_sections: true
    toc: true

header-includes: \usepackage{amsmath}

bibliography: [./biblio.bib]

---



<div id="TOC">
<ul>
<li><a href="#purpose"><span class="toc-section-number">1</span> Purpose</a></li>
<li><a href="#posterior-predictive-distributions"><span class="toc-section-number">2</span> Posterior Predictive Distributions</a>
<ul>
<li><a href="#conditional-probability"><span class="toc-section-number">2.1</span> Conditional Probability</a></li>
<li><a href="#law-of-total-probability"><span class="toc-section-number">2.2</span> Law of Total Probability</a></li>
<li><a href="#conditional-independence"><span class="toc-section-number">2.3</span> Conditional Independence</a></li>
<li><a href="#bayes-theorem"><span class="toc-section-number">2.4</span> Bayes Theorem</a></li>
<li><a href="#bayes-with-extra-conditioning"><span class="toc-section-number">2.5</span> Bayes with Extra Conditioning</a></li>
<li><a href="#deriving-the-ppd"><span class="toc-section-number">2.6</span> Deriving the PPD</a></li>
</ul></li>
<li><a href="#sampling-and-integration"><span class="toc-section-number">3</span> Sampling and Integration</a></li>
<li><a href="#logits-probabilities-and-odds"><span class="toc-section-number">4</span> Logits, Probabilities and Odds</a>
<ul>
<li><a href="#odds"><span class="toc-section-number">4.1</span> Odds</a></li>
<li><a href="#odds-ratios"><span class="toc-section-number">4.2</span> Odds Ratios</a></li>
<li><a href="#logistic-regression"><span class="toc-section-number">4.3</span> Logistic Regression</a></li>
</ul></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="purpose" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Purpose</h1>
<p>This is a living, annotated bibliography of stuff I need to use on a semi-regular basis, always have to look up but in my chaotic file system, can never find. It also documents some embarrassing truths – stuff like <em>“Is variance the square root of standard deviation, or the other way round?”</em> … Of course, it’s the other way round.</p>
<p>So, point number one: If <span class="math inline">\(\sigma_X\)</span> is the standard deviation of <span class="math inline">\(X\)</span> then:
<span class="math display">\[
\mathrm{Var}(X) = \sigma^2_X
\]</span></p>
</div>
<div id="posterior-predictive-distributions" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Posterior Predictive Distributions</h1>
<p>I spent a week trying to find a derivation of equation 1.4 on pp.7 of <span class="citation">(<a href="#ref-gelman2014bayesian" role="doc-biblioref">Gelman et al. 2014</a>)</span>, the posterior predictive distribution <span class="citation">(<a href="#ref-rubin1984bayesianly" role="doc-biblioref">Rubin 1984</a>)</span> of new data <span class="math inline">\(\tilde{y}\)</span> given previously observed data <span class="math inline">\(y\)</span> and a model with parameters <span class="math inline">\(\theta\)</span>
<span class="math display" id="eq:PPD">\[\begin{equation}
p( \tilde{y} | y ) = \int p\left( \tilde{y} | \theta \right) p\left( \theta | y \right)  d\theta \tag{2.1}
\label{eqn:finalPPD}
\end{equation}\]</span></p>
<p>There are explanations floating around on the internet, but none I could follow because they skipped steps and left me confused.</p>
<p>We need a few basic laws and definitions from probability theory as follows <span class="citation">(<a href="#ref-blitzstein2014introduction" role="doc-biblioref">Blitzstein and Hwang 2019</a>)</span>:</p>
<div id="conditional-probability" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Conditional Probability</h2>
<p>For two variables <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display" id="eq:condprob">\[\begin{equation}
p(a | b) = \frac{p(a,b)}{p(b)} \tag{2.2}
\end{equation}\]</span></p>
<p>Or re-arranged:
<span class="math display" id="eq:condprob2">\[\begin{equation}
p(a,b) = p(a | b) p(b) \tag{2.3}
\end{equation}\]</span></p>
<p>And for two variables <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, conditioned on <span class="math inline">\(c\)</span>:
<span class="math display" id="eq:condprob3">\[\begin{equation}
p(a,b | c) = \frac{p(a,b,c)}{p(c)} \tag{2.4}
\end{equation}\]</span></p>
<p>and the re-arrangement:
<span class="math display">\[\begin{equation}
p(a,b,c) =  p(a,b | c) p(c)
\end{equation}\]</span></p>
<p>Note that <span class="math inline">\(p(a,b,c)\)</span> can also be factorised as <em>any</em> of the following (depending on what we want to achieve):
<span class="math display" id="eq:factors3" id="eq:factors2" id="eq:factors1">\[\begin{align}
  p(a,b,c) &amp;= p(b,c|a) p(a) \tag{2.5} \\
          &amp;= p(a,c|b) p(b) \tag{2.6} \\
          &amp;= p(a,b|c) p(c) \tag{2.7}
\end{align}\]</span></p>
</div>
<div id="law-of-total-probability" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Law of Total Probability</h2>
<p>From the joint probability <span class="math inline">\(p(a,b)\)</span>, the marginal <span class="math inline">\(p(a)\)</span> is:
<span class="math display">\[\begin{equation}
p(a) = \int p \left( a, b \right) db
\end{equation}\]</span></p>
<p>And the continuous <em>law of total probability</em> is <span class="citation">(<a href="#ref-blitzstein2014introduction" role="doc-biblioref">Blitzstein and Hwang 2019</a>)</span> pp. 289:
<span class="math display" id="eq:lotp2">\[\begin{align}
p(a) &amp;= \int p \left( a,b \right) db \\
     &amp;= \int p \left( a|b \right) p( b )db \tag{2.8}
\end{align}\]</span></p>
<p>where we’ve used equation <a href="#eq:condprob2">(2.3)</a> to re-write <span class="math inline">\(p \left( a,b \right)\)</span> as <span class="math inline">\(p \left( a|b \right) p( b )\)</span></p>
<p>Adding conditioning on <span class="math inline">\(c\)</span> we obtain <span class="citation">(<a href="#ref-blitzstein2014introduction" role="doc-biblioref">Blitzstein and Hwang 2019</a>)</span> pp. 54:
<span class="math display" id="eq:lotp-cond">\[\begin{align}
\label{eqn:lotp_cont}
p \left( a|c \right) &amp;= \int p \left( a,b | c \right) db \\
                     &amp;= \int p \left( a|b,c \right) p( b | c ) db \tag{2.9}
\end{align}\]</span></p>
</div>
<div id="conditional-independence" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Conditional Independence</h2>
<p>Two variables <span class="math inline">\(a\)</span> and <span class="math inline">\(c\)</span> are <em>conditionally independent</em> given a third variable <span class="math inline">\(b\)</span> <span class="citation">(<a href="#ref-blitzstein2014introduction" role="doc-biblioref">Blitzstein and Hwang 2019</a>)</span> pp. 58:
<span class="math display" id="eq:condind1">\[\begin{equation}
( a \perp\!\!\!\perp c ) | b \iff p(a,c | b) = p( a | b ) p( c | b) \tag{2.10}
\end{equation}\]</span></p>
</div>
<div id="bayes-theorem" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Bayes Theorem</h2>
<p>For two variables <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>:
<span class="math display" id="eq:bayes">\[\begin{equation}
p(a|b) = \frac{p(b|a)p(a)}{p(b)} \tag{2.11}
\end{equation}\]</span></p>
</div>
<div id="bayes-with-extra-conditioning" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Bayes with Extra Conditioning</h2>
<p>I frequently have to remind myself how to rewrite this form: <span class="math inline">\(p(a | b, c)\)</span> – this is covered in <span class="citation">(<a href="#ref-blitzstein2014introduction" role="doc-biblioref">Blitzstein and Hwang 2019</a>)</span> Theorem 2.4.2 on pp. 54-56.</p>
<p>There are a few useful ways to re-write.</p>
<p><strong>Using Conditional Probability</strong></p>
<p>Assume it makes sense (in the problem we’re trying to solve) to view <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> as “one thing together” then using the formula for conditional probability we get:
<span class="math display" id="eq:bayesrewritecond">\[\begin{equation}
  p( a | b, c ) = \frac{p(a,b,c)}{p(b,c)}  \tag{2.12}
\end{equation}\]</span></p>
<p>Applying conditional probability again – equation <a href="#eq:condprob3">(2.4)</a> and the different factorisations in <a href="#eq:factors1">(2.5)</a> through <a href="#eq:factors3">(2.7)</a> – to rewrite <span class="math inline">\(p(a,b,c)\)</span>:
<span class="math display">\[\begin{align}
  p(a,b,c) &amp;= p(b,c|a) p(a)  \\
          &amp;= p(a,c|b) p(b)  \\
          &amp;= p(a,b|c) p(c)
\end{align}\]</span></p>
<p>Choosing the RHS as suited to the problem - here, we take <span class="math inline">\(p(b,c|a) p(a)\)</span> as we are treating <span class="math inline">\(b\)</span> and <span class="math inline">\(c\)</span> as “one event” (we want to keep them together) and substitute in <a href="#eq:bayesrewritecond">(2.12)</a>:
<span class="math display" id="eq:bayesrewritecond2">\[\begin{equation}
  p( a | b, c ) = \frac{ p(b,c|a) p(a) }{ p( b,c ) } \tag{2.13}
\end{equation}\]</span></p>
<p>Now, we have to find an expression for the denominator <span class="math inline">\(p( b,c )\)</span> and we have options including another application of conditional probability so <span class="math inline">\(p( b, c ) = p(b|c) p(c)\)</span> resulting in:
<span class="math display" id="eq:bayesrewritecond2">\[\begin{equation}
  p( a | b, c ) = \frac{ p(b,c|a) p(a) }{ p(b|c) p(c) } \tag{2.13}
\end{equation}\]</span></p>
<p><strong>Using the Chain Rule of Probability</strong></p>
<p>Start, as before, with:
<span class="math display" id="eq:bayesrewritecond3">\[\begin{equation}
  p( a | b, c ) = \frac{p(a,b,c)}{p(b,c)} \tag{2.14}
\end{equation}\]</span></p>
<p>This time, decompose <span class="math inline">\(p(a,b,c)\)</span> differently, using the chain rule:
<span class="math display">\[\begin{align}
  p(a,b,c) &amp;= p(a|b,c)p(b,c)  \\
           &amp;= p(b|a,c)p(a,c)  \\
           &amp;= p(c|a,b)p(a,b)
\end{align}\]</span>
Obviously, the first re-write gets us nowhere – it merely restates <span class="math inline">\(p(a|b,c)\)</span>. Lets say the second factorisation is helpful for our problem <span class="math inline">\(p(b|a,c)p(a,c)\)</span>, then we substitute in <a href="#eq:bayesrewritecond3">(2.14)</a>:</p>
<p><span class="math display" id="eq:bayesrewritecond4">\[\begin{equation}
  p( a | b, c ) = \frac{p(b|a,c)p(a,c)}{p(b,c)} \tag{2.15}
\end{equation}\]</span></p>
<p>We now have to deal with <span class="math inline">\(p(a,c)\)</span> in the numerator and <span class="math inline">\(p(b,c)\)</span> in the denominator. Starting with the numerator, apply conditional probability again:
<span class="math display">\[\begin{equation}
  p( a, c ) = p(a|c)p(c)
\end{equation}\]</span></p>
<p>For the denominator:
<span class="math display">\[\begin{equation}
  p( b, c ) = p(b|c)p(c)
\end{equation}\]</span></p>
<p>What we end up with is Bayes theorem with extra conditioning on <span class="math inline">\(c\)</span> …</p>
<p><strong>Bayes Theorem with Extra Conditioning on <span class="math inline">\(c\)</span></strong></p>
<p>Substitute both into <a href="#eq:bayesrewritecond4">(2.15)</a>:
<span class="math display">\[\begin{align}
  p( a | b, c ) &amp;= \frac{p(b|a,c) p(a|c)p(c)} {p(b|c)p(c)} \\
                &amp;= \frac{p(b|a,c) p(a|c)} {p(b|c)}
\end{align}\]</span></p>
<p><strong>Bayes Theorem with Extra Conditioning on <span class="math inline">\(b\)</span></strong></p>
<p>Had we chosen, instead, to use <span class="math inline">\(p(c|a,b)p(a,b)\)</span>, we end up with:
<span class="math display">\[\begin{align}
  p( a | b, c ) &amp;= \frac{p(a,b,c)}{p(b,c)} \\
                &amp;= \frac{p(c|a,b)p(a,b)}{p(b,c)}
\end{align}\]</span></p>
<p>with <span class="math inline">\(p(a,b) = p(a|b)p(b)\)</span>:
<span class="math display">\[\begin{equation}
  p( a | b, c ) = \frac{p(c|a,b)p(a|b)p(b)}{p(b,c)}
\end{equation}\]</span></p>
<p>Of course, <span class="math inline">\(p(b,c) = p(c,b) = p(c|b)p(b)\)</span> in the denominator:
<span class="math display">\[\begin{align}
  p( a | b, c ) &amp;= \frac{p(c|a,b)p(a|b)p(b)}{p(c|b)p(b)} \\
                &amp;= \frac{p(c|a,b)p(a|b)}{p(c|b)}
\end{align}\]</span></p>
<p>We chose between conditioning on <span class="math inline">\(c\)</span> or <span class="math inline">\(b\)</span> depending on the problem we are trying to solve (i.e. does it make sense to consider everything being conditioned on <span class="math inline">\(c\)</span> or <span class="math inline">\(b\)</span> ?)</p>
</div>
<div id="deriving-the-ppd" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Deriving the PPD</h2>
<p>Starting with the fundamental Bayesian modelling framework:</p>
<ol style="list-style-type: decimal">
<li>before observing the data, <span class="math inline">\(y\)</span>, the <em>prior distribution</em> of the parameters is <span class="math inline">\(p(\theta)\)</span></li>
<li>we have a <em>sampling distribution</em> of the data <em>given</em> parameters <span class="math inline">\(p(y|\theta)\)</span></li>
<li>the joint distribution of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(y\)</span> is <span class="math inline">\(p(\theta, y) = p(\theta)p(y|\theta)\)</span></li>
<li>we then obtain the <em>posterior distribution</em> of the parameters of the model given the observed data:</li>
</ol>
<p><span class="math display">\[\begin{equation}
    p(\theta|y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)p(y|\theta)}{p(y)}
\end{equation}\]</span></p>
<p>So, we can think of the posterior distribution <span class="math inline">\(p(\theta|y)\)</span> as the `output’ of Bayesian model estimation.</p>
<p><strong>Step 1</strong></p>
<p>We want to obtain a distribution for future values <span class="math inline">\(\tilde{y}\)</span> given the observed (and modelled) data <span class="math inline">\(y\)</span> which is <span class="math inline">\(p(\tilde{y}|y)\)</span> using what we know about the posterior distribution arising from parameter estimation <span class="math inline">\(p(\theta | y)\)</span>.</p>
<p>The first step is to write <span class="math inline">\(p(\tilde{y}|y)\)</span> using the law of total probability (with conditioning): equation <a href="#eq:lotp-cond">(2.9)</a>:
<span class="math display" id="eq:step1">\[\begin{equation}
    p( \tilde{y} | y ) = \int p\left(\tilde{y},\theta | y \right) d\theta \tag{2.16}
\end{equation}\]</span></p>
<p><strong>Step 2</strong></p>
<p>Re-write the integrand <span class="math inline">\(p\left(\tilde{y},\theta | y \right)\)</span> using equation <a href="#eq:condprob3">(2.4)</a>:</p>
<p><span class="math display" id="eq:step2">\[\begin{equation}
    p( \tilde{y} | y ) = \int \frac{p(\tilde{y},\theta,y)}{p(y)} d\theta \tag{2.17}
\end{equation}\]</span></p>
<p><strong>Step 3</strong></p>
<p>We assert that <span class="math inline">\(\tilde{y}\)</span> is conditionally independent of <span class="math inline">\(y\)</span> <em>given</em> the model parameters <span class="math inline">\(\theta\)</span>:
<span class="math display" id="eq:condind-ppd">\[\begin{equation}
  ( \tilde{y} \perp\!\!\!\perp y ) | \theta \iff p( \tilde{y}, y | \theta) = p( \tilde{y} | \theta ) p( y | \theta) \tag{2.18}
\end{equation}\]</span></p>
<p>To make use of the conditional independence <span class="math inline">\(p( \tilde{y}, y | \theta)\)</span>, we have to factorise <span class="math inline">\(p(\tilde{y},\theta,y)\)</span> in equation <a href="#eq:step2">(2.17)</a>.</p>
<p>Let <span class="math inline">\(a = \tilde{y}\)</span>, <span class="math inline">\(b = \theta\)</span> and <span class="math inline">\(c = y\)</span>; we are seeking a factorisation of <span class="math inline">\(p(a,b,c)\)</span> as <span class="math inline">\(p(a,c|b)\)</span> and inspecting the factorisations <a href="#eq:factors1">(2.5)</a> through <a href="#eq:factors3">(2.7)</a> we find that <a href="#eq:factors2">(2.6)</a> matches:</p>
<p><span class="math display" id="eq:factor-cond-ind">\[\begin{align}
  p(a,b,c) &amp;= p(a,c|b) p(b) \\
  p( \tilde{y}, \theta, y) &amp;= p( \tilde{y}, y | \theta) p(\theta) \tag{2.19}
\end{align}\]</span></p>
<p>Substitution <a href="#eq:factor-cond-ind">(2.19)</a> into <a href="#eq:step2">(2.17)</a> we have:</p>
<p><span class="math display" id="eq:step3a">\[\begin{equation}
p(\tilde{y} | y ) = \int \frac{p(\tilde{y},y | \theta) p(\theta)}{p(y)} d \theta \tag{2.20}
\end{equation}\]</span></p>
<p>We make use of the equality from equation <a href="#eq:condind-ppd">(2.18)</a> i.e. that <span class="math inline">\(p( \tilde{y}, y | \theta) = p( \tilde{y} | \theta ) p( y | \theta)\)</span> and substitute into <a href="#eq:step3a">(2.20)</a>:</p>
<p><span class="math display" id="eq:step3b">\[\begin{equation}
p(\tilde{y} | y ) = \int \frac{ p( \tilde{y} | \theta ) p( y | \theta)  p(\theta)}{p(y)} d \theta \tag{2.21}
\end{equation}\]</span></p>
<p><strong>Step 4</strong></p>
<p>Recall that we want to make use of the ‘output’ of parameter estimation, the posterior distribution of the model parameters given the observed data <span class="math inline">\(p(\theta|y)\)</span>, and in equation <a href="#eq:step3b">(2.21)</a> we see the term <span class="math inline">\(p(y|\theta)\)</span>. All we need to do is re-write <span class="math inline">\(p(y|\theta)\)</span> using Bayes rule, equation <a href="#eq:bayes">(2.11)</a>:</p>
<p><span class="math display">\[\begin{equation}
p(y|\theta) = \frac{p(\theta|y) p(y)}{p(\theta)}
\end{equation}\]</span></p>
<p>And substitute into equation <a href="#eq:step3b">(2.21)</a>:
<span class="math display" id="eq:step4">\[\begin{align}
p(\tilde{y} | y ) &amp;= \int \frac{ p( \tilde{y} | \theta ) p(\theta|y) p(y)   p(\theta)}{p(y)p(\theta)} d \theta  \\
                  &amp;= \int p( \tilde{y} | \theta ) p(\theta|y) d \theta \tag{2.22} \\

\end{align}\]</span></p>
<p>… and there we have it, the expression for the PPD, equation <a href="#eq:PPD">(2.1)</a></p>
</div>
</div>
<div id="sampling-and-integration" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Sampling and Integration</h1>
<p>This one came from <a href="https://twitter.com/ChadScherrer">Chad Scherrer</a> who posted a <a href="https://twitter.com/ChadScherrer/status/1292528021568552962?s=20">tweet</a> about the relationship between integration and sampling and I thought this was a really helpful heuristic.</p>
<p>When I work through examples of Bayesian problems, my first thought is “<strong>how will I code this?</strong>” and Chad’s tweet comes to mind, and helpfully, it follows nicely from the posterior predictive distribution example.</p>
<p>To paraphrase Chad’s tweet:</p>
<ul>
<li>To sample from <span class="math inline">\(p(y|x)p(x)\)</span> …
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(x\)</span></li>
<li>Use the sample of <span class="math inline">\(x\)</span> to sample <span class="math inline">\(y\)</span></li>
<li>Return <span class="math inline">\((x,y)\)</span></li>
</ol></li>
<li>To sample from <span class="math inline">\(\int p(y|x)p(x) dx\)</span> …
<ol style="list-style-type: decimal">
<li>Sample <span class="math inline">\(x\)</span></li>
<li>Use the sample of <span class="math inline">\(x\)</span> to sample <span class="math inline">\(y\)</span></li>
<li>Discard <span class="math inline">\(x\)</span></li>
<li>Return <span class="math inline">\(y\)</span></li>
</ol></li>
</ul>
<p>In practice then: we want to obtain samples from the <em>posterior predictive distribution</em> <span class="math inline">\(p(\tilde{y}|y)\)</span> i.e. for some new or unseen <span class="math inline">\(\tilde{x}\)</span>.</p>
<p><span class="math display">\[\begin{equation}
p(\tilde{y} | y ) = \int p( \tilde{y} | \theta ) p(\theta|y) d \theta
\end{equation}\]</span></p>
<p>Here’s how to proceed. We’ve used some Bayesian parameter estimation method (e.g. MCMC or similar) to obtain samples from <span class="math inline">\(p(\theta|y)\)</span> and stored them as <span class="math inline">\(\Theta_S\)</span></p>
<p>We have a function that returns a value <span class="math inline">\(\tilde{y}\)</span> given an input <span class="math inline">\(\tilde{x}\)</span> and parameters <span class="math inline">\(\theta\)</span> such that <span class="math inline">\(\tilde{y} = f(\tilde{x}; \theta)\)</span></p>
<p>For each sample <span class="math inline">\(\theta^{s} \in \Theta_S\)</span> – one sample from <span class="math inline">\(p(\theta|y)\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>Compute <span class="math inline">\(\tilde{y}^s = f(\tilde{x}; \theta^{s})\)</span> – a sample from <span class="math inline">\(p(\tilde{y}|\theta)\)</span></p></li>
<li><p>Store <span class="math inline">\(\tilde{y}^s\)</span> and throw away <span class="math inline">\(\theta^{s}\)</span></p></li>
</ol>
<p>Our resulting collection of <span class="math inline">\(\tilde{y}^s\)</span> are samples from the PPD from which we can then take expected values, quantiles etc.</p>
</div>
<div id="logits-probabilities-and-odds" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Logits, Probabilities and Odds</h1>
<p>Can never remember these relations, so wrote them down explicitly for future reference.</p>
<div id="odds" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Odds</h2>
<p>If <span class="math inline">\(p_A\)</span> is the probability of event <span class="math inline">\(A\)</span> then:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\textrm{odds}_A = \frac{1}{(1-p_A)}\)</span></li>
<li>given the odds, we recover the probability as <span class="math inline">\(p_A = \frac{\textrm{odds}_A}{1+\textrm{odds}_A}\)</span></li>
<li>the <strong>log odds</strong> are given by <span class="math inline">\(\ln(\textrm{odds}_A) = \ln \left( \frac{1}{1-p_A} \right) = \textrm{logit}(p_A) = \ln(p_A) - \ln(1-p_A)\)</span></li>
</ol>
<p>So, odds of 1.0 equals a probability <span class="math inline">\(p_A = 0.5\)</span> – the probability of the event occurring is at chance level.</p>
</div>
<div id="odds-ratios" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Odds Ratios</h2>
<p>For two events, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with probabilities <span class="math inline">\(p_A\)</span> and <span class="math inline">\(p_B\)</span>:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\textrm{odds}_A = \frac{1}{(1-p_A)}\)</span> and <span class="math inline">\(\textrm{odds}_B = \frac{1}{(1-p_B)}\)</span></li>
<li>the <strong>odds ratio</strong> <span class="math inline">\(\textrm{OR}_{AB} = \frac{\textrm{odds}_A}{\textrm{odds}_B} = \frac{1-p_B}{1-p_A}\)</span></li>
</ol>
</div>
<div id="logistic-regression" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Logistic Regression</h2>
<p>In logistic regression applied to clinical situations, we are usually interested in a single “thing” associated with two discrete and mutually-exclusive events (e.g. <span class="math inline">\(A\)</span> = “dying” or <span class="math inline">\(B\)</span> = “not dying”) – the thing either occurs or it does <em>not</em> occur. In these circumstances <span class="math inline">\(p_B = 1 - p_A\)</span>.</p>
<p>To convert to the language of regression, denote an outcome <span class="math inline">\(y\)</span> (for example, death, experiencing a side effect, obtaining a positive response to a treatment) then:</p>
<ol style="list-style-type: decimal">
<li>the probability that <span class="math inline">\(y\)</span> occurred is <span class="math inline">\(p_y\)</span> (equivalent to <span class="math inline">\(p_A\)</span> in the example above)</li>
<li>the corresponding probability <span class="math inline">\(y\)</span> <strong>does not</strong> occur (<span class="math inline">\(¬y\)</span>) is <span class="math inline">\(1-p_y\)</span> (equivalent to <span class="math inline">\(p_B\)</span> in the example above)</li>
<li>the odds of <span class="math inline">\(y\)</span> occurring are <span class="math inline">\(\textrm{odds}_{y} = \frac{1}{(1-p_y)}\)</span> and the odds of <span class="math inline">\(¬y\)</span> are <span class="math inline">\(\textrm{odds}_{¬y} = \frac{1}{1-(1-p_y)} = \frac{1}{p_y}\)</span></li>
<li>the odds ratio of <span class="math inline">\(y\)</span> and <span class="math inline">\(¬y\)</span> is then <span class="math inline">\(\textrm{OR}_{(y,¬y)} = \frac{p_y}{1-p_y}\)</span></li>
</ol>
<p>Further reading: Chapter 13 of <span class="citation">(<a href="#ref-gelman2020regression" role="doc-biblioref">Gelman, Hill, and Vehtari 2020</a>)</span> walks through all this with examples and code in R. On the web, Jay Rotella has a <a href="https://www.montana.edu/rotella/documents/502/Prob_odds_log-odds.pdf">nice PDF walk through</a> of the same material with elaboration and graphical examples in R.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-blitzstein2014introduction" class="csl-entry">
Blitzstein, Joseph K, and Jessica Hwang. 2019. <em>Introduction to Probability</em>. 2nd ed. Chapman; Hall/CRC.
</div>
<div id="ref-gelman2020regression" class="csl-entry">
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. <em>Regression and Other Stories</em>. Cambridge University Press.
</div>
<div id="ref-gelman2014bayesian" class="csl-entry">
Gelman, Andrew, Hal S Stern, John B Carlin, David B Dunson, Aki Vehtari, and Donald B Rubin. 2014. <em>Bayesian Data Analysis</em>. 3rd ed. Chapman; Hall/CRC.
</div>
<div id="ref-rubin1984bayesianly" class="csl-entry">
Rubin, Donald B. 1984. <span>“Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician.”</span> <em>The Annals of Statistics</em> 12 (4): 1151–72.
</div>
</div>
</div>
